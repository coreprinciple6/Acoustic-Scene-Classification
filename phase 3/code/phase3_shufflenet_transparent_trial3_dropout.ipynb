{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "phase3_shufflenet_transparent_trial3_dropout.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpGC6nSSUxv3",
        "outputId": "71348b8a-aa23-4209-de66-e5358023eb4a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4bgeSpmWRKL",
        "outputId": "d9d2ce11-35de-4672-b523-9f9cbcf5d8c0"
      },
      "source": [
        "pip install torchvision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8ar3proWXa3"
      },
      "source": [
        "from torchvision import models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3gnsUPyWcBu",
        "outputId": "58344996-47a4-4605-e675-7f4dbcdff289"
      },
      "source": [
        "shufflenet = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "for name,child in shufflenet.named_children():\n",
        "  print(name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1\n",
            "maxpool\n",
            "stage2\n",
            "stage3\n",
            "stage4\n",
            "conv5\n",
            "fc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpEiRAZzWiaa"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGRxw-kSqOug"
      },
      "source": [
        "def save_checkpoint(state, filename=\"/content/drive/MyDrive/Checkpoints/phase3_shufflenet_transparent_trial_3_dropout.pth\"):\n",
        "  print(\"Saving checkpoint\")\n",
        "  torch.save(state, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bk_-w0wOWpIv",
        "outputId": "c5d3b9ed-84a7-45e9-839f-953183607131"
      },
      "source": [
        "for name, child in shufflenet.named_children():\n",
        "   if name in ['stage3','stage4','conv5','fc']:\n",
        "       print(name + ' is unfrozen')\n",
        "       for param in child.parameters():\n",
        "           param.requires_grad = True\n",
        "   elif name in ['conv1','maxpool','stage2']:\n",
        "       print(name + ' is frozen')\n",
        "       for param in child.parameters():\n",
        "           param.requires_grad = False\n",
        "\n",
        "shufflenet.last_linear = nn.Sequential(\n",
        "    nn.ReLU(),  \n",
        "    nn.Dropout(0.20),\n",
        "    nn.Linear(512, 128),\n",
        "    nn.ReLU(),  \n",
        "    nn.Dropout(0.25), \n",
        "    nn.Linear(in_features=128, out_features=2))\n",
        "\n",
        "shufflenet = shufflenet.cuda() if device else shufflenet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1 is frozen\n",
            "maxpool is frozen\n",
            "stage2 is frozen\n",
            "stage3 is unfrozen\n",
            "stage4 is unfrozen\n",
            "conv5 is unfrozen\n",
            "fc is unfrozen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F6bXMuPYPb7"
      },
      "source": [
        "from torchvision import transforms\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])\n",
        "image_transforms = transforms.Compose([\n",
        "                            transforms.Resize((224,224)),\n",
        "                            transforms.ColorJitter(hue=.05, saturation=.05),\n",
        "                            transforms.RandomRotation(degrees=15),\n",
        "                            transforms.RandomHorizontalFlip(),\n",
        "                            transforms.ToTensor(),\n",
        "                            normalize,\n",
        "                        ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTzRMYFFYVAC",
        "outputId": "f97ec22a-eccb-4931-8def-1867cda18e42"
      },
      "source": [
        "from torchvision import *\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Load the Data\n",
        "\n",
        "# Set train and valid directory paths\n",
        "train_directory = '/content/drive/MyDrive/img final dset/Train'\n",
        "valid_directory = '/content/drive/MyDrive/img final dset/Test'\n",
        "\n",
        "# Batch size\n",
        "bs = 32\n",
        "\n",
        "# Load Data from folders\n",
        "data = {\n",
        "    'train': datasets.ImageFolder(train_directory, image_transforms),\n",
        "    'test': datasets.ImageFolder(valid_directory, image_transforms)\n",
        "}\n",
        "\n",
        "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
        "train_data_size = len(data['train'])\n",
        "test_data_size = len(data['test'])\n",
        "\n",
        "# Create iterators for the Data loaded using DataLoader module\n",
        "train_data = DataLoader(data['train'], batch_size=bs, shuffle=True)\n",
        "test_data = DataLoader(data['test'], batch_size=bs, shuffle=True)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Print the train, validation and test set data sizes\n",
        "train_data_size, test_data_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27648, 6912)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szplUSxpdzSo"
      },
      "source": [
        "# HYPER-PARAMETERS\n",
        "epochs=60\n",
        "train_only_last_layer = False # boolean variable\n",
        "num_of_output_classes=2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s65rJ2Lid2_b"
      },
      "source": [
        "# Learning rate scheduler\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^\n",
        "# Let's create our learning rate scheduler. We will exponentially\n",
        "# decrease the learning rate once every few epochs.\n",
        "\n",
        "\"\"\"This function is useful only if using SGD otherwise no use\"\"\"\n",
        "\n",
        "def exp_lr_scheduler(optimizer, epoch, init_lr=0.001, lr_decay_epoch=7):\n",
        "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
        "    lr = init_lr * (0.1**(epoch // lr_decay_epoch))\n",
        "\n",
        "    if epoch % lr_decay_epoch == 0:\n",
        "        print('LR is set to {}'.format(lr))\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXopzsXwd7gx"
      },
      "source": [
        "# defining loss criterion, for these\n",
        "# models CrossEntropyLoss works the best\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opoosed to before.\n",
        "\"\"\" Defining an optimiser function here, can use Adam, RMSprop or simple SGD\"\"\"\n",
        "optimizer_conv = optim.SGD(shufflenet.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-5, nesterov=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEI-qCRzeEow"
      },
      "source": [
        "# Training the model\n",
        "# ------------------\n",
        "#\n",
        "# -  Saving (deep copying) the best model\n",
        "#\n",
        "# In the following, parameter ``lr_scheduler(optimizer, epoch)``\n",
        "# is a function  which modifies ``optimizer`` so that the learning\n",
        "# rate is changed according to desired schedule.\n",
        "\n",
        "def train_model(model, criterion, optimizer, lr_scheduler, num_epochs):\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "     epoch_start = time.time()\n",
        "     print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "\n",
        "     if epoch==1 or epoch==20 or epoch==45:\n",
        "       checkpoint= {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "       save_checkpoint(checkpoint)\n",
        "     \n",
        "     # Set to training mode\n",
        "     shufflenet.train()\n",
        "     \n",
        "     # Loss and Accuracy within the epoch\n",
        "     train_loss = 0.0\n",
        "     train_acc = 0.0\n",
        "     \n",
        "     valid_loss = 0.0\n",
        "     valid_acc = 0.0\n",
        "     \n",
        "     for i, (inputs, labels) in enumerate(train_data):\n",
        "       \n",
        "       inputs = inputs.to(device)\n",
        "       labels = labels.to(device)\n",
        "       \n",
        "       # Clean existing gradients\n",
        "       optimizer_conv.zero_grad()\n",
        "       \n",
        "       # Forward pass - compute outputs on input data using the model\n",
        "       outputs = shufflenet(inputs)\n",
        "       \n",
        "       # Compute loss\n",
        "       loss = criterion(outputs, labels)\n",
        "       \n",
        "       # Backpropagate the gradients\n",
        "       loss.backward()\n",
        "       \n",
        "       # Update the parameters\n",
        "       optimizer_conv.step()\n",
        "       \n",
        "       # Compute the total loss for the batch and add it to train_loss\n",
        "       train_loss += loss.item() * inputs.size(0)\n",
        "       \n",
        "       # Compute the accuracy\n",
        "       ret, predictions = torch.max(outputs.data, 1)\n",
        "       correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "       \n",
        "       # Convert correct_counts to float and then compute the mean\n",
        "       acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "       \n",
        "       # Compute total accuracy in the whole batch and add to train_acc\n",
        "       train_acc += acc.item() * inputs.size(0)\n",
        "       print(\"Batch number: {:03d}, Training:  Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "690nozMxgiGI",
        "outputId": "72f818ee-51df-4323-9172-9eddc126b55b"
      },
      "source": [
        "train_model(shufflenet,criterion,optimizer_conv,exp_lr_scheduler,epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Batch number: 754, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 755, Training:  Loss: 0.0146, Accuracy: 1.0000\n",
            "Batch number: 756, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 757, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 758, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 759, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 760, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 761, Training:  Loss: 0.0965, Accuracy: 0.9688\n",
            "Batch number: 762, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 763, Training:  Loss: 0.0057, Accuracy: 1.0000\n",
            "Batch number: 764, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 765, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 766, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 767, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 768, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 769, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 770, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 771, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 772, Training:  Loss: 0.0328, Accuracy: 0.9688\n",
            "Batch number: 773, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 774, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 775, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 776, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 777, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 778, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 779, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 780, Training:  Loss: 0.2421, Accuracy: 0.9375\n",
            "Batch number: 781, Training:  Loss: 0.0049, Accuracy: 1.0000\n",
            "Batch number: 782, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 783, Training:  Loss: 0.0765, Accuracy: 0.9688\n",
            "Batch number: 784, Training:  Loss: 0.0146, Accuracy: 1.0000\n",
            "Batch number: 785, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 786, Training:  Loss: 0.1020, Accuracy: 0.9688\n",
            "Batch number: 787, Training:  Loss: 0.0062, Accuracy: 1.0000\n",
            "Batch number: 788, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 789, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 790, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 791, Training:  Loss: 0.0292, Accuracy: 1.0000\n",
            "Batch number: 792, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 793, Training:  Loss: 0.0051, Accuracy: 1.0000\n",
            "Batch number: 794, Training:  Loss: 0.0079, Accuracy: 1.0000\n",
            "Batch number: 795, Training:  Loss: 0.0170, Accuracy: 1.0000\n",
            "Batch number: 796, Training:  Loss: 0.1078, Accuracy: 0.9688\n",
            "Batch number: 797, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 798, Training:  Loss: 0.0824, Accuracy: 0.9688\n",
            "Batch number: 799, Training:  Loss: 0.0049, Accuracy: 1.0000\n",
            "Batch number: 800, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 801, Training:  Loss: 0.0101, Accuracy: 1.0000\n",
            "Batch number: 802, Training:  Loss: 0.0044, Accuracy: 1.0000\n",
            "Batch number: 803, Training:  Loss: 0.0152, Accuracy: 1.0000\n",
            "Batch number: 804, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 805, Training:  Loss: 0.0054, Accuracy: 1.0000\n",
            "Batch number: 806, Training:  Loss: 0.0098, Accuracy: 1.0000\n",
            "Batch number: 807, Training:  Loss: 0.0093, Accuracy: 1.0000\n",
            "Batch number: 808, Training:  Loss: 0.0211, Accuracy: 1.0000\n",
            "Batch number: 809, Training:  Loss: 0.0564, Accuracy: 0.9688\n",
            "Batch number: 810, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 811, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 812, Training:  Loss: 0.0161, Accuracy: 1.0000\n",
            "Batch number: 813, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 814, Training:  Loss: 0.0911, Accuracy: 0.9688\n",
            "Batch number: 815, Training:  Loss: 0.0060, Accuracy: 1.0000\n",
            "Batch number: 816, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 817, Training:  Loss: 0.0073, Accuracy: 1.0000\n",
            "Batch number: 818, Training:  Loss: 0.0109, Accuracy: 1.0000\n",
            "Batch number: 819, Training:  Loss: 0.0054, Accuracy: 1.0000\n",
            "Batch number: 820, Training:  Loss: 0.0104, Accuracy: 1.0000\n",
            "Batch number: 821, Training:  Loss: 0.0077, Accuracy: 1.0000\n",
            "Batch number: 822, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 823, Training:  Loss: 0.0044, Accuracy: 1.0000\n",
            "Batch number: 824, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 825, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 826, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 827, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 828, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 829, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 830, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 831, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 832, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 833, Training:  Loss: 0.1525, Accuracy: 0.9688\n",
            "Batch number: 834, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 835, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 836, Training:  Loss: 0.0064, Accuracy: 1.0000\n",
            "Batch number: 837, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 838, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 839, Training:  Loss: 0.0223, Accuracy: 1.0000\n",
            "Batch number: 840, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 841, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 842, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 843, Training:  Loss: 0.0572, Accuracy: 0.9688\n",
            "Batch number: 844, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 845, Training:  Loss: 0.0081, Accuracy: 1.0000\n",
            "Batch number: 846, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 847, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 848, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 849, Training:  Loss: 0.0061, Accuracy: 1.0000\n",
            "Batch number: 850, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 851, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 852, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 853, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 854, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 855, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 856, Training:  Loss: 0.0562, Accuracy: 0.9688\n",
            "Batch number: 857, Training:  Loss: 0.1201, Accuracy: 0.9688\n",
            "Batch number: 858, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 859, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 860, Training:  Loss: 0.0069, Accuracy: 1.0000\n",
            "Batch number: 861, Training:  Loss: 0.0094, Accuracy: 1.0000\n",
            "Batch number: 862, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 863, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Epoch: 35/60\n",
            "Batch number: 000, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 001, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 002, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 003, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 004, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 005, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 006, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 007, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 008, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 009, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 010, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 011, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 012, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 013, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 014, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 015, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 016, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 017, Training:  Loss: 0.0126, Accuracy: 1.0000\n",
            "Batch number: 018, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 019, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 020, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 021, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 022, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 023, Training:  Loss: 0.1730, Accuracy: 0.9688\n",
            "Batch number: 024, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 025, Training:  Loss: 0.0993, Accuracy: 0.9688\n",
            "Batch number: 026, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 027, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 028, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 029, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 030, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 031, Training:  Loss: 0.0142, Accuracy: 1.0000\n",
            "Batch number: 032, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 033, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 034, Training:  Loss: 0.0178, Accuracy: 1.0000\n",
            "Batch number: 035, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 036, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 037, Training:  Loss: 0.1504, Accuracy: 0.9688\n",
            "Batch number: 038, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 039, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 040, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 041, Training:  Loss: 0.0531, Accuracy: 0.9688\n",
            "Batch number: 042, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 043, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 044, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 045, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 046, Training:  Loss: 0.0049, Accuracy: 1.0000\n",
            "Batch number: 047, Training:  Loss: 0.0056, Accuracy: 1.0000\n",
            "Batch number: 048, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 049, Training:  Loss: 0.0131, Accuracy: 1.0000\n",
            "Batch number: 050, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 051, Training:  Loss: 0.0351, Accuracy: 1.0000\n",
            "Batch number: 052, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 053, Training:  Loss: 0.0061, Accuracy: 1.0000\n",
            "Batch number: 054, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 055, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 056, Training:  Loss: 0.0254, Accuracy: 1.0000\n",
            "Batch number: 057, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 058, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 059, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 060, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 061, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 062, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 063, Training:  Loss: 0.1793, Accuracy: 0.9688\n",
            "Batch number: 064, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 065, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 066, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 067, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 068, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 069, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 070, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 071, Training:  Loss: 0.1164, Accuracy: 0.9688\n",
            "Batch number: 072, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 073, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 074, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 075, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 076, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 077, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 078, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 079, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 080, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 081, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 082, Training:  Loss: 0.0065, Accuracy: 1.0000\n",
            "Batch number: 083, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 084, Training:  Loss: 0.0123, Accuracy: 1.0000\n",
            "Batch number: 085, Training:  Loss: 0.0156, Accuracy: 1.0000\n",
            "Batch number: 086, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 087, Training:  Loss: 0.0049, Accuracy: 1.0000\n",
            "Batch number: 088, Training:  Loss: 0.0078, Accuracy: 1.0000\n",
            "Batch number: 089, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 090, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 091, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 092, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 093, Training:  Loss: 0.0340, Accuracy: 0.9688\n",
            "Batch number: 094, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 095, Training:  Loss: 0.1209, Accuracy: 0.9688\n",
            "Batch number: 096, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 097, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 098, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 099, Training:  Loss: 0.0090, Accuracy: 1.0000\n",
            "Batch number: 100, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 101, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 102, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 103, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 104, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 105, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 106, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 107, Training:  Loss: 0.0483, Accuracy: 0.9688\n",
            "Batch number: 108, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 109, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 110, Training:  Loss: 0.1487, Accuracy: 0.9688\n",
            "Batch number: 111, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 112, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 113, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 114, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 115, Training:  Loss: 0.0060, Accuracy: 1.0000\n",
            "Batch number: 116, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 117, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 118, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 119, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 120, Training:  Loss: 0.0946, Accuracy: 0.9688\n",
            "Batch number: 121, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 122, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 123, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 124, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 125, Training:  Loss: 0.0085, Accuracy: 1.0000\n",
            "Batch number: 126, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 127, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 128, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 129, Training:  Loss: 0.1034, Accuracy: 0.9688\n",
            "Batch number: 130, Training:  Loss: 0.0073, Accuracy: 1.0000\n",
            "Batch number: 131, Training:  Loss: 0.0074, Accuracy: 1.0000\n",
            "Batch number: 132, Training:  Loss: 0.0083, Accuracy: 1.0000\n",
            "Batch number: 133, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 134, Training:  Loss: 0.0105, Accuracy: 1.0000\n",
            "Batch number: 135, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 136, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 137, Training:  Loss: 0.0059, Accuracy: 1.0000\n",
            "Batch number: 138, Training:  Loss: 0.0055, Accuracy: 1.0000\n",
            "Batch number: 139, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 140, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 141, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 142, Training:  Loss: 0.0315, Accuracy: 0.9688\n",
            "Batch number: 143, Training:  Loss: 0.0442, Accuracy: 0.9688\n",
            "Batch number: 144, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 145, Training:  Loss: 0.0529, Accuracy: 0.9688\n",
            "Batch number: 146, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 147, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 148, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 149, Training:  Loss: 0.0063, Accuracy: 1.0000\n",
            "Batch number: 150, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 151, Training:  Loss: 0.0164, Accuracy: 1.0000\n",
            "Batch number: 152, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 153, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 154, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 155, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 156, Training:  Loss: 0.0165, Accuracy: 1.0000\n",
            "Batch number: 157, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 158, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 159, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 160, Training:  Loss: 0.0569, Accuracy: 0.9688\n",
            "Batch number: 161, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 162, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 163, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 164, Training:  Loss: 0.0146, Accuracy: 1.0000\n",
            "Batch number: 165, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 166, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 167, Training:  Loss: 0.0123, Accuracy: 1.0000\n",
            "Batch number: 168, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 169, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 170, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 171, Training:  Loss: 0.0267, Accuracy: 0.9688\n",
            "Batch number: 172, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 173, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 174, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 175, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 176, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 177, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 178, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 179, Training:  Loss: 0.0147, Accuracy: 1.0000\n",
            "Batch number: 180, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 181, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 182, Training:  Loss: 0.0074, Accuracy: 1.0000\n",
            "Batch number: 183, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 184, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 185, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 186, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 187, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 188, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 189, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 190, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 191, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 192, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 193, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 194, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 195, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 196, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 197, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 198, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 199, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 200, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 201, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 202, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 203, Training:  Loss: 0.1680, Accuracy: 0.9688\n",
            "Batch number: 204, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 205, Training:  Loss: 0.0553, Accuracy: 0.9688\n",
            "Batch number: 206, Training:  Loss: 0.0070, Accuracy: 1.0000\n",
            "Batch number: 207, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 208, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 209, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 210, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 211, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 212, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 213, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 214, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 215, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 216, Training:  Loss: 0.0121, Accuracy: 1.0000\n",
            "Batch number: 217, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 218, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 219, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 220, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 221, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 222, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 223, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 224, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 225, Training:  Loss: 0.2330, Accuracy: 0.9688\n",
            "Batch number: 226, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 227, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 228, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 229, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 230, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 231, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 232, Training:  Loss: 0.0190, Accuracy: 1.0000\n",
            "Batch number: 233, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 234, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 235, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 236, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 237, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 238, Training:  Loss: 0.1286, Accuracy: 0.9688\n",
            "Batch number: 239, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 240, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 241, Training:  Loss: 0.0059, Accuracy: 1.0000\n",
            "Batch number: 242, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 243, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 244, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 245, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 246, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 247, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 248, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 249, Training:  Loss: 0.0051, Accuracy: 1.0000\n",
            "Batch number: 250, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 251, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 252, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 253, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 254, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 255, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 256, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 257, Training:  Loss: 0.0197, Accuracy: 1.0000\n",
            "Batch number: 258, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 259, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 260, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 261, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 262, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 263, Training:  Loss: 0.1154, Accuracy: 0.9688\n",
            "Batch number: 264, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 265, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 266, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 267, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 268, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 269, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 270, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 271, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 272, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 273, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 274, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 275, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 276, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 277, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 278, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 279, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 280, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 281, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 282, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 283, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 284, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 285, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 286, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 287, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 288, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 289, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 290, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 291, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 292, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 293, Training:  Loss: 0.0064, Accuracy: 1.0000\n",
            "Batch number: 294, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 295, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 296, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 297, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 298, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 299, Training:  Loss: 0.0246, Accuracy: 1.0000\n",
            "Batch number: 300, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 301, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 302, Training:  Loss: 0.0096, Accuracy: 1.0000\n",
            "Batch number: 303, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 304, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 305, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 306, Training:  Loss: 0.0805, Accuracy: 0.9688\n",
            "Batch number: 307, Training:  Loss: 0.0058, Accuracy: 1.0000\n",
            "Batch number: 308, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 309, Training:  Loss: 0.0047, Accuracy: 1.0000\n",
            "Batch number: 310, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 311, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 312, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 313, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 314, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 315, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 316, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 317, Training:  Loss: 0.0047, Accuracy: 1.0000\n",
            "Batch number: 318, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 319, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 320, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 321, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 322, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 323, Training:  Loss: 0.0853, Accuracy: 0.9688\n",
            "Batch number: 324, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 325, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 326, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 327, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 328, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 329, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 330, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 331, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 332, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 333, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 334, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 335, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 336, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 337, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 338, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 339, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 340, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 341, Training:  Loss: 0.0063, Accuracy: 1.0000\n",
            "Batch number: 342, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 343, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 344, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 345, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 346, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 347, Training:  Loss: 0.0338, Accuracy: 0.9688\n",
            "Batch number: 348, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 349, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 350, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 351, Training:  Loss: 0.1580, Accuracy: 0.9688\n",
            "Batch number: 352, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 353, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 354, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 355, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 356, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 357, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 358, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 359, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 360, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 361, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 362, Training:  Loss: 0.0114, Accuracy: 1.0000\n",
            "Batch number: 363, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 364, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 365, Training:  Loss: 0.0149, Accuracy: 1.0000\n",
            "Batch number: 366, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 367, Training:  Loss: 0.1144, Accuracy: 0.9688\n",
            "Batch number: 368, Training:  Loss: 0.0254, Accuracy: 0.9688\n",
            "Batch number: 369, Training:  Loss: 0.0044, Accuracy: 1.0000\n",
            "Batch number: 370, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 371, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 372, Training:  Loss: 0.1541, Accuracy: 0.9688\n",
            "Batch number: 373, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 374, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 375, Training:  Loss: 0.0145, Accuracy: 1.0000\n",
            "Batch number: 376, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 377, Training:  Loss: 0.0070, Accuracy: 1.0000\n",
            "Batch number: 378, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 379, Training:  Loss: 0.0047, Accuracy: 1.0000\n",
            "Batch number: 380, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 381, Training:  Loss: 0.0395, Accuracy: 0.9688\n",
            "Batch number: 382, Training:  Loss: 0.0049, Accuracy: 1.0000\n",
            "Batch number: 383, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 384, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 385, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 386, Training:  Loss: 0.0084, Accuracy: 1.0000\n",
            "Batch number: 387, Training:  Loss: 0.0070, Accuracy: 1.0000\n",
            "Batch number: 388, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 389, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 390, Training:  Loss: 0.1569, Accuracy: 0.9375\n",
            "Batch number: 391, Training:  Loss: 0.0115, Accuracy: 1.0000\n",
            "Batch number: 392, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 393, Training:  Loss: 0.0079, Accuracy: 1.0000\n",
            "Batch number: 394, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 395, Training:  Loss: 0.0088, Accuracy: 1.0000\n",
            "Batch number: 396, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 397, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 398, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 399, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 400, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 401, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 402, Training:  Loss: 0.0206, Accuracy: 1.0000\n",
            "Batch number: 403, Training:  Loss: 0.0062, Accuracy: 1.0000\n",
            "Batch number: 404, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 405, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 406, Training:  Loss: 0.0053, Accuracy: 1.0000\n",
            "Batch number: 407, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 408, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 409, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 410, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 411, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 412, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 413, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 414, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 415, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 416, Training:  Loss: 0.2151, Accuracy: 0.9375\n",
            "Batch number: 417, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 418, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 419, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 420, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 421, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 422, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 423, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 424, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 425, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 426, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 427, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 428, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 429, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 430, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 431, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 432, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 433, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 434, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 435, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 436, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 437, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 438, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 439, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 440, Training:  Loss: 0.1051, Accuracy: 0.9688\n",
            "Batch number: 441, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 442, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 443, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 444, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 445, Training:  Loss: 0.0062, Accuracy: 1.0000\n",
            "Batch number: 446, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 447, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 448, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 449, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 450, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 451, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 452, Training:  Loss: 0.0192, Accuracy: 1.0000\n",
            "Batch number: 453, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 454, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 455, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 456, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 457, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 458, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 459, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 460, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 461, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 462, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 463, Training:  Loss: 0.0079, Accuracy: 1.0000\n",
            "Batch number: 464, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 465, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 466, Training:  Loss: 0.2051, Accuracy: 0.9688\n",
            "Batch number: 467, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 468, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 469, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 470, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 471, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 472, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 473, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 474, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 475, Training:  Loss: 0.0728, Accuracy: 0.9688\n",
            "Batch number: 476, Training:  Loss: 0.0042, Accuracy: 1.0000\n",
            "Batch number: 477, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 478, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 479, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 480, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 481, Training:  Loss: 0.0083, Accuracy: 1.0000\n",
            "Batch number: 482, Training:  Loss: 0.0087, Accuracy: 1.0000\n",
            "Batch number: 483, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 484, Training:  Loss: 0.0120, Accuracy: 1.0000\n",
            "Batch number: 485, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 486, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 487, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 488, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 489, Training:  Loss: 0.0106, Accuracy: 1.0000\n",
            "Batch number: 490, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 491, Training:  Loss: 0.0075, Accuracy: 1.0000\n",
            "Batch number: 492, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 493, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 494, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 495, Training:  Loss: 0.0094, Accuracy: 1.0000\n",
            "Batch number: 496, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 497, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 498, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 499, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 500, Training:  Loss: 0.0246, Accuracy: 0.9688\n",
            "Batch number: 501, Training:  Loss: 0.0068, Accuracy: 1.0000\n",
            "Batch number: 502, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 503, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 504, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 505, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 506, Training:  Loss: 0.0981, Accuracy: 0.9688\n",
            "Batch number: 507, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 508, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 509, Training:  Loss: 0.0667, Accuracy: 0.9688\n",
            "Batch number: 510, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 511, Training:  Loss: 0.1412, Accuracy: 0.9688\n",
            "Batch number: 512, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 513, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 514, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 515, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 516, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 517, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 518, Training:  Loss: 0.0144, Accuracy: 1.0000\n",
            "Batch number: 519, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 520, Training:  Loss: 0.0122, Accuracy: 1.0000\n",
            "Batch number: 521, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 522, Training:  Loss: 0.0924, Accuracy: 0.9375\n",
            "Batch number: 523, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 524, Training:  Loss: 0.0543, Accuracy: 0.9688\n",
            "Batch number: 525, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 526, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 527, Training:  Loss: 0.0063, Accuracy: 1.0000\n",
            "Batch number: 528, Training:  Loss: 0.0074, Accuracy: 1.0000\n",
            "Batch number: 529, Training:  Loss: 0.0112, Accuracy: 1.0000\n",
            "Batch number: 530, Training:  Loss: 0.0086, Accuracy: 1.0000\n",
            "Batch number: 531, Training:  Loss: 0.0486, Accuracy: 0.9688\n",
            "Batch number: 532, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 533, Training:  Loss: 0.0156, Accuracy: 1.0000\n",
            "Batch number: 534, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 535, Training:  Loss: 0.0054, Accuracy: 1.0000\n",
            "Batch number: 536, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 537, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 538, Training:  Loss: 0.0154, Accuracy: 1.0000\n",
            "Batch number: 539, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 540, Training:  Loss: 0.0206, Accuracy: 1.0000\n",
            "Batch number: 541, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 542, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 543, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 544, Training:  Loss: 0.0047, Accuracy: 1.0000\n",
            "Batch number: 545, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 546, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 547, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 548, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 549, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 550, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 551, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 552, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 553, Training:  Loss: 0.0126, Accuracy: 1.0000\n",
            "Batch number: 554, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 555, Training:  Loss: 0.0090, Accuracy: 1.0000\n",
            "Batch number: 556, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 557, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 558, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 559, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 560, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 561, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 562, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 563, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 564, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 565, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 566, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 567, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 568, Training:  Loss: 0.0090, Accuracy: 1.0000\n",
            "Batch number: 569, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 570, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 571, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 572, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 573, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 574, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 575, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 576, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 577, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 578, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 579, Training:  Loss: 0.1430, Accuracy: 0.9688\n",
            "Batch number: 580, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 581, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 582, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 583, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 584, Training:  Loss: 0.0954, Accuracy: 0.9688\n",
            "Batch number: 585, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 586, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 587, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 588, Training:  Loss: 0.0234, Accuracy: 0.9688\n",
            "Batch number: 589, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 590, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 591, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 592, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 593, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 594, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 595, Training:  Loss: 0.0089, Accuracy: 1.0000\n",
            "Batch number: 596, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 597, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 598, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 599, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 600, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 601, Training:  Loss: 0.0063, Accuracy: 1.0000\n",
            "Batch number: 602, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 603, Training:  Loss: 0.0165, Accuracy: 1.0000\n",
            "Batch number: 604, Training:  Loss: 0.0145, Accuracy: 1.0000\n",
            "Batch number: 605, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 606, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 607, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 608, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 609, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 610, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 611, Training:  Loss: 0.0114, Accuracy: 1.0000\n",
            "Batch number: 612, Training:  Loss: 0.1177, Accuracy: 0.9688\n",
            "Batch number: 613, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 614, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 615, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 616, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 617, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 618, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 619, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 620, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 621, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 622, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 623, Training:  Loss: 0.0181, Accuracy: 1.0000\n",
            "Batch number: 624, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 625, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 626, Training:  Loss: 0.1651, Accuracy: 0.9688\n",
            "Batch number: 627, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 628, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 629, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 630, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 631, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 632, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 633, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 634, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 635, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 636, Training:  Loss: 0.0068, Accuracy: 1.0000\n",
            "Batch number: 637, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 638, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 639, Training:  Loss: 0.0103, Accuracy: 1.0000\n",
            "Batch number: 640, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 641, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 642, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 643, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 644, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 645, Training:  Loss: 0.1949, Accuracy: 0.9688\n",
            "Batch number: 646, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 647, Training:  Loss: 0.0082, Accuracy: 1.0000\n",
            "Batch number: 648, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 649, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 650, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 651, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 652, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 653, Training:  Loss: 0.0070, Accuracy: 1.0000\n",
            "Batch number: 654, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 655, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 656, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 657, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 658, Training:  Loss: 0.0202, Accuracy: 1.0000\n",
            "Batch number: 659, Training:  Loss: 0.1317, Accuracy: 0.9688\n",
            "Batch number: 660, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 661, Training:  Loss: 0.0807, Accuracy: 0.9688\n",
            "Batch number: 662, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 663, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 664, Training:  Loss: 0.0167, Accuracy: 1.0000\n",
            "Batch number: 665, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 666, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 667, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 668, Training:  Loss: 0.0049, Accuracy: 1.0000\n",
            "Batch number: 669, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 670, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 671, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 672, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 673, Training:  Loss: 0.0389, Accuracy: 0.9688\n",
            "Batch number: 674, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 675, Training:  Loss: 0.0318, Accuracy: 0.9688\n",
            "Batch number: 676, Training:  Loss: 0.0050, Accuracy: 1.0000\n",
            "Batch number: 677, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 678, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 679, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 680, Training:  Loss: 0.0103, Accuracy: 1.0000\n",
            "Batch number: 681, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 682, Training:  Loss: 0.0064, Accuracy: 1.0000\n",
            "Batch number: 683, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 684, Training:  Loss: 0.0247, Accuracy: 1.0000\n",
            "Batch number: 685, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 686, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 687, Training:  Loss: 0.0046, Accuracy: 1.0000\n",
            "Batch number: 688, Training:  Loss: 0.0050, Accuracy: 1.0000\n",
            "Batch number: 689, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 690, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 691, Training:  Loss: 0.0110, Accuracy: 1.0000\n",
            "Batch number: 692, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 693, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 694, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 695, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 696, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 697, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 698, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 699, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 700, Training:  Loss: 0.0058, Accuracy: 1.0000\n",
            "Batch number: 701, Training:  Loss: 0.0278, Accuracy: 1.0000\n",
            "Batch number: 702, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 703, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 704, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 705, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 706, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 707, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 708, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 709, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 710, Training:  Loss: 0.0368, Accuracy: 0.9688\n",
            "Batch number: 711, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 712, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 713, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 714, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 715, Training:  Loss: 0.0302, Accuracy: 0.9688\n",
            "Batch number: 716, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 717, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 718, Training:  Loss: 0.0672, Accuracy: 0.9375\n",
            "Batch number: 719, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 720, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 721, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 722, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 723, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 724, Training:  Loss: 0.0358, Accuracy: 0.9688\n",
            "Batch number: 725, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 726, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 727, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 728, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 729, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 730, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 731, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 732, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 733, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 734, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 735, Training:  Loss: 0.1621, Accuracy: 0.9688\n",
            "Batch number: 736, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 737, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 738, Training:  Loss: 0.0073, Accuracy: 1.0000\n",
            "Batch number: 739, Training:  Loss: 0.0064, Accuracy: 1.0000\n",
            "Batch number: 740, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 741, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 742, Training:  Loss: 0.0077, Accuracy: 1.0000\n",
            "Batch number: 743, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 744, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 745, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 746, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 747, Training:  Loss: 0.0327, Accuracy: 0.9688\n",
            "Batch number: 748, Training:  Loss: 0.0077, Accuracy: 1.0000\n",
            "Batch number: 749, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 750, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 751, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 752, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 753, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 754, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 755, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 756, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 757, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 758, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 759, Training:  Loss: 0.0042, Accuracy: 1.0000\n",
            "Batch number: 760, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 761, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 762, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 763, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 764, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 765, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 766, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 767, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 768, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 769, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 770, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 771, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 772, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 773, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 774, Training:  Loss: 0.0720, Accuracy: 0.9688\n",
            "Batch number: 775, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 776, Training:  Loss: 0.0044, Accuracy: 1.0000\n",
            "Batch number: 777, Training:  Loss: 0.0095, Accuracy: 1.0000\n",
            "Batch number: 778, Training:  Loss: 0.0069, Accuracy: 1.0000\n",
            "Batch number: 779, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 780, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 781, Training:  Loss: 0.0637, Accuracy: 0.9688\n",
            "Batch number: 782, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 783, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 784, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 785, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 786, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 787, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 788, Training:  Loss: 0.0883, Accuracy: 0.9688\n",
            "Batch number: 789, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 790, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 791, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 792, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 793, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 794, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 795, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 796, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 797, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 798, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 799, Training:  Loss: 0.0307, Accuracy: 1.0000\n",
            "Batch number: 800, Training:  Loss: 0.0046, Accuracy: 1.0000\n",
            "Batch number: 801, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 802, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 803, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 804, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 805, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 806, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 807, Training:  Loss: 0.0076, Accuracy: 1.0000\n",
            "Batch number: 808, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 809, Training:  Loss: 0.0520, Accuracy: 0.9688\n",
            "Batch number: 810, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 811, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 812, Training:  Loss: 0.0047, Accuracy: 1.0000\n",
            "Batch number: 813, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 814, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 815, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 816, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 817, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 818, Training:  Loss: 0.0064, Accuracy: 1.0000\n",
            "Batch number: 819, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 820, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 821, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 822, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 823, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 824, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 825, Training:  Loss: 0.0051, Accuracy: 1.0000\n",
            "Batch number: 826, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 827, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 828, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 829, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 830, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 831, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 832, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 833, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 834, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 835, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 836, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 837, Training:  Loss: 0.0061, Accuracy: 1.0000\n",
            "Batch number: 838, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 839, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 840, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 841, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 842, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 843, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 844, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 845, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 846, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 847, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 848, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 849, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 850, Training:  Loss: 0.1916, Accuracy: 0.9688\n",
            "Batch number: 851, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 852, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 853, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 854, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 855, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 856, Training:  Loss: 0.1895, Accuracy: 0.9688\n",
            "Batch number: 857, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 858, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 859, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 860, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 861, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 862, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 863, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Epoch: 36/60\n",
            "Batch number: 000, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 001, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 002, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 003, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 004, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 005, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 006, Training:  Loss: 0.1399, Accuracy: 0.9688\n",
            "Batch number: 007, Training:  Loss: 0.1260, Accuracy: 0.9688\n",
            "Batch number: 008, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 009, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 010, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 011, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 012, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 013, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 014, Training:  Loss: 0.0088, Accuracy: 1.0000\n",
            "Batch number: 015, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 016, Training:  Loss: 0.0085, Accuracy: 1.0000\n",
            "Batch number: 017, Training:  Loss: 0.1952, Accuracy: 0.9688\n",
            "Batch number: 018, Training:  Loss: 0.0375, Accuracy: 0.9688\n",
            "Batch number: 019, Training:  Loss: 0.0049, Accuracy: 1.0000\n",
            "Batch number: 020, Training:  Loss: 0.0068, Accuracy: 1.0000\n",
            "Batch number: 021, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 022, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 023, Training:  Loss: 0.0096, Accuracy: 1.0000\n",
            "Batch number: 024, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 025, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 026, Training:  Loss: 0.1271, Accuracy: 0.9688\n",
            "Batch number: 027, Training:  Loss: 0.0062, Accuracy: 1.0000\n",
            "Batch number: 028, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 029, Training:  Loss: 0.0469, Accuracy: 0.9688\n",
            "Batch number: 030, Training:  Loss: 0.0575, Accuracy: 0.9688\n",
            "Batch number: 031, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 032, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 033, Training:  Loss: 0.0160, Accuracy: 1.0000\n",
            "Batch number: 034, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 035, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 036, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 037, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 038, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 039, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 040, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 041, Training:  Loss: 0.0227, Accuracy: 1.0000\n",
            "Batch number: 042, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 043, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 044, Training:  Loss: 0.0054, Accuracy: 1.0000\n",
            "Batch number: 045, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 046, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 047, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 048, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 049, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 050, Training:  Loss: 0.0068, Accuracy: 1.0000\n",
            "Batch number: 051, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 052, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 053, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 054, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 055, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 056, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 057, Training:  Loss: 0.0050, Accuracy: 1.0000\n",
            "Batch number: 058, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 059, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 060, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 061, Training:  Loss: 0.0079, Accuracy: 1.0000\n",
            "Batch number: 062, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 063, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 064, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 065, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 066, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 067, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 068, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 069, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 070, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 071, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 072, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 073, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 074, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 075, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 076, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 077, Training:  Loss: 0.0070, Accuracy: 1.0000\n",
            "Batch number: 078, Training:  Loss: 0.2545, Accuracy: 0.9688\n",
            "Batch number: 079, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 080, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 081, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 082, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 083, Training:  Loss: 0.0527, Accuracy: 0.9688\n",
            "Batch number: 084, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 085, Training:  Loss: 0.0051, Accuracy: 1.0000\n",
            "Batch number: 086, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 087, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 088, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 089, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 090, Training:  Loss: 0.0084, Accuracy: 1.0000\n",
            "Batch number: 091, Training:  Loss: 0.0915, Accuracy: 0.9688\n",
            "Batch number: 092, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 093, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 094, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 095, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 096, Training:  Loss: 0.0362, Accuracy: 0.9688\n",
            "Batch number: 097, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 098, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 099, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 100, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 101, Training:  Loss: 0.0200, Accuracy: 1.0000\n",
            "Batch number: 102, Training:  Loss: 0.0203, Accuracy: 1.0000\n",
            "Batch number: 103, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 104, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 105, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 106, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 107, Training:  Loss: 0.0075, Accuracy: 1.0000\n",
            "Batch number: 108, Training:  Loss: 0.0644, Accuracy: 0.9688\n",
            "Batch number: 109, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 110, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 111, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 112, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 113, Training:  Loss: 0.0226, Accuracy: 1.0000\n",
            "Batch number: 114, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 115, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 116, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 117, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 118, Training:  Loss: 0.0383, Accuracy: 0.9688\n",
            "Batch number: 119, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 120, Training:  Loss: 0.0087, Accuracy: 1.0000\n",
            "Batch number: 121, Training:  Loss: 0.0072, Accuracy: 1.0000\n",
            "Batch number: 122, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 123, Training:  Loss: 0.0068, Accuracy: 1.0000\n",
            "Batch number: 124, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 125, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 126, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 127, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 128, Training:  Loss: 0.1526, Accuracy: 0.9688\n",
            "Batch number: 129, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 130, Training:  Loss: 0.0358, Accuracy: 0.9688\n",
            "Batch number: 131, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 132, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 133, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 134, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 135, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 136, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 137, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 138, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 139, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 140, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 141, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 142, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 143, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 144, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 145, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 146, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 147, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 148, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 149, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 150, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 151, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 152, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 153, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 154, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 155, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 156, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 157, Training:  Loss: 0.1212, Accuracy: 0.9688\n",
            "Batch number: 158, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 159, Training:  Loss: 0.0509, Accuracy: 0.9688\n",
            "Batch number: 160, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 161, Training:  Loss: 0.0570, Accuracy: 0.9688\n",
            "Batch number: 162, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 163, Training:  Loss: 0.0072, Accuracy: 1.0000\n",
            "Batch number: 164, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 165, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 166, Training:  Loss: 0.2296, Accuracy: 0.9688\n",
            "Batch number: 167, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 168, Training:  Loss: 0.0349, Accuracy: 0.9688\n",
            "Batch number: 169, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 170, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 171, Training:  Loss: 0.0058, Accuracy: 1.0000\n",
            "Batch number: 172, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 173, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 174, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 175, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 176, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 177, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 178, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 179, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 180, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 181, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 182, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 183, Training:  Loss: 0.1366, Accuracy: 0.9688\n",
            "Batch number: 184, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 185, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 186, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 187, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 188, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 189, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 190, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 191, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 192, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 193, Training:  Loss: 0.0554, Accuracy: 0.9688\n",
            "Batch number: 194, Training:  Loss: 0.0103, Accuracy: 1.0000\n",
            "Batch number: 195, Training:  Loss: 0.0065, Accuracy: 1.0000\n",
            "Batch number: 196, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 197, Training:  Loss: 0.0596, Accuracy: 0.9688\n",
            "Batch number: 198, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 199, Training:  Loss: 0.0071, Accuracy: 1.0000\n",
            "Batch number: 200, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 201, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 202, Training:  Loss: 0.0053, Accuracy: 1.0000\n",
            "Batch number: 203, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 204, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 205, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 206, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 207, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 208, Training:  Loss: 0.1945, Accuracy: 0.9688\n",
            "Batch number: 209, Training:  Loss: 0.0160, Accuracy: 1.0000\n",
            "Batch number: 210, Training:  Loss: 0.0481, Accuracy: 0.9688\n",
            "Batch number: 211, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 212, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 213, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 214, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 215, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 216, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 217, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 218, Training:  Loss: 0.0275, Accuracy: 0.9688\n",
            "Batch number: 219, Training:  Loss: 0.0062, Accuracy: 1.0000\n",
            "Batch number: 220, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 221, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 222, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 223, Training:  Loss: 0.0094, Accuracy: 1.0000\n",
            "Batch number: 224, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 225, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 226, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 227, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 228, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 229, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 230, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 231, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 232, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 233, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 234, Training:  Loss: 0.1526, Accuracy: 0.9688\n",
            "Batch number: 235, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 236, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 237, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 238, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 239, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 240, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 241, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 242, Training:  Loss: 0.0128, Accuracy: 1.0000\n",
            "Batch number: 243, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 244, Training:  Loss: 0.1122, Accuracy: 0.9688\n",
            "Batch number: 245, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 246, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 247, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 248, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 249, Training:  Loss: 0.0099, Accuracy: 1.0000\n",
            "Batch number: 250, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 251, Training:  Loss: 0.1573, Accuracy: 0.9688\n",
            "Batch number: 252, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 253, Training:  Loss: 0.0137, Accuracy: 1.0000\n",
            "Batch number: 254, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 255, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 256, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 257, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 258, Training:  Loss: 0.0203, Accuracy: 1.0000\n",
            "Batch number: 259, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 260, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 261, Training:  Loss: 0.1596, Accuracy: 0.9688\n",
            "Batch number: 262, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 263, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 264, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 265, Training:  Loss: 0.0046, Accuracy: 1.0000\n",
            "Batch number: 266, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 267, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 268, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 269, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 270, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 271, Training:  Loss: 0.0054, Accuracy: 1.0000\n",
            "Batch number: 272, Training:  Loss: 0.0055, Accuracy: 1.0000\n",
            "Batch number: 273, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 274, Training:  Loss: 0.0231, Accuracy: 1.0000\n",
            "Batch number: 275, Training:  Loss: 0.0047, Accuracy: 1.0000\n",
            "Batch number: 276, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 277, Training:  Loss: 0.0079, Accuracy: 1.0000\n",
            "Batch number: 278, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 279, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 280, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 281, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 282, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 283, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 284, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 285, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 286, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 287, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 288, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 289, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 290, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 291, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 292, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 293, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 294, Training:  Loss: 0.0295, Accuracy: 0.9688\n",
            "Batch number: 295, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 296, Training:  Loss: 0.0287, Accuracy: 0.9688\n",
            "Batch number: 297, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 298, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 299, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 300, Training:  Loss: 0.1296, Accuracy: 0.9688\n",
            "Batch number: 301, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 302, Training:  Loss: 0.0060, Accuracy: 1.0000\n",
            "Batch number: 303, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 304, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 305, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 306, Training:  Loss: 0.0114, Accuracy: 1.0000\n",
            "Batch number: 307, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 308, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 309, Training:  Loss: 0.1893, Accuracy: 0.9688\n",
            "Batch number: 310, Training:  Loss: 0.0136, Accuracy: 1.0000\n",
            "Batch number: 311, Training:  Loss: 0.0065, Accuracy: 1.0000\n",
            "Batch number: 312, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 313, Training:  Loss: 0.0094, Accuracy: 1.0000\n",
            "Batch number: 314, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 315, Training:  Loss: 0.0056, Accuracy: 1.0000\n",
            "Batch number: 316, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 317, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 318, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 319, Training:  Loss: 0.0162, Accuracy: 1.0000\n",
            "Batch number: 320, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 321, Training:  Loss: 0.0064, Accuracy: 1.0000\n",
            "Batch number: 322, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 323, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 324, Training:  Loss: 0.0095, Accuracy: 1.0000\n",
            "Batch number: 325, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 326, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 327, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 328, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 329, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 330, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 331, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 332, Training:  Loss: 0.0676, Accuracy: 0.9688\n",
            "Batch number: 333, Training:  Loss: 0.0083, Accuracy: 1.0000\n",
            "Batch number: 334, Training:  Loss: 0.0182, Accuracy: 1.0000\n",
            "Batch number: 335, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 336, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 337, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 338, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 339, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 340, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 341, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 342, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 343, Training:  Loss: 0.0090, Accuracy: 1.0000\n",
            "Batch number: 344, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 345, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 346, Training:  Loss: 0.0176, Accuracy: 1.0000\n",
            "Batch number: 347, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 348, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 349, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 350, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 351, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 352, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 353, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 354, Training:  Loss: 0.1712, Accuracy: 0.9688\n",
            "Batch number: 355, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 356, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 357, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 358, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 359, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 360, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 361, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 362, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 363, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 364, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 365, Training:  Loss: 0.1337, Accuracy: 0.9688\n",
            "Batch number: 366, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 367, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 368, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 369, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 370, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 371, Training:  Loss: 0.0989, Accuracy: 0.9688\n",
            "Batch number: 372, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 373, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 374, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 375, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 376, Training:  Loss: 0.0707, Accuracy: 0.9688\n",
            "Batch number: 377, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 378, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 379, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 380, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 381, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 382, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 383, Training:  Loss: 0.0077, Accuracy: 1.0000\n",
            "Batch number: 384, Training:  Loss: 0.0088, Accuracy: 1.0000\n",
            "Batch number: 385, Training:  Loss: 0.0085, Accuracy: 1.0000\n",
            "Batch number: 386, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 387, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 388, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 389, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 390, Training:  Loss: 0.0058, Accuracy: 1.0000\n",
            "Batch number: 391, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 392, Training:  Loss: 0.0060, Accuracy: 1.0000\n",
            "Batch number: 393, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 394, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 395, Training:  Loss: 0.1158, Accuracy: 0.9688\n",
            "Batch number: 396, Training:  Loss: 0.0056, Accuracy: 1.0000\n",
            "Batch number: 397, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 398, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 399, Training:  Loss: 0.0304, Accuracy: 0.9688\n",
            "Batch number: 400, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 401, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 402, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 403, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 404, Training:  Loss: 0.0044, Accuracy: 1.0000\n",
            "Batch number: 405, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 406, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 407, Training:  Loss: 0.0165, Accuracy: 1.0000\n",
            "Batch number: 408, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 409, Training:  Loss: 0.0148, Accuracy: 1.0000\n",
            "Batch number: 410, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 411, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 412, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 413, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 414, Training:  Loss: 0.0059, Accuracy: 1.0000\n",
            "Batch number: 415, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 416, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 417, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 418, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 419, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 420, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 421, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 422, Training:  Loss: 0.0123, Accuracy: 1.0000\n",
            "Batch number: 423, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 424, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 425, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 426, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 427, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 428, Training:  Loss: 0.0210, Accuracy: 1.0000\n",
            "Batch number: 429, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 430, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 431, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 432, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 433, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 434, Training:  Loss: 0.0075, Accuracy: 1.0000\n",
            "Batch number: 435, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 436, Training:  Loss: 0.0075, Accuracy: 1.0000\n",
            "Batch number: 437, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 438, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 439, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 440, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 441, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 442, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 443, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 444, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 445, Training:  Loss: 0.0892, Accuracy: 0.9688\n",
            "Batch number: 446, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 447, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 448, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 449, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 450, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 451, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 452, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 453, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 454, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 455, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 456, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 457, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 458, Training:  Loss: 0.0142, Accuracy: 1.0000\n",
            "Batch number: 459, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 460, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 461, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 462, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 463, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 464, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 465, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 466, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 467, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 468, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 469, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 470, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 471, Training:  Loss: 0.0091, Accuracy: 1.0000\n",
            "Batch number: 472, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 473, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 474, Training:  Loss: 0.0661, Accuracy: 0.9688\n",
            "Batch number: 475, Training:  Loss: 0.0231, Accuracy: 0.9688\n",
            "Batch number: 476, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 477, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 478, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 479, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 480, Training:  Loss: 0.0228, Accuracy: 0.9688\n",
            "Batch number: 481, Training:  Loss: 0.0140, Accuracy: 1.0000\n",
            "Batch number: 482, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 483, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 484, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 485, Training:  Loss: 0.0410, Accuracy: 0.9688\n",
            "Batch number: 486, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 487, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 488, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 489, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 490, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 491, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 492, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 493, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 494, Training:  Loss: 0.2677, Accuracy: 0.9375\n",
            "Batch number: 495, Training:  Loss: 0.1363, Accuracy: 0.9688\n",
            "Batch number: 496, Training:  Loss: 0.0061, Accuracy: 1.0000\n",
            "Batch number: 497, Training:  Loss: 0.0181, Accuracy: 1.0000\n",
            "Batch number: 498, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 499, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 500, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 501, Training:  Loss: 0.0054, Accuracy: 1.0000\n",
            "Batch number: 502, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 503, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 504, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 505, Training:  Loss: 0.0431, Accuracy: 0.9688\n",
            "Batch number: 506, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 507, Training:  Loss: 0.0552, Accuracy: 0.9688\n",
            "Batch number: 508, Training:  Loss: 0.0122, Accuracy: 1.0000\n",
            "Batch number: 509, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 510, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 511, Training:  Loss: 0.0309, Accuracy: 0.9688\n",
            "Batch number: 512, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 513, Training:  Loss: 0.0097, Accuracy: 1.0000\n",
            "Batch number: 514, Training:  Loss: 0.0049, Accuracy: 1.0000\n",
            "Batch number: 515, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 516, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 517, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 518, Training:  Loss: 0.1024, Accuracy: 0.9688\n",
            "Batch number: 519, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 520, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 521, Training:  Loss: 0.0074, Accuracy: 1.0000\n",
            "Batch number: 522, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 523, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 524, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 525, Training:  Loss: 0.0056, Accuracy: 1.0000\n",
            "Batch number: 526, Training:  Loss: 0.0062, Accuracy: 1.0000\n",
            "Batch number: 527, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 528, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 529, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 530, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 531, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 532, Training:  Loss: 0.0097, Accuracy: 1.0000\n",
            "Batch number: 533, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 534, Training:  Loss: 0.0058, Accuracy: 1.0000\n",
            "Batch number: 535, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 536, Training:  Loss: 0.0065, Accuracy: 1.0000\n",
            "Batch number: 537, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 538, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 539, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 540, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 541, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 542, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 543, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 544, Training:  Loss: 0.0125, Accuracy: 1.0000\n",
            "Batch number: 545, Training:  Loss: 0.0150, Accuracy: 1.0000\n",
            "Batch number: 546, Training:  Loss: 0.0079, Accuracy: 1.0000\n",
            "Batch number: 547, Training:  Loss: 0.0195, Accuracy: 1.0000\n",
            "Batch number: 548, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 549, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 550, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 551, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 552, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 553, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 554, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 555, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 556, Training:  Loss: 0.0189, Accuracy: 1.0000\n",
            "Batch number: 557, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 558, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 559, Training:  Loss: 0.2410, Accuracy: 0.9688\n",
            "Batch number: 560, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 561, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 562, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 563, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 564, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 565, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 566, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 567, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 568, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 569, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 570, Training:  Loss: 0.1158, Accuracy: 0.9688\n",
            "Batch number: 571, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 572, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 573, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 574, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 575, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 576, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 577, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 578, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 579, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 580, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 581, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 582, Training:  Loss: 0.0322, Accuracy: 0.9688\n",
            "Batch number: 583, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 584, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 585, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 586, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 587, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 588, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 589, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 590, Training:  Loss: 0.0062, Accuracy: 1.0000\n",
            "Batch number: 591, Training:  Loss: 0.2230, Accuracy: 0.9688\n",
            "Batch number: 592, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 593, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 594, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 595, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 596, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 597, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 598, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 599, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 600, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 601, Training:  Loss: 0.0044, Accuracy: 1.0000\n",
            "Batch number: 602, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 603, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 604, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 605, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 606, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 607, Training:  Loss: 0.0046, Accuracy: 1.0000\n",
            "Batch number: 608, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 609, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 610, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 611, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 612, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 613, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 614, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 615, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 616, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 617, Training:  Loss: 0.0225, Accuracy: 0.9688\n",
            "Batch number: 618, Training:  Loss: 0.0538, Accuracy: 0.9688\n",
            "Batch number: 619, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 620, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 621, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 622, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 623, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 624, Training:  Loss: 0.0810, Accuracy: 0.9688\n",
            "Batch number: 625, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 626, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 627, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 628, Training:  Loss: 0.0044, Accuracy: 1.0000\n",
            "Batch number: 629, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 630, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 631, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 632, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 633, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 634, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 635, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 636, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 637, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 638, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 639, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 640, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 641, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 642, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 643, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 644, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 645, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 646, Training:  Loss: 0.0046, Accuracy: 1.0000\n",
            "Batch number: 647, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 648, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 649, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 650, Training:  Loss: 0.0056, Accuracy: 1.0000\n",
            "Batch number: 651, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 652, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 653, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 654, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 655, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 656, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 657, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 658, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 659, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 660, Training:  Loss: 0.0121, Accuracy: 1.0000\n",
            "Batch number: 661, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 662, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 663, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 664, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 665, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 666, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 667, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 668, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 669, Training:  Loss: 0.1659, Accuracy: 0.9688\n",
            "Batch number: 670, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 671, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 672, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 673, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 674, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 675, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 676, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 677, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 678, Training:  Loss: 0.0050, Accuracy: 1.0000\n",
            "Batch number: 679, Training:  Loss: 0.0105, Accuracy: 1.0000\n",
            "Batch number: 680, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 681, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 682, Training:  Loss: 0.1196, Accuracy: 0.9688\n",
            "Batch number: 683, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 684, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 685, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 686, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 687, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 688, Training:  Loss: 0.1209, Accuracy: 0.9688\n",
            "Batch number: 689, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 690, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 691, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 692, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 693, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 694, Training:  Loss: 0.2170, Accuracy: 0.9688\n",
            "Batch number: 695, Training:  Loss: 0.0078, Accuracy: 1.0000\n",
            "Batch number: 696, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 697, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 698, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 699, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 700, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 701, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 702, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 703, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 704, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 705, Training:  Loss: 0.0053, Accuracy: 1.0000\n",
            "Batch number: 706, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 707, Training:  Loss: 0.0134, Accuracy: 1.0000\n",
            "Batch number: 708, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 709, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 710, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 711, Training:  Loss: 0.0125, Accuracy: 1.0000\n",
            "Batch number: 712, Training:  Loss: 0.0440, Accuracy: 0.9688\n",
            "Batch number: 713, Training:  Loss: 0.0044, Accuracy: 1.0000\n",
            "Batch number: 714, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 715, Training:  Loss: 0.0159, Accuracy: 1.0000\n",
            "Batch number: 716, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 717, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 718, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 719, Training:  Loss: 0.0079, Accuracy: 1.0000\n",
            "Batch number: 720, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 721, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 722, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 723, Training:  Loss: 0.0054, Accuracy: 1.0000\n",
            "Batch number: 724, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 725, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 726, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 727, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 728, Training:  Loss: 0.0060, Accuracy: 1.0000\n",
            "Batch number: 729, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 730, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 731, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 732, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 733, Training:  Loss: 0.0098, Accuracy: 1.0000\n",
            "Batch number: 734, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 735, Training:  Loss: 0.2820, Accuracy: 0.9375\n",
            "Batch number: 736, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 737, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 738, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 739, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 740, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 741, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 742, Training:  Loss: 0.0046, Accuracy: 1.0000\n",
            "Batch number: 743, Training:  Loss: 0.0044, Accuracy: 1.0000\n",
            "Batch number: 744, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 745, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 746, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 747, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 748, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 749, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 750, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 751, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 752, Training:  Loss: 0.0084, Accuracy: 1.0000\n",
            "Batch number: 753, Training:  Loss: 0.0236, Accuracy: 1.0000\n",
            "Batch number: 754, Training:  Loss: 0.1954, Accuracy: 0.9688\n",
            "Batch number: 755, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 756, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 757, Training:  Loss: 0.0062, Accuracy: 1.0000\n",
            "Batch number: 758, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 759, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 760, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 761, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 762, Training:  Loss: 0.0428, Accuracy: 0.9688\n",
            "Batch number: 763, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 764, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 765, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 766, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 767, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 768, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 769, Training:  Loss: 0.0246, Accuracy: 1.0000\n",
            "Batch number: 770, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 771, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 772, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 773, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 774, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 775, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 776, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 777, Training:  Loss: 0.0060, Accuracy: 1.0000\n",
            "Batch number: 778, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 779, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 780, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 781, Training:  Loss: 0.0053, Accuracy: 1.0000\n",
            "Batch number: 782, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 783, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 784, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 785, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 786, Training:  Loss: 0.0061, Accuracy: 1.0000\n",
            "Batch number: 787, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 788, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 789, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 790, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 791, Training:  Loss: 0.0128, Accuracy: 1.0000\n",
            "Batch number: 792, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 793, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 794, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 795, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 796, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 797, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 798, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 799, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 800, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 801, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 802, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 803, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 804, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 805, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 806, Training:  Loss: 0.1736, Accuracy: 0.9375\n",
            "Batch number: 807, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 808, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 809, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 810, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 811, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 812, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 813, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 814, Training:  Loss: 0.0053, Accuracy: 1.0000\n",
            "Batch number: 815, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 816, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 817, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 818, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 819, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 820, Training:  Loss: 0.0131, Accuracy: 1.0000\n",
            "Batch number: 821, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 822, Training:  Loss: 0.2136, Accuracy: 0.9688\n",
            "Batch number: 823, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 824, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 825, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 826, Training:  Loss: 0.0122, Accuracy: 1.0000\n",
            "Batch number: 827, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 828, Training:  Loss: 0.0368, Accuracy: 0.9688\n",
            "Batch number: 829, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 830, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 831, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 832, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 833, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 834, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 835, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 836, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 837, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 838, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 839, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 840, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 841, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 842, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 843, Training:  Loss: 0.1482, Accuracy: 0.9688\n",
            "Batch number: 844, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 845, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 846, Training:  Loss: 0.0044, Accuracy: 1.0000\n",
            "Batch number: 847, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 848, Training:  Loss: 0.0119, Accuracy: 1.0000\n",
            "Batch number: 849, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 850, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 851, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 852, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 853, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 854, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 855, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 856, Training:  Loss: 0.0060, Accuracy: 1.0000\n",
            "Batch number: 857, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 858, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 859, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 860, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 861, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 862, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 863, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Epoch: 37/60\n",
            "Batch number: 000, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 001, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 002, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 003, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 004, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 005, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 006, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 007, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 008, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 009, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 010, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 011, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 012, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 013, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 014, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 015, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 016, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 017, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 018, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 019, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 020, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 021, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 022, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 023, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 024, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 025, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 026, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 027, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 028, Training:  Loss: 0.0063, Accuracy: 1.0000\n",
            "Batch number: 029, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 030, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 031, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 032, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 033, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 034, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 035, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 036, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 037, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 038, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 039, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 040, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 041, Training:  Loss: 0.0881, Accuracy: 0.9688\n",
            "Batch number: 042, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 043, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 044, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 045, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 046, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 047, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 048, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 049, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 050, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 051, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 052, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 053, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 054, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 055, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 056, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 057, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 058, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 059, Training:  Loss: 0.0060, Accuracy: 1.0000\n",
            "Batch number: 060, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 061, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 062, Training:  Loss: 0.1251, Accuracy: 0.9688\n",
            "Batch number: 063, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 064, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 065, Training:  Loss: 0.0758, Accuracy: 0.9688\n",
            "Batch number: 066, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 067, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 068, Training:  Loss: 0.1597, Accuracy: 0.9688\n",
            "Batch number: 069, Training:  Loss: 0.1744, Accuracy: 0.9688\n",
            "Batch number: 070, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 071, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 072, Training:  Loss: 0.0095, Accuracy: 1.0000\n",
            "Batch number: 073, Training:  Loss: 0.0049, Accuracy: 1.0000\n",
            "Batch number: 074, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 075, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 076, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 077, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 078, Training:  Loss: 0.0105, Accuracy: 1.0000\n",
            "Batch number: 079, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 080, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 081, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 082, Training:  Loss: 0.0098, Accuracy: 1.0000\n",
            "Batch number: 083, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 084, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 085, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 086, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 087, Training:  Loss: 0.0300, Accuracy: 0.9688\n",
            "Batch number: 088, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 089, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 090, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 091, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 092, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 093, Training:  Loss: 0.0074, Accuracy: 1.0000\n",
            "Batch number: 094, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 095, Training:  Loss: 0.0061, Accuracy: 1.0000\n",
            "Batch number: 096, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 097, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 098, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 099, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 100, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 101, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 102, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 103, Training:  Loss: 0.0054, Accuracy: 1.0000\n",
            "Batch number: 104, Training:  Loss: 0.1453, Accuracy: 0.9688\n",
            "Batch number: 105, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 106, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 107, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 108, Training:  Loss: 0.0085, Accuracy: 1.0000\n",
            "Batch number: 109, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 110, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 111, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 112, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 113, Training:  Loss: 0.0062, Accuracy: 1.0000\n",
            "Batch number: 114, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 115, Training:  Loss: 0.0086, Accuracy: 1.0000\n",
            "Batch number: 116, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 117, Training:  Loss: 0.0053, Accuracy: 1.0000\n",
            "Batch number: 118, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 119, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 120, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 121, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 122, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 123, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 124, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 125, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 126, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 127, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 128, Training:  Loss: 0.0648, Accuracy: 0.9688\n",
            "Batch number: 129, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 130, Training:  Loss: 0.1202, Accuracy: 0.9688\n",
            "Batch number: 131, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 132, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 133, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 134, Training:  Loss: 0.0055, Accuracy: 1.0000\n",
            "Batch number: 135, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 136, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 137, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 138, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 139, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 140, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 141, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 142, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 143, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 144, Training:  Loss: 0.0532, Accuracy: 0.9688\n",
            "Batch number: 145, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 146, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 147, Training:  Loss: 0.0113, Accuracy: 1.0000\n",
            "Batch number: 148, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 149, Training:  Loss: 0.0046, Accuracy: 1.0000\n",
            "Batch number: 150, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 151, Training:  Loss: 0.0042, Accuracy: 1.0000\n",
            "Batch number: 152, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 153, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 154, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 155, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 156, Training:  Loss: 0.0063, Accuracy: 1.0000\n",
            "Batch number: 157, Training:  Loss: 0.0079, Accuracy: 1.0000\n",
            "Batch number: 158, Training:  Loss: 0.0051, Accuracy: 1.0000\n",
            "Batch number: 159, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 160, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 161, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 162, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 163, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 164, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 165, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 166, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 167, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 168, Training:  Loss: 0.0068, Accuracy: 1.0000\n",
            "Batch number: 169, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 170, Training:  Loss: 0.2050, Accuracy: 0.9688\n",
            "Batch number: 171, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 172, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 173, Training:  Loss: 0.1168, Accuracy: 0.9688\n",
            "Batch number: 174, Training:  Loss: 0.0044, Accuracy: 1.0000\n",
            "Batch number: 175, Training:  Loss: 0.0047, Accuracy: 1.0000\n",
            "Batch number: 176, Training:  Loss: 0.0078, Accuracy: 1.0000\n",
            "Batch number: 177, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 178, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 179, Training:  Loss: 0.1346, Accuracy: 0.9688\n",
            "Batch number: 180, Training:  Loss: 0.0049, Accuracy: 1.0000\n",
            "Batch number: 181, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 182, Training:  Loss: 0.0172, Accuracy: 1.0000\n",
            "Batch number: 183, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 184, Training:  Loss: 0.0163, Accuracy: 1.0000\n",
            "Batch number: 185, Training:  Loss: 0.0059, Accuracy: 1.0000\n",
            "Batch number: 186, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 187, Training:  Loss: 0.0329, Accuracy: 0.9688\n",
            "Batch number: 188, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 189, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 190, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 191, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 192, Training:  Loss: 0.0085, Accuracy: 1.0000\n",
            "Batch number: 193, Training:  Loss: 0.0143, Accuracy: 1.0000\n",
            "Batch number: 194, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 195, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 196, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 197, Training:  Loss: 0.0132, Accuracy: 1.0000\n",
            "Batch number: 198, Training:  Loss: 0.0142, Accuracy: 1.0000\n",
            "Batch number: 199, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 200, Training:  Loss: 0.0046, Accuracy: 1.0000\n",
            "Batch number: 201, Training:  Loss: 0.0094, Accuracy: 1.0000\n",
            "Batch number: 202, Training:  Loss: 0.0248, Accuracy: 1.0000\n",
            "Batch number: 203, Training:  Loss: 0.0091, Accuracy: 1.0000\n",
            "Batch number: 204, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 205, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 206, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 207, Training:  Loss: 0.0068, Accuracy: 1.0000\n",
            "Batch number: 208, Training:  Loss: 0.1049, Accuracy: 0.9688\n",
            "Batch number: 209, Training:  Loss: 0.0092, Accuracy: 1.0000\n",
            "Batch number: 210, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 211, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 212, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 213, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 214, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 215, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 216, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 217, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 218, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 219, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 220, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 221, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 222, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 223, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 224, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 225, Training:  Loss: 0.0156, Accuracy: 1.0000\n",
            "Batch number: 226, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 227, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 228, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 229, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 230, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 231, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 232, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 233, Training:  Loss: 0.0601, Accuracy: 0.9688\n",
            "Batch number: 234, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 235, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 236, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 237, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 238, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 239, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 240, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 241, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 242, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 243, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 244, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 245, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 246, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 247, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 248, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 249, Training:  Loss: 0.0085, Accuracy: 1.0000\n",
            "Batch number: 250, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 251, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 252, Training:  Loss: 0.2166, Accuracy: 0.9688\n",
            "Batch number: 253, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 254, Training:  Loss: 0.0046, Accuracy: 1.0000\n",
            "Batch number: 255, Training:  Loss: 0.1738, Accuracy: 0.9688\n",
            "Batch number: 256, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 257, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 258, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 259, Training:  Loss: 0.0070, Accuracy: 1.0000\n",
            "Batch number: 260, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 261, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 262, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 263, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 264, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 265, Training:  Loss: 0.1442, Accuracy: 0.9688\n",
            "Batch number: 266, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 267, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 268, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 269, Training:  Loss: 0.0443, Accuracy: 0.9688\n",
            "Batch number: 270, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 271, Training:  Loss: 0.0061, Accuracy: 1.0000\n",
            "Batch number: 272, Training:  Loss: 0.0158, Accuracy: 1.0000\n",
            "Batch number: 273, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 274, Training:  Loss: 0.0054, Accuracy: 1.0000\n",
            "Batch number: 275, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 276, Training:  Loss: 0.0042, Accuracy: 1.0000\n",
            "Batch number: 277, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 278, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 279, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 280, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 281, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 282, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 283, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 284, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 285, Training:  Loss: 0.0060, Accuracy: 1.0000\n",
            "Batch number: 286, Training:  Loss: 0.0124, Accuracy: 1.0000\n",
            "Batch number: 287, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 288, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 289, Training:  Loss: 0.0070, Accuracy: 1.0000\n",
            "Batch number: 290, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 291, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 292, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 293, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 294, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 295, Training:  Loss: 0.0063, Accuracy: 1.0000\n",
            "Batch number: 296, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 297, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 298, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 299, Training:  Loss: 0.0314, Accuracy: 0.9688\n",
            "Batch number: 300, Training:  Loss: 0.0697, Accuracy: 0.9688\n",
            "Batch number: 301, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 302, Training:  Loss: 0.0946, Accuracy: 0.9688\n",
            "Batch number: 303, Training:  Loss: 0.1925, Accuracy: 0.9688\n",
            "Batch number: 304, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 305, Training:  Loss: 0.0042, Accuracy: 1.0000\n",
            "Batch number: 306, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 307, Training:  Loss: 0.0107, Accuracy: 1.0000\n",
            "Batch number: 308, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 309, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 310, Training:  Loss: 0.0060, Accuracy: 1.0000\n",
            "Batch number: 311, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 312, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 313, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 314, Training:  Loss: 0.0076, Accuracy: 1.0000\n",
            "Batch number: 315, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 316, Training:  Loss: 0.0072, Accuracy: 1.0000\n",
            "Batch number: 317, Training:  Loss: 0.0093, Accuracy: 1.0000\n",
            "Batch number: 318, Training:  Loss: 0.0106, Accuracy: 1.0000\n",
            "Batch number: 319, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 320, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 321, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 322, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 323, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 324, Training:  Loss: 0.0055, Accuracy: 1.0000\n",
            "Batch number: 325, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 326, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 327, Training:  Loss: 0.0249, Accuracy: 1.0000\n",
            "Batch number: 328, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 329, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 330, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 331, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 332, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 333, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 334, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 335, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 336, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 337, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 338, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 339, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 340, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 341, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 342, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 343, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 344, Training:  Loss: 0.0085, Accuracy: 1.0000\n",
            "Batch number: 345, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 346, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 347, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 348, Training:  Loss: 0.0994, Accuracy: 0.9375\n",
            "Batch number: 349, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 350, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 351, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 352, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 353, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 354, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 355, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 356, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 357, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 358, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 359, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 360, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 361, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 362, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 363, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 364, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 365, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 366, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 367, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 368, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 369, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 370, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 371, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 372, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 373, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 374, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 375, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 376, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 377, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 378, Training:  Loss: 0.1789, Accuracy: 0.9688\n",
            "Batch number: 379, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 380, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 381, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 382, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 383, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 384, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 385, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 386, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 387, Training:  Loss: 0.0240, Accuracy: 1.0000\n",
            "Batch number: 388, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 389, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 390, Training:  Loss: 0.0099, Accuracy: 1.0000\n",
            "Batch number: 391, Training:  Loss: 0.0460, Accuracy: 0.9688\n",
            "Batch number: 392, Training:  Loss: 0.0060, Accuracy: 1.0000\n",
            "Batch number: 393, Training:  Loss: 0.0062, Accuracy: 1.0000\n",
            "Batch number: 394, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 395, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 396, Training:  Loss: 0.0077, Accuracy: 1.0000\n",
            "Batch number: 397, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 398, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 399, Training:  Loss: 0.0074, Accuracy: 1.0000\n",
            "Batch number: 400, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 401, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 402, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 403, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 404, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 405, Training:  Loss: 0.0080, Accuracy: 1.0000\n",
            "Batch number: 406, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 407, Training:  Loss: 0.0152, Accuracy: 1.0000\n",
            "Batch number: 408, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 409, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 410, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 411, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 412, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 413, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 414, Training:  Loss: 0.0292, Accuracy: 0.9688\n",
            "Batch number: 415, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 416, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 417, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 418, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 419, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 420, Training:  Loss: 0.0056, Accuracy: 1.0000\n",
            "Batch number: 421, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 422, Training:  Loss: 0.0074, Accuracy: 1.0000\n",
            "Batch number: 423, Training:  Loss: 0.1757, Accuracy: 0.9688\n",
            "Batch number: 424, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 425, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 426, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 427, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 428, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 429, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 430, Training:  Loss: 0.0051, Accuracy: 1.0000\n",
            "Batch number: 431, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 432, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 433, Training:  Loss: 0.0196, Accuracy: 1.0000\n",
            "Batch number: 434, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 435, Training:  Loss: 0.0633, Accuracy: 0.9688\n",
            "Batch number: 436, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 437, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 438, Training:  Loss: 0.0063, Accuracy: 1.0000\n",
            "Batch number: 439, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 440, Training:  Loss: 0.0105, Accuracy: 1.0000\n",
            "Batch number: 441, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 442, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 443, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 444, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 445, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 446, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 447, Training:  Loss: 0.0054, Accuracy: 1.0000\n",
            "Batch number: 448, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 449, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 450, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 451, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 452, Training:  Loss: 0.1325, Accuracy: 0.9688\n",
            "Batch number: 453, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 454, Training:  Loss: 0.0169, Accuracy: 1.0000\n",
            "Batch number: 455, Training:  Loss: 0.0226, Accuracy: 0.9688\n",
            "Batch number: 456, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 457, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 458, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 459, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 460, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 461, Training:  Loss: 0.0067, Accuracy: 1.0000\n",
            "Batch number: 462, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 463, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 464, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 465, Training:  Loss: 0.0057, Accuracy: 1.0000\n",
            "Batch number: 466, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 467, Training:  Loss: 0.1445, Accuracy: 0.9375\n",
            "Batch number: 468, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 469, Training:  Loss: 0.0072, Accuracy: 1.0000\n",
            "Batch number: 470, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 471, Training:  Loss: 0.0057, Accuracy: 1.0000\n",
            "Batch number: 472, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 473, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 474, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 475, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 476, Training:  Loss: 0.1158, Accuracy: 0.9688\n",
            "Batch number: 477, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 478, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 479, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 480, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 481, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 482, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 483, Training:  Loss: 0.0060, Accuracy: 1.0000\n",
            "Batch number: 484, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 485, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 486, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 487, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 488, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 489, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 490, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 491, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 492, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 493, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 494, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 495, Training:  Loss: 0.0044, Accuracy: 1.0000\n",
            "Batch number: 496, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 497, Training:  Loss: 0.1337, Accuracy: 0.9688\n",
            "Batch number: 498, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 499, Training:  Loss: 0.0429, Accuracy: 0.9688\n",
            "Batch number: 500, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 501, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 502, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 503, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 504, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 505, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 506, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 507, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 508, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 509, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 510, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 511, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 512, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 513, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 514, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 515, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 516, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 517, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 518, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 519, Training:  Loss: 0.1219, Accuracy: 0.9688\n",
            "Batch number: 520, Training:  Loss: 0.0166, Accuracy: 1.0000\n",
            "Batch number: 521, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 522, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 523, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 524, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 525, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 526, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 527, Training:  Loss: 0.1204, Accuracy: 0.9688\n",
            "Batch number: 528, Training:  Loss: 0.0160, Accuracy: 1.0000\n",
            "Batch number: 529, Training:  Loss: 0.0051, Accuracy: 1.0000\n",
            "Batch number: 530, Training:  Loss: 0.1014, Accuracy: 0.9688\n",
            "Batch number: 531, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 532, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 533, Training:  Loss: 0.0057, Accuracy: 1.0000\n",
            "Batch number: 534, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 535, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 536, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 537, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 538, Training:  Loss: 0.1027, Accuracy: 0.9688\n",
            "Batch number: 539, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 540, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 541, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 542, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 543, Training:  Loss: 0.0072, Accuracy: 1.0000\n",
            "Batch number: 544, Training:  Loss: 0.0050, Accuracy: 1.0000\n",
            "Batch number: 545, Training:  Loss: 0.0098, Accuracy: 1.0000\n",
            "Batch number: 546, Training:  Loss: 0.0056, Accuracy: 1.0000\n",
            "Batch number: 547, Training:  Loss: 0.0064, Accuracy: 1.0000\n",
            "Batch number: 548, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 549, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 550, Training:  Loss: 0.0322, Accuracy: 0.9688\n",
            "Batch number: 551, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 552, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 553, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 554, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 555, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 556, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 557, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 558, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 559, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 560, Training:  Loss: 0.0074, Accuracy: 1.0000\n",
            "Batch number: 561, Training:  Loss: 0.0112, Accuracy: 1.0000\n",
            "Batch number: 562, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 563, Training:  Loss: 0.0101, Accuracy: 1.0000\n",
            "Batch number: 564, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 565, Training:  Loss: 0.0051, Accuracy: 1.0000\n",
            "Batch number: 566, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 567, Training:  Loss: 0.0916, Accuracy: 0.9688\n",
            "Batch number: 568, Training:  Loss: 0.0866, Accuracy: 0.9688\n",
            "Batch number: 569, Training:  Loss: 0.0054, Accuracy: 1.0000\n",
            "Batch number: 570, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 571, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 572, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 573, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 574, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 575, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 576, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 577, Training:  Loss: 0.0076, Accuracy: 1.0000\n",
            "Batch number: 578, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 579, Training:  Loss: 0.0194, Accuracy: 1.0000\n",
            "Batch number: 580, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 581, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 582, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 583, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 584, Training:  Loss: 0.0046, Accuracy: 1.0000\n",
            "Batch number: 585, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 586, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 587, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 588, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 589, Training:  Loss: 0.0154, Accuracy: 1.0000\n",
            "Batch number: 590, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 591, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 592, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 593, Training:  Loss: 0.0067, Accuracy: 1.0000\n",
            "Batch number: 594, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 595, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 596, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 597, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 598, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 599, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 600, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 601, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 602, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 603, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 604, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 605, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 606, Training:  Loss: 0.0095, Accuracy: 1.0000\n",
            "Batch number: 607, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 608, Training:  Loss: 0.0293, Accuracy: 0.9688\n",
            "Batch number: 609, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 610, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 611, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 612, Training:  Loss: 0.0281, Accuracy: 0.9688\n",
            "Batch number: 613, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 614, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 615, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 616, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 617, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 618, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 619, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 620, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 621, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 622, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 623, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 624, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 625, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 626, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 627, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 628, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 629, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 630, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 631, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 632, Training:  Loss: 0.0096, Accuracy: 1.0000\n",
            "Batch number: 633, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 634, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 635, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 636, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 637, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 638, Training:  Loss: 0.0092, Accuracy: 1.0000\n",
            "Batch number: 639, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 640, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 641, Training:  Loss: 0.1389, Accuracy: 0.9688\n",
            "Batch number: 642, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 643, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 644, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 645, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 646, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 647, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 648, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 649, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 650, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 651, Training:  Loss: 0.1393, Accuracy: 0.9688\n",
            "Batch number: 652, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 653, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 654, Training:  Loss: 0.0781, Accuracy: 0.9688\n",
            "Batch number: 655, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 656, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 657, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 658, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 659, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 660, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 661, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 662, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 663, Training:  Loss: 0.0319, Accuracy: 1.0000\n",
            "Batch number: 664, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 665, Training:  Loss: 0.0113, Accuracy: 1.0000\n",
            "Batch number: 666, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 667, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 668, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 669, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 670, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 671, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 672, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 673, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 674, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 675, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 676, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 677, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 678, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 679, Training:  Loss: 0.0101, Accuracy: 1.0000\n",
            "Batch number: 680, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 681, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 682, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 683, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 684, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 685, Training:  Loss: 0.0188, Accuracy: 1.0000\n",
            "Batch number: 686, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 687, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 688, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 689, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 690, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 691, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 692, Training:  Loss: 0.0893, Accuracy: 0.9688\n",
            "Batch number: 693, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 694, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 695, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 696, Training:  Loss: 0.1394, Accuracy: 0.9688\n",
            "Batch number: 697, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 698, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 699, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 700, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 701, Training:  Loss: 0.0049, Accuracy: 1.0000\n",
            "Batch number: 702, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 703, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 704, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 705, Training:  Loss: 0.0244, Accuracy: 0.9688\n",
            "Batch number: 706, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 707, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 708, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 709, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 710, Training:  Loss: 0.0062, Accuracy: 1.0000\n",
            "Batch number: 711, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 712, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 713, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 714, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 715, Training:  Loss: 0.0177, Accuracy: 1.0000\n",
            "Batch number: 716, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 717, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 718, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 719, Training:  Loss: 0.0139, Accuracy: 1.0000\n",
            "Batch number: 720, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 721, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 722, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 723, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 724, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 725, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 726, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 727, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 728, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 729, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 730, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 731, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 732, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 733, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 734, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 735, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 736, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 737, Training:  Loss: 0.0261, Accuracy: 0.9688\n",
            "Batch number: 738, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 739, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 740, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 741, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 742, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 743, Training:  Loss: 0.0106, Accuracy: 1.0000\n",
            "Batch number: 744, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 745, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 746, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 747, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 748, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 749, Training:  Loss: 0.0118, Accuracy: 1.0000\n",
            "Batch number: 750, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 751, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 752, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 753, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 754, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 755, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 756, Training:  Loss: 0.0047, Accuracy: 1.0000\n",
            "Batch number: 757, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 758, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 759, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 760, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 761, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 762, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 763, Training:  Loss: 0.0298, Accuracy: 0.9688\n",
            "Batch number: 764, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 765, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 766, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 767, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 768, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 769, Training:  Loss: 0.0095, Accuracy: 1.0000\n",
            "Batch number: 770, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 771, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 772, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 773, Training:  Loss: 0.0062, Accuracy: 1.0000\n",
            "Batch number: 774, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 775, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 776, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 777, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 778, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 779, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 780, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 781, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 782, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 783, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 784, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 785, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 786, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 787, Training:  Loss: 0.0663, Accuracy: 0.9688\n",
            "Batch number: 788, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 789, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 790, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 791, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 792, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 793, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 794, Training:  Loss: 0.0044, Accuracy: 1.0000\n",
            "Batch number: 795, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 796, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 797, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 798, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 799, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 800, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 801, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 802, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 803, Training:  Loss: 0.0547, Accuracy: 0.9688\n",
            "Batch number: 804, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 805, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 806, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 807, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 808, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 809, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 810, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 811, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 812, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 813, Training:  Loss: 0.0051, Accuracy: 1.0000\n",
            "Batch number: 814, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 815, Training:  Loss: 0.0068, Accuracy: 1.0000\n",
            "Batch number: 816, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 817, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 818, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 819, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 820, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 821, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 822, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 823, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 824, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 825, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 826, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 827, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 828, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 829, Training:  Loss: 0.0080, Accuracy: 1.0000\n",
            "Batch number: 830, Training:  Loss: 0.0505, Accuracy: 0.9688\n",
            "Batch number: 831, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 832, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 833, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 834, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 835, Training:  Loss: 0.0065, Accuracy: 1.0000\n",
            "Batch number: 836, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 837, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 838, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 839, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 840, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 841, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 842, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 843, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 844, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 845, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 846, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 847, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 848, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 849, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 850, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 851, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 852, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 853, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 854, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 855, Training:  Loss: 0.1940, Accuracy: 0.9688\n",
            "Batch number: 856, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 857, Training:  Loss: 0.1121, Accuracy: 0.9688\n",
            "Batch number: 858, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 859, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 860, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 861, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 862, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 863, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Epoch: 38/60\n",
            "Batch number: 000, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 001, Training:  Loss: 0.0229, Accuracy: 0.9688\n",
            "Batch number: 002, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 003, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 004, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 005, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 006, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 007, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 008, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 009, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 010, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 011, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 012, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 013, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 014, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 015, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 016, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 017, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 018, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 019, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 020, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 021, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 022, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 023, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 024, Training:  Loss: 0.0061, Accuracy: 1.0000\n",
            "Batch number: 025, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 026, Training:  Loss: 0.1275, Accuracy: 0.9688\n",
            "Batch number: 027, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 028, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 029, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 030, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 031, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 032, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 033, Training:  Loss: 0.0097, Accuracy: 1.0000\n",
            "Batch number: 034, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 035, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 036, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 037, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 038, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 039, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 040, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 041, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 042, Training:  Loss: 0.1313, Accuracy: 0.9688\n",
            "Batch number: 043, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 044, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 045, Training:  Loss: 0.0085, Accuracy: 1.0000\n",
            "Batch number: 046, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 047, Training:  Loss: 0.0058, Accuracy: 1.0000\n",
            "Batch number: 048, Training:  Loss: 0.0183, Accuracy: 1.0000\n",
            "Batch number: 049, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 050, Training:  Loss: 0.1442, Accuracy: 0.9375\n",
            "Batch number: 051, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 052, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 053, Training:  Loss: 0.0076, Accuracy: 1.0000\n",
            "Batch number: 054, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 055, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 056, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 057, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 058, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 059, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 060, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 061, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 062, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 063, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 064, Training:  Loss: 0.0068, Accuracy: 1.0000\n",
            "Batch number: 065, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 066, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 067, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 068, Training:  Loss: 0.0056, Accuracy: 1.0000\n",
            "Batch number: 069, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 070, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 071, Training:  Loss: 0.0055, Accuracy: 1.0000\n",
            "Batch number: 072, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 073, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 074, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 075, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 076, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 077, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 078, Training:  Loss: 0.0155, Accuracy: 1.0000\n",
            "Batch number: 079, Training:  Loss: 0.2438, Accuracy: 0.9375\n",
            "Batch number: 080, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 081, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 082, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 083, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 084, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 085, Training:  Loss: 0.0137, Accuracy: 1.0000\n",
            "Batch number: 086, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 087, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 088, Training:  Loss: 0.0299, Accuracy: 0.9688\n",
            "Batch number: 089, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 090, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 091, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 092, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 093, Training:  Loss: 0.1211, Accuracy: 0.9688\n",
            "Batch number: 094, Training:  Loss: 0.0424, Accuracy: 0.9688\n",
            "Batch number: 095, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 096, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 097, Training:  Loss: 0.0076, Accuracy: 1.0000\n",
            "Batch number: 098, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 099, Training:  Loss: 0.0084, Accuracy: 1.0000\n",
            "Batch number: 100, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 101, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 102, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 103, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 104, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 105, Training:  Loss: 0.0119, Accuracy: 1.0000\n",
            "Batch number: 106, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 107, Training:  Loss: 0.0086, Accuracy: 1.0000\n",
            "Batch number: 108, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 109, Training:  Loss: 0.0069, Accuracy: 1.0000\n",
            "Batch number: 110, Training:  Loss: 0.0171, Accuracy: 1.0000\n",
            "Batch number: 111, Training:  Loss: 0.0067, Accuracy: 1.0000\n",
            "Batch number: 112, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 113, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 114, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 115, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 116, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 117, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 118, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 119, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 120, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 121, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 122, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 123, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 124, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 125, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 126, Training:  Loss: 0.0074, Accuracy: 1.0000\n",
            "Batch number: 127, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 128, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 129, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 130, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 131, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 132, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 133, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 134, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 135, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 136, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 137, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 138, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 139, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 140, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 141, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 142, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 143, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 144, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 145, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 146, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 147, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 148, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 149, Training:  Loss: 0.0155, Accuracy: 1.0000\n",
            "Batch number: 150, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 151, Training:  Loss: 0.0112, Accuracy: 1.0000\n",
            "Batch number: 152, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 153, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 154, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 155, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 156, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 157, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 158, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 159, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 160, Training:  Loss: 0.0109, Accuracy: 1.0000\n",
            "Batch number: 161, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 162, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 163, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 164, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 165, Training:  Loss: 0.1886, Accuracy: 0.9688\n",
            "Batch number: 166, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 167, Training:  Loss: 0.1677, Accuracy: 0.9688\n",
            "Batch number: 168, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 169, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 170, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 171, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 172, Training:  Loss: 0.0127, Accuracy: 1.0000\n",
            "Batch number: 173, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 174, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 175, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 176, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 177, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 178, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 179, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 180, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 181, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 182, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 183, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 184, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 185, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 186, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 187, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 188, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 189, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 190, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 191, Training:  Loss: 0.0066, Accuracy: 1.0000\n",
            "Batch number: 192, Training:  Loss: 0.0115, Accuracy: 1.0000\n",
            "Batch number: 193, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 194, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 195, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 196, Training:  Loss: 0.0078, Accuracy: 1.0000\n",
            "Batch number: 197, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 198, Training:  Loss: 0.0945, Accuracy: 0.9688\n",
            "Batch number: 199, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 200, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 201, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 202, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 203, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 204, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 205, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 206, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 207, Training:  Loss: 0.0190, Accuracy: 1.0000\n",
            "Batch number: 208, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 209, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 210, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 211, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 212, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 213, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 214, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 215, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 216, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 217, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 218, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 219, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 220, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 221, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 222, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 223, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 224, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 225, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 226, Training:  Loss: 0.0960, Accuracy: 0.9688\n",
            "Batch number: 227, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 228, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 229, Training:  Loss: 0.0358, Accuracy: 0.9688\n",
            "Batch number: 230, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 231, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 232, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 233, Training:  Loss: 0.0115, Accuracy: 1.0000\n",
            "Batch number: 234, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 235, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 236, Training:  Loss: 0.1528, Accuracy: 0.9688\n",
            "Batch number: 237, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 238, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 239, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 240, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 241, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 242, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 243, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 244, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 245, Training:  Loss: 0.1172, Accuracy: 0.9688\n",
            "Batch number: 246, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 247, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 248, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 249, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 250, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 251, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 252, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 253, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 254, Training:  Loss: 0.0087, Accuracy: 1.0000\n",
            "Batch number: 255, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 256, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 257, Training:  Loss: 0.0042, Accuracy: 1.0000\n",
            "Batch number: 258, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 259, Training:  Loss: 0.0127, Accuracy: 1.0000\n",
            "Batch number: 260, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 261, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 262, Training:  Loss: 0.0095, Accuracy: 1.0000\n",
            "Batch number: 263, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 264, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 265, Training:  Loss: 0.0065, Accuracy: 1.0000\n",
            "Batch number: 266, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 267, Training:  Loss: 0.0346, Accuracy: 0.9688\n",
            "Batch number: 268, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 269, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 270, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 271, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 272, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 273, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 274, Training:  Loss: 0.0322, Accuracy: 0.9688\n",
            "Batch number: 275, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 276, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 277, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 278, Training:  Loss: 0.0161, Accuracy: 1.0000\n",
            "Batch number: 279, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 280, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 281, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 282, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 283, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 284, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 285, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 286, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 287, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 288, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 289, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 290, Training:  Loss: 0.0084, Accuracy: 1.0000\n",
            "Batch number: 291, Training:  Loss: 0.0075, Accuracy: 1.0000\n",
            "Batch number: 292, Training:  Loss: 0.0212, Accuracy: 1.0000\n",
            "Batch number: 293, Training:  Loss: 0.0046, Accuracy: 1.0000\n",
            "Batch number: 294, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 295, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 296, Training:  Loss: 0.0061, Accuracy: 1.0000\n",
            "Batch number: 297, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 298, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 299, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 300, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 301, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 302, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 303, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 304, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 305, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 306, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 307, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 308, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 309, Training:  Loss: 0.0047, Accuracy: 1.0000\n",
            "Batch number: 310, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 311, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 312, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 313, Training:  Loss: 0.0590, Accuracy: 0.9688\n",
            "Batch number: 314, Training:  Loss: 0.0283, Accuracy: 0.9688\n",
            "Batch number: 315, Training:  Loss: 0.0096, Accuracy: 1.0000\n",
            "Batch number: 316, Training:  Loss: 0.0208, Accuracy: 1.0000\n",
            "Batch number: 317, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 318, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 319, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 320, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 321, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 322, Training:  Loss: 0.1314, Accuracy: 0.9688\n",
            "Batch number: 323, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 324, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 325, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 326, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 327, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 328, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 329, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 330, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 331, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 332, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 333, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 334, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 335, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 336, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 337, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 338, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 339, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 340, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 341, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 342, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 343, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 344, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 345, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 346, Training:  Loss: 0.0174, Accuracy: 1.0000\n",
            "Batch number: 347, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 348, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 349, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 350, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 351, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 352, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 353, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 354, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 355, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 356, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 357, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 358, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 359, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 360, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 361, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 362, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 363, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 364, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 365, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 366, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 367, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 368, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 369, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 370, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 371, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 372, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 373, Training:  Loss: 0.0135, Accuracy: 1.0000\n",
            "Batch number: 374, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 375, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 376, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 377, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 378, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 379, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 380, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 381, Training:  Loss: 0.1119, Accuracy: 0.9688\n",
            "Batch number: 382, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 383, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 384, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 385, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 386, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 387, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 388, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 389, Training:  Loss: 0.0108, Accuracy: 1.0000\n",
            "Batch number: 390, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 391, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 392, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 393, Training:  Loss: 0.1726, Accuracy: 0.9688\n",
            "Batch number: 394, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 395, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 396, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 397, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 398, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 399, Training:  Loss: 0.0302, Accuracy: 0.9688\n",
            "Batch number: 400, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 401, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 402, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 403, Training:  Loss: 0.0625, Accuracy: 0.9688\n",
            "Batch number: 404, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 405, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 406, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 407, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 408, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 409, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 410, Training:  Loss: 0.0146, Accuracy: 1.0000\n",
            "Batch number: 411, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 412, Training:  Loss: 0.2472, Accuracy: 0.9688\n",
            "Batch number: 413, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 414, Training:  Loss: 0.0083, Accuracy: 1.0000\n",
            "Batch number: 415, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 416, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 417, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 418, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 419, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 420, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 421, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 422, Training:  Loss: 0.1446, Accuracy: 0.9688\n",
            "Batch number: 423, Training:  Loss: 0.0051, Accuracy: 1.0000\n",
            "Batch number: 424, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 425, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 426, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 427, Training:  Loss: 0.0055, Accuracy: 1.0000\n",
            "Batch number: 428, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 429, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 430, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 431, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 432, Training:  Loss: 0.0047, Accuracy: 1.0000\n",
            "Batch number: 433, Training:  Loss: 0.0047, Accuracy: 1.0000\n",
            "Batch number: 434, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 435, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 436, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 437, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 438, Training:  Loss: 0.0109, Accuracy: 1.0000\n",
            "Batch number: 439, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 440, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 441, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 442, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 443, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 444, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 445, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 446, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 447, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 448, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 449, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 450, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 451, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 452, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 453, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 454, Training:  Loss: 0.1408, Accuracy: 0.9688\n",
            "Batch number: 455, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 456, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 457, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 458, Training:  Loss: 0.0145, Accuracy: 1.0000\n",
            "Batch number: 459, Training:  Loss: 0.1274, Accuracy: 0.9688\n",
            "Batch number: 460, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 461, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 462, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 463, Training:  Loss: 0.0770, Accuracy: 0.9688\n",
            "Batch number: 464, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 465, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 466, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 467, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 468, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 469, Training:  Loss: 0.0449, Accuracy: 0.9688\n",
            "Batch number: 470, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 471, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 472, Training:  Loss: 0.0107, Accuracy: 1.0000\n",
            "Batch number: 473, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 474, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 475, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 476, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 477, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 478, Training:  Loss: 0.0049, Accuracy: 1.0000\n",
            "Batch number: 479, Training:  Loss: 0.0084, Accuracy: 1.0000\n",
            "Batch number: 480, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 481, Training:  Loss: 0.0214, Accuracy: 1.0000\n",
            "Batch number: 482, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 483, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 484, Training:  Loss: 0.0055, Accuracy: 1.0000\n",
            "Batch number: 485, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 486, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 487, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 488, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 489, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 490, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 491, Training:  Loss: 0.0175, Accuracy: 1.0000\n",
            "Batch number: 492, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 493, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 494, Training:  Loss: 0.1239, Accuracy: 0.9688\n",
            "Batch number: 495, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 496, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 497, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 498, Training:  Loss: 0.0824, Accuracy: 0.9688\n",
            "Batch number: 499, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 500, Training:  Loss: 0.2284, Accuracy: 0.9688\n",
            "Batch number: 501, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 502, Training:  Loss: 0.0226, Accuracy: 1.0000\n",
            "Batch number: 503, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 504, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 505, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 506, Training:  Loss: 0.0049, Accuracy: 1.0000\n",
            "Batch number: 507, Training:  Loss: 0.0248, Accuracy: 1.0000\n",
            "Batch number: 508, Training:  Loss: 0.0797, Accuracy: 0.9688\n",
            "Batch number: 509, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 510, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 511, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 512, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 513, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 514, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 515, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 516, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 517, Training:  Loss: 0.0042, Accuracy: 1.0000\n",
            "Batch number: 518, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 519, Training:  Loss: 0.0126, Accuracy: 1.0000\n",
            "Batch number: 520, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 521, Training:  Loss: 0.0256, Accuracy: 0.9688\n",
            "Batch number: 522, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 523, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 524, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 525, Training:  Loss: 0.0473, Accuracy: 0.9688\n",
            "Batch number: 526, Training:  Loss: 0.0122, Accuracy: 1.0000\n",
            "Batch number: 527, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 528, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 529, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 530, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 531, Training:  Loss: 0.0917, Accuracy: 0.9688\n",
            "Batch number: 532, Training:  Loss: 0.1941, Accuracy: 0.9688\n",
            "Batch number: 533, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 534, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 535, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 536, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 537, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 538, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 539, Training:  Loss: 0.0050, Accuracy: 1.0000\n",
            "Batch number: 540, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 541, Training:  Loss: 0.0076, Accuracy: 1.0000\n",
            "Batch number: 542, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 543, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 544, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 545, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 546, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 547, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 548, Training:  Loss: 0.0113, Accuracy: 1.0000\n",
            "Batch number: 549, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 550, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 551, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 552, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 553, Training:  Loss: 0.0394, Accuracy: 0.9688\n",
            "Batch number: 554, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 555, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 556, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 557, Training:  Loss: 0.0061, Accuracy: 1.0000\n",
            "Batch number: 558, Training:  Loss: 0.0094, Accuracy: 1.0000\n",
            "Batch number: 559, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 560, Training:  Loss: 0.0056, Accuracy: 1.0000\n",
            "Batch number: 561, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 562, Training:  Loss: 0.0161, Accuracy: 1.0000\n",
            "Batch number: 563, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 564, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 565, Training:  Loss: 0.0071, Accuracy: 1.0000\n",
            "Batch number: 566, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 567, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 568, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 569, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 570, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 571, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 572, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 573, Training:  Loss: 0.0080, Accuracy: 1.0000\n",
            "Batch number: 574, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 575, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 576, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 577, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 578, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 579, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 580, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 581, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 582, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 583, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 584, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 585, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 586, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 587, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 588, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 589, Training:  Loss: 0.2010, Accuracy: 0.9688\n",
            "Batch number: 590, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 591, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 592, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 593, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 594, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 595, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 596, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 597, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 598, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 599, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 600, Training:  Loss: 0.0055, Accuracy: 1.0000\n",
            "Batch number: 601, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 602, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 603, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 604, Training:  Loss: 0.0150, Accuracy: 1.0000\n",
            "Batch number: 605, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 606, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 607, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 608, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 609, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 610, Training:  Loss: 0.0540, Accuracy: 0.9688\n",
            "Batch number: 611, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 612, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 613, Training:  Loss: 0.0116, Accuracy: 1.0000\n",
            "Batch number: 614, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 615, Training:  Loss: 0.0101, Accuracy: 1.0000\n",
            "Batch number: 616, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 617, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 618, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 619, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 620, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 621, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 622, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 623, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 624, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 625, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 626, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 627, Training:  Loss: 0.0542, Accuracy: 0.9688\n",
            "Batch number: 628, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 629, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 630, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 631, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 632, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 633, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 634, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 635, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 636, Training:  Loss: 0.0060, Accuracy: 1.0000\n",
            "Batch number: 637, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 638, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 639, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 640, Training:  Loss: 0.0704, Accuracy: 0.9688\n",
            "Batch number: 641, Training:  Loss: 0.0275, Accuracy: 0.9688\n",
            "Batch number: 642, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 643, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 644, Training:  Loss: 0.1126, Accuracy: 0.9688\n",
            "Batch number: 645, Training:  Loss: 0.0222, Accuracy: 1.0000\n",
            "Batch number: 646, Training:  Loss: 0.0127, Accuracy: 1.0000\n",
            "Batch number: 647, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 648, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 649, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 650, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 651, Training:  Loss: 0.0576, Accuracy: 0.9688\n",
            "Batch number: 652, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 653, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 654, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 655, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 656, Training:  Loss: 0.0118, Accuracy: 1.0000\n",
            "Batch number: 657, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 658, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 659, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 660, Training:  Loss: 0.0096, Accuracy: 1.0000\n",
            "Batch number: 661, Training:  Loss: 0.0065, Accuracy: 1.0000\n",
            "Batch number: 662, Training:  Loss: 0.0436, Accuracy: 0.9688\n",
            "Batch number: 663, Training:  Loss: 0.0669, Accuracy: 0.9688\n",
            "Batch number: 664, Training:  Loss: 0.0047, Accuracy: 1.0000\n",
            "Batch number: 665, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 666, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 667, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 668, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 669, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 670, Training:  Loss: 0.0464, Accuracy: 1.0000\n",
            "Batch number: 671, Training:  Loss: 0.0076, Accuracy: 1.0000\n",
            "Batch number: 672, Training:  Loss: 0.0076, Accuracy: 1.0000\n",
            "Batch number: 673, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 674, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 675, Training:  Loss: 0.0103, Accuracy: 1.0000\n",
            "Batch number: 676, Training:  Loss: 0.0717, Accuracy: 0.9688\n",
            "Batch number: 677, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 678, Training:  Loss: 0.0078, Accuracy: 1.0000\n",
            "Batch number: 679, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 680, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 681, Training:  Loss: 0.0301, Accuracy: 0.9688\n",
            "Batch number: 682, Training:  Loss: 0.0751, Accuracy: 0.9688\n",
            "Batch number: 683, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 684, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 685, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 686, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 687, Training:  Loss: 0.0044, Accuracy: 1.0000\n",
            "Batch number: 688, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 689, Training:  Loss: 0.0068, Accuracy: 1.0000\n",
            "Batch number: 690, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 691, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 692, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 693, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 694, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 695, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 696, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 697, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 698, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 699, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 700, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 701, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 702, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 703, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 704, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 705, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 706, Training:  Loss: 0.0089, Accuracy: 1.0000\n",
            "Batch number: 707, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 708, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 709, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 710, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 711, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 712, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 713, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 714, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 715, Training:  Loss: 0.0158, Accuracy: 1.0000\n",
            "Batch number: 716, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 717, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 718, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 719, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 720, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 721, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 722, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 723, Training:  Loss: 0.0154, Accuracy: 1.0000\n",
            "Batch number: 724, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 725, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 726, Training:  Loss: 0.0042, Accuracy: 1.0000\n",
            "Batch number: 727, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 728, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 729, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 730, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 731, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 732, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 733, Training:  Loss: 0.0056, Accuracy: 1.0000\n",
            "Batch number: 734, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 735, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 736, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 737, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 738, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 739, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 740, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 741, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 742, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 743, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 744, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 745, Training:  Loss: 0.0111, Accuracy: 1.0000\n",
            "Batch number: 746, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 747, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 748, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 749, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 750, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 751, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 752, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 753, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 754, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 755, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 756, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 757, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 758, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 759, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 760, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 761, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 762, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 763, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 764, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 765, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 766, Training:  Loss: 0.2632, Accuracy: 0.9375\n",
            "Batch number: 767, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 768, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 769, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 770, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 771, Training:  Loss: 0.0061, Accuracy: 1.0000\n",
            "Batch number: 772, Training:  Loss: 0.0425, Accuracy: 0.9688\n",
            "Batch number: 773, Training:  Loss: 0.0959, Accuracy: 0.9375\n",
            "Batch number: 774, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 775, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 776, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 777, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 778, Training:  Loss: 0.1412, Accuracy: 0.9688\n",
            "Batch number: 779, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 780, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 781, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 782, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 783, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 784, Training:  Loss: 0.0077, Accuracy: 1.0000\n",
            "Batch number: 785, Training:  Loss: 0.0970, Accuracy: 0.9688\n",
            "Batch number: 786, Training:  Loss: 0.0201, Accuracy: 1.0000\n",
            "Batch number: 787, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 788, Training:  Loss: 0.0069, Accuracy: 1.0000\n",
            "Batch number: 789, Training:  Loss: 0.0106, Accuracy: 1.0000\n",
            "Batch number: 790, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 791, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 792, Training:  Loss: 0.0067, Accuracy: 1.0000\n",
            "Batch number: 793, Training:  Loss: 0.0102, Accuracy: 1.0000\n",
            "Batch number: 794, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 795, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 796, Training:  Loss: 0.0190, Accuracy: 1.0000\n",
            "Batch number: 797, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 798, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 799, Training:  Loss: 0.0316, Accuracy: 1.0000\n",
            "Batch number: 800, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 801, Training:  Loss: 0.0063, Accuracy: 1.0000\n",
            "Batch number: 802, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 803, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 804, Training:  Loss: 0.0297, Accuracy: 0.9688\n",
            "Batch number: 805, Training:  Loss: 0.0361, Accuracy: 0.9688\n",
            "Batch number: 806, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 807, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 808, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 809, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 810, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 811, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 812, Training:  Loss: 0.0191, Accuracy: 1.0000\n",
            "Batch number: 813, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 814, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 815, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 816, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 817, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 818, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 819, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 820, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 821, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 822, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 823, Training:  Loss: 0.0725, Accuracy: 0.9688\n",
            "Batch number: 824, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 825, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 826, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 827, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 828, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 829, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 830, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 831, Training:  Loss: 0.0073, Accuracy: 1.0000\n",
            "Batch number: 832, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 833, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 834, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 835, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 836, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 837, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 838, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 839, Training:  Loss: 0.0167, Accuracy: 1.0000\n",
            "Batch number: 840, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 841, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 842, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 843, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 844, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 845, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 846, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 847, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 848, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 849, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 850, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 851, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 852, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 853, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 854, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 855, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 856, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 857, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 858, Training:  Loss: 0.1301, Accuracy: 0.9688\n",
            "Batch number: 859, Training:  Loss: 0.0099, Accuracy: 1.0000\n",
            "Batch number: 860, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 861, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 862, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 863, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Epoch: 39/60\n",
            "Batch number: 000, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 001, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 002, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 003, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 004, Training:  Loss: 0.0220, Accuracy: 1.0000\n",
            "Batch number: 005, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 006, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 007, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 008, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 009, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 010, Training:  Loss: 0.2263, Accuracy: 0.9688\n",
            "Batch number: 011, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 012, Training:  Loss: 0.0096, Accuracy: 1.0000\n",
            "Batch number: 013, Training:  Loss: 0.0053, Accuracy: 1.0000\n",
            "Batch number: 014, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 015, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 016, Training:  Loss: 0.0667, Accuracy: 0.9688\n",
            "Batch number: 017, Training:  Loss: 0.1045, Accuracy: 0.9688\n",
            "Batch number: 018, Training:  Loss: 0.0058, Accuracy: 1.0000\n",
            "Batch number: 019, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 020, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 021, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 022, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 023, Training:  Loss: 0.0251, Accuracy: 0.9688\n",
            "Batch number: 024, Training:  Loss: 0.0072, Accuracy: 1.0000\n",
            "Batch number: 025, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 026, Training:  Loss: 0.0117, Accuracy: 1.0000\n",
            "Batch number: 027, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 028, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 029, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 030, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 031, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 032, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 033, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 034, Training:  Loss: 0.0056, Accuracy: 1.0000\n",
            "Batch number: 035, Training:  Loss: 0.0113, Accuracy: 1.0000\n",
            "Batch number: 036, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 037, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 038, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 039, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 040, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 041, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 042, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 043, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 044, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 045, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 046, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 047, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 048, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 049, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 050, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 051, Training:  Loss: 0.1129, Accuracy: 0.9688\n",
            "Batch number: 052, Training:  Loss: 0.0060, Accuracy: 1.0000\n",
            "Batch number: 053, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 054, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 055, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 056, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 057, Training:  Loss: 0.0385, Accuracy: 0.9688\n",
            "Batch number: 058, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 059, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 060, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 061, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 062, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 063, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 064, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 065, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 066, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 067, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 068, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 069, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 070, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 071, Training:  Loss: 0.0186, Accuracy: 1.0000\n",
            "Batch number: 072, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 073, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 074, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 075, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 076, Training:  Loss: 0.0049, Accuracy: 1.0000\n",
            "Batch number: 077, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 078, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 079, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 080, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 081, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 082, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 083, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 084, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 085, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 086, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 087, Training:  Loss: 0.2549, Accuracy: 0.9688\n",
            "Batch number: 088, Training:  Loss: 0.0042, Accuracy: 1.0000\n",
            "Batch number: 089, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 090, Training:  Loss: 0.0065, Accuracy: 1.0000\n",
            "Batch number: 091, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 092, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 093, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 094, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 095, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 096, Training:  Loss: 0.0431, Accuracy: 0.9688\n",
            "Batch number: 097, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 098, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 099, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 100, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 101, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 102, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 103, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 104, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 105, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 106, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 107, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 108, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 109, Training:  Loss: 0.0083, Accuracy: 1.0000\n",
            "Batch number: 110, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 111, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 112, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 113, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 114, Training:  Loss: 0.0072, Accuracy: 1.0000\n",
            "Batch number: 115, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 116, Training:  Loss: 0.0379, Accuracy: 0.9688\n",
            "Batch number: 117, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 118, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 119, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 120, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 121, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 122, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 123, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 124, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 125, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 126, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 127, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 128, Training:  Loss: 0.0748, Accuracy: 0.9688\n",
            "Batch number: 129, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 130, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 131, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 132, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 133, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 134, Training:  Loss: 0.0127, Accuracy: 1.0000\n",
            "Batch number: 135, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 136, Training:  Loss: 0.0546, Accuracy: 0.9688\n",
            "Batch number: 137, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 138, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 139, Training:  Loss: 0.0075, Accuracy: 1.0000\n",
            "Batch number: 140, Training:  Loss: 0.0079, Accuracy: 1.0000\n",
            "Batch number: 141, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 142, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 143, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 144, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 145, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 146, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 147, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 148, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 149, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 150, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 151, Training:  Loss: 0.0116, Accuracy: 1.0000\n",
            "Batch number: 152, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 153, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 154, Training:  Loss: 0.1308, Accuracy: 0.9688\n",
            "Batch number: 155, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 156, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 157, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 158, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 159, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 160, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 161, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 162, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 163, Training:  Loss: 0.0055, Accuracy: 1.0000\n",
            "Batch number: 164, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 165, Training:  Loss: 0.1173, Accuracy: 0.9688\n",
            "Batch number: 166, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 167, Training:  Loss: 0.0354, Accuracy: 0.9688\n",
            "Batch number: 168, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 169, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 170, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 171, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 172, Training:  Loss: 0.0504, Accuracy: 0.9688\n",
            "Batch number: 173, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 174, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 175, Training:  Loss: 0.0074, Accuracy: 1.0000\n",
            "Batch number: 176, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 177, Training:  Loss: 0.0134, Accuracy: 1.0000\n",
            "Batch number: 178, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 179, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 180, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 181, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 182, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 183, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 184, Training:  Loss: 0.0047, Accuracy: 1.0000\n",
            "Batch number: 185, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 186, Training:  Loss: 0.0062, Accuracy: 1.0000\n",
            "Batch number: 187, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 188, Training:  Loss: 0.0297, Accuracy: 0.9688\n",
            "Batch number: 189, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 190, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 191, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 192, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 193, Training:  Loss: 0.0076, Accuracy: 1.0000\n",
            "Batch number: 194, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 195, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 196, Training:  Loss: 0.0054, Accuracy: 1.0000\n",
            "Batch number: 197, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 198, Training:  Loss: 0.0150, Accuracy: 1.0000\n",
            "Batch number: 199, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 200, Training:  Loss: 0.0042, Accuracy: 1.0000\n",
            "Batch number: 201, Training:  Loss: 0.0062, Accuracy: 1.0000\n",
            "Batch number: 202, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 203, Training:  Loss: 0.0073, Accuracy: 1.0000\n",
            "Batch number: 204, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 205, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 206, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 207, Training:  Loss: 0.0096, Accuracy: 1.0000\n",
            "Batch number: 208, Training:  Loss: 0.1884, Accuracy: 0.9688\n",
            "Batch number: 209, Training:  Loss: 0.0069, Accuracy: 1.0000\n",
            "Batch number: 210, Training:  Loss: 0.0061, Accuracy: 1.0000\n",
            "Batch number: 211, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 212, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 213, Training:  Loss: 0.0106, Accuracy: 1.0000\n",
            "Batch number: 214, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 215, Training:  Loss: 0.0390, Accuracy: 0.9688\n",
            "Batch number: 216, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 217, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 218, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 219, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 220, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 221, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 222, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 223, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 224, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 225, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 226, Training:  Loss: 0.0250, Accuracy: 0.9688\n",
            "Batch number: 227, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 228, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 229, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 230, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 231, Training:  Loss: 0.0086, Accuracy: 1.0000\n",
            "Batch number: 232, Training:  Loss: 0.0124, Accuracy: 1.0000\n",
            "Batch number: 233, Training:  Loss: 0.0709, Accuracy: 0.9688\n",
            "Batch number: 234, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 235, Training:  Loss: 0.1735, Accuracy: 0.9688\n",
            "Batch number: 236, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 237, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 238, Training:  Loss: 0.0199, Accuracy: 1.0000\n",
            "Batch number: 239, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 240, Training:  Loss: 0.0047, Accuracy: 1.0000\n",
            "Batch number: 241, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 242, Training:  Loss: 0.0094, Accuracy: 1.0000\n",
            "Batch number: 243, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 244, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 245, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 246, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 247, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 248, Training:  Loss: 0.0121, Accuracy: 1.0000\n",
            "Batch number: 249, Training:  Loss: 0.0707, Accuracy: 0.9688\n",
            "Batch number: 250, Training:  Loss: 0.0076, Accuracy: 1.0000\n",
            "Batch number: 251, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 252, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 253, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 254, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 255, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 256, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 257, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 258, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 259, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 260, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 261, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 262, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 263, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 264, Training:  Loss: 0.0050, Accuracy: 1.0000\n",
            "Batch number: 265, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 266, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 267, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 268, Training:  Loss: 0.1212, Accuracy: 0.9688\n",
            "Batch number: 269, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 270, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 271, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 272, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 273, Training:  Loss: 0.0589, Accuracy: 0.9688\n",
            "Batch number: 274, Training:  Loss: 0.1060, Accuracy: 0.9688\n",
            "Batch number: 275, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 276, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 277, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 278, Training:  Loss: 0.0063, Accuracy: 1.0000\n",
            "Batch number: 279, Training:  Loss: 0.2236, Accuracy: 0.9688\n",
            "Batch number: 280, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 281, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 282, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 283, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 284, Training:  Loss: 0.1494, Accuracy: 0.9688\n",
            "Batch number: 285, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 286, Training:  Loss: 0.0099, Accuracy: 1.0000\n",
            "Batch number: 287, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 288, Training:  Loss: 0.0107, Accuracy: 1.0000\n",
            "Batch number: 289, Training:  Loss: 0.0059, Accuracy: 1.0000\n",
            "Batch number: 290, Training:  Loss: 0.0072, Accuracy: 1.0000\n",
            "Batch number: 291, Training:  Loss: 0.0255, Accuracy: 0.9688\n",
            "Batch number: 292, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 293, Training:  Loss: 0.0126, Accuracy: 1.0000\n",
            "Batch number: 294, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 295, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 296, Training:  Loss: 0.0060, Accuracy: 1.0000\n",
            "Batch number: 297, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 298, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 299, Training:  Loss: 0.0257, Accuracy: 0.9688\n",
            "Batch number: 300, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 301, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 302, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 303, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 304, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 305, Training:  Loss: 0.0066, Accuracy: 1.0000\n",
            "Batch number: 306, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 307, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 308, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 309, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 310, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 311, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 312, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 313, Training:  Loss: 0.0422, Accuracy: 0.9688\n",
            "Batch number: 314, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 315, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 316, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 317, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 318, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 319, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 320, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 321, Training:  Loss: 0.0050, Accuracy: 1.0000\n",
            "Batch number: 322, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 323, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 324, Training:  Loss: 0.0145, Accuracy: 1.0000\n",
            "Batch number: 325, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 326, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 327, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 328, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 329, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 330, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 331, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 332, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 333, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 334, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 335, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 336, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 337, Training:  Loss: 0.0042, Accuracy: 1.0000\n",
            "Batch number: 338, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 339, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 340, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 341, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 342, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 343, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 344, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 345, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 346, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 347, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 348, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 349, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 350, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 351, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 352, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 353, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 354, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 355, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 356, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 357, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 358, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 359, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 360, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 361, Training:  Loss: 0.0076, Accuracy: 1.0000\n",
            "Batch number: 362, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 363, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 364, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 365, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 366, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 367, Training:  Loss: 0.1114, Accuracy: 0.9375\n",
            "Batch number: 368, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 369, Training:  Loss: 0.0683, Accuracy: 0.9688\n",
            "Batch number: 370, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 371, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 372, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 373, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 374, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 375, Training:  Loss: 0.0057, Accuracy: 1.0000\n",
            "Batch number: 376, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 377, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 378, Training:  Loss: 0.1788, Accuracy: 0.9688\n",
            "Batch number: 379, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 380, Training:  Loss: 0.1589, Accuracy: 0.9688\n",
            "Batch number: 381, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 382, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 383, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 384, Training:  Loss: 0.0049, Accuracy: 1.0000\n",
            "Batch number: 385, Training:  Loss: 0.0211, Accuracy: 1.0000\n",
            "Batch number: 386, Training:  Loss: 0.0099, Accuracy: 1.0000\n",
            "Batch number: 387, Training:  Loss: 0.0075, Accuracy: 1.0000\n",
            "Batch number: 388, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 389, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 390, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 391, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 392, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 393, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 394, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 395, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 396, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 397, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 398, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 399, Training:  Loss: 0.0065, Accuracy: 1.0000\n",
            "Batch number: 400, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 401, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 402, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 403, Training:  Loss: 0.0895, Accuracy: 0.9688\n",
            "Batch number: 404, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 405, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 406, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 407, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 408, Training:  Loss: 0.0110, Accuracy: 1.0000\n",
            "Batch number: 409, Training:  Loss: 0.0053, Accuracy: 1.0000\n",
            "Batch number: 410, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 411, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 412, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 413, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 414, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 415, Training:  Loss: 0.1370, Accuracy: 0.9688\n",
            "Batch number: 416, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 417, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 418, Training:  Loss: 0.0081, Accuracy: 1.0000\n",
            "Batch number: 419, Training:  Loss: 0.0112, Accuracy: 1.0000\n",
            "Batch number: 420, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 421, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 422, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 423, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 424, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 425, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 426, Training:  Loss: 0.0078, Accuracy: 1.0000\n",
            "Batch number: 427, Training:  Loss: 0.0126, Accuracy: 1.0000\n",
            "Batch number: 428, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 429, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 430, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 431, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 432, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 433, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 434, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 435, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 436, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 437, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 438, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 439, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 440, Training:  Loss: 0.0044, Accuracy: 1.0000\n",
            "Batch number: 441, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 442, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 443, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 444, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 445, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 446, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 447, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 448, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 449, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 450, Training:  Loss: 0.0336, Accuracy: 0.9688\n",
            "Batch number: 451, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 452, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 453, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 454, Training:  Loss: 0.0044, Accuracy: 1.0000\n",
            "Batch number: 455, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 456, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 457, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 458, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 459, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 460, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 461, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 462, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 463, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 464, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 465, Training:  Loss: 0.0128, Accuracy: 1.0000\n",
            "Batch number: 466, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 467, Training:  Loss: 0.0671, Accuracy: 0.9688\n",
            "Batch number: 468, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 469, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 470, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 471, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 472, Training:  Loss: 0.0192, Accuracy: 1.0000\n",
            "Batch number: 473, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 474, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 475, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 476, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 477, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 478, Training:  Loss: 0.1157, Accuracy: 0.9688\n",
            "Batch number: 479, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 480, Training:  Loss: 0.1720, Accuracy: 0.9688\n",
            "Batch number: 481, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 482, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 483, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 484, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 485, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 486, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 487, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 488, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 489, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 490, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 491, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 492, Training:  Loss: 0.0978, Accuracy: 0.9375\n",
            "Batch number: 493, Training:  Loss: 0.0087, Accuracy: 1.0000\n",
            "Batch number: 494, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 495, Training:  Loss: 0.0081, Accuracy: 1.0000\n",
            "Batch number: 496, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 497, Training:  Loss: 0.0185, Accuracy: 1.0000\n",
            "Batch number: 498, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 499, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 500, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 501, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 502, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 503, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 504, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 505, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 506, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 507, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 508, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 509, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 510, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 511, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 512, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 513, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 514, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 515, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 516, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 517, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 518, Training:  Loss: 0.2003, Accuracy: 0.9688\n",
            "Batch number: 519, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 520, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 521, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 522, Training:  Loss: 0.0051, Accuracy: 1.0000\n",
            "Batch number: 523, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 524, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 525, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 526, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 527, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 528, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 529, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 530, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 531, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 532, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 533, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 534, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 535, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 536, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 537, Training:  Loss: 0.0082, Accuracy: 1.0000\n",
            "Batch number: 538, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 539, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 540, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 541, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 542, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 543, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 544, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 545, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 546, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 547, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 548, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 549, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 550, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 551, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 552, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 553, Training:  Loss: 0.1698, Accuracy: 0.9688\n",
            "Batch number: 554, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 555, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 556, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 557, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 558, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 559, Training:  Loss: 0.0066, Accuracy: 1.0000\n",
            "Batch number: 560, Training:  Loss: 0.0116, Accuracy: 1.0000\n",
            "Batch number: 561, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 562, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 563, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 564, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 565, Training:  Loss: 0.1995, Accuracy: 0.9688\n",
            "Batch number: 566, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 567, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 568, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 569, Training:  Loss: 0.0165, Accuracy: 1.0000\n",
            "Batch number: 570, Training:  Loss: 0.0639, Accuracy: 0.9688\n",
            "Batch number: 571, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 572, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 573, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 574, Training:  Loss: 0.0074, Accuracy: 1.0000\n",
            "Batch number: 575, Training:  Loss: 0.0140, Accuracy: 1.0000\n",
            "Batch number: 576, Training:  Loss: 0.0377, Accuracy: 0.9688\n",
            "Batch number: 577, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 578, Training:  Loss: 0.0056, Accuracy: 1.0000\n",
            "Batch number: 579, Training:  Loss: 0.0106, Accuracy: 1.0000\n",
            "Batch number: 580, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 581, Training:  Loss: 0.0563, Accuracy: 0.9688\n",
            "Batch number: 582, Training:  Loss: 0.0290, Accuracy: 0.9688\n",
            "Batch number: 583, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 584, Training:  Loss: 0.0383, Accuracy: 0.9688\n",
            "Batch number: 585, Training:  Loss: 0.0099, Accuracy: 1.0000\n",
            "Batch number: 586, Training:  Loss: 0.0132, Accuracy: 1.0000\n",
            "Batch number: 587, Training:  Loss: 0.0067, Accuracy: 1.0000\n",
            "Batch number: 588, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 589, Training:  Loss: 0.0081, Accuracy: 1.0000\n",
            "Batch number: 590, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 591, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 592, Training:  Loss: 0.0096, Accuracy: 1.0000\n",
            "Batch number: 593, Training:  Loss: 0.0418, Accuracy: 0.9688\n",
            "Batch number: 594, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 595, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 596, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 597, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 598, Training:  Loss: 0.0136, Accuracy: 1.0000\n",
            "Batch number: 599, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 600, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 601, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 602, Training:  Loss: 0.1165, Accuracy: 0.9688\n",
            "Batch number: 603, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 604, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 605, Training:  Loss: 0.0272, Accuracy: 1.0000\n",
            "Batch number: 606, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 607, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 608, Training:  Loss: 0.0093, Accuracy: 1.0000\n",
            "Batch number: 609, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 610, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 611, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 612, Training:  Loss: 0.1637, Accuracy: 0.9688\n",
            "Batch number: 613, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 614, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 615, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 616, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 617, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 618, Training:  Loss: 0.0170, Accuracy: 1.0000\n",
            "Batch number: 619, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 620, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 621, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 622, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 623, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 624, Training:  Loss: 0.0150, Accuracy: 1.0000\n",
            "Batch number: 625, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 626, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 627, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 628, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 629, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 630, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 631, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 632, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 633, Training:  Loss: 0.0063, Accuracy: 1.0000\n",
            "Batch number: 634, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 635, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 636, Training:  Loss: 0.0740, Accuracy: 0.9688\n",
            "Batch number: 637, Training:  Loss: 0.1476, Accuracy: 0.9688\n",
            "Batch number: 638, Training:  Loss: 0.0046, Accuracy: 1.0000\n",
            "Batch number: 639, Training:  Loss: 0.0108, Accuracy: 1.0000\n",
            "Batch number: 640, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 641, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 642, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 643, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 644, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 645, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 646, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 647, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 648, Training:  Loss: 0.0176, Accuracy: 1.0000\n",
            "Batch number: 649, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 650, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 651, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 652, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 653, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 654, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 655, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 656, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 657, Training:  Loss: 0.0913, Accuracy: 0.9688\n",
            "Batch number: 658, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 659, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 660, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 661, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 662, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 663, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 664, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 665, Training:  Loss: 0.0101, Accuracy: 1.0000\n",
            "Batch number: 666, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 667, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 668, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 669, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 670, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 671, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 672, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 673, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 674, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 675, Training:  Loss: 0.0139, Accuracy: 1.0000\n",
            "Batch number: 676, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 677, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 678, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 679, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 680, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 681, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 682, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 683, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 684, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 685, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 686, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 687, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 688, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 689, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 690, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 691, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 692, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 693, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 694, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 695, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 696, Training:  Loss: 0.0085, Accuracy: 1.0000\n",
            "Batch number: 697, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 698, Training:  Loss: 0.0094, Accuracy: 1.0000\n",
            "Batch number: 699, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 700, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 701, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 702, Training:  Loss: 0.1694, Accuracy: 0.9688\n",
            "Batch number: 703, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 704, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 705, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 706, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 707, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 708, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 709, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 710, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 711, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 712, Training:  Loss: 0.0276, Accuracy: 1.0000\n",
            "Batch number: 713, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 714, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 715, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 716, Training:  Loss: 0.0103, Accuracy: 1.0000\n",
            "Batch number: 717, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 718, Training:  Loss: 0.1454, Accuracy: 0.9688\n",
            "Batch number: 719, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 720, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 721, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 722, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 723, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 724, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 725, Training:  Loss: 0.1207, Accuracy: 0.9688\n",
            "Batch number: 726, Training:  Loss: 0.0281, Accuracy: 0.9688\n",
            "Batch number: 727, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 728, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 729, Training:  Loss: 0.0042, Accuracy: 1.0000\n",
            "Batch number: 730, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 731, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 732, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 733, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 734, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 735, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 736, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 737, Training:  Loss: 0.0089, Accuracy: 1.0000\n",
            "Batch number: 738, Training:  Loss: 0.1266, Accuracy: 0.9688\n",
            "Batch number: 739, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 740, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 741, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 742, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 743, Training:  Loss: 0.0094, Accuracy: 1.0000\n",
            "Batch number: 744, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 745, Training:  Loss: 0.0677, Accuracy: 0.9688\n",
            "Batch number: 746, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 747, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 748, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 749, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 750, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 751, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 752, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 753, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 754, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 755, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 756, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 757, Training:  Loss: 0.0097, Accuracy: 1.0000\n",
            "Batch number: 758, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 759, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 760, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 761, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 762, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 763, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 764, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 765, Training:  Loss: 0.0380, Accuracy: 0.9688\n",
            "Batch number: 766, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 767, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 768, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 769, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 770, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 771, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 772, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 773, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 774, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 775, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 776, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 777, Training:  Loss: 0.0043, Accuracy: 1.0000\n",
            "Batch number: 778, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 779, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 780, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 781, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 782, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 783, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 784, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 785, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 786, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 787, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 788, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 789, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 790, Training:  Loss: 0.0955, Accuracy: 0.9688\n",
            "Batch number: 791, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 792, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 793, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 794, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 795, Training:  Loss: 0.0044, Accuracy: 1.0000\n",
            "Batch number: 796, Training:  Loss: 0.0067, Accuracy: 1.0000\n",
            "Batch number: 797, Training:  Loss: 0.0066, Accuracy: 1.0000\n",
            "Batch number: 798, Training:  Loss: 0.0532, Accuracy: 0.9688\n",
            "Batch number: 799, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 800, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 801, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 802, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 803, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 804, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 805, Training:  Loss: 0.0049, Accuracy: 1.0000\n",
            "Batch number: 806, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 807, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 808, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 809, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 810, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 811, Training:  Loss: 0.0060, Accuracy: 1.0000\n",
            "Batch number: 812, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 813, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 814, Training:  Loss: 0.0062, Accuracy: 1.0000\n",
            "Batch number: 815, Training:  Loss: 0.0838, Accuracy: 0.9688\n",
            "Batch number: 816, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 817, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 818, Training:  Loss: 0.0081, Accuracy: 1.0000\n",
            "Batch number: 819, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 820, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 821, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 822, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 823, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 824, Training:  Loss: 0.1065, Accuracy: 0.9688\n",
            "Batch number: 825, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 826, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 827, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 828, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 829, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 830, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 831, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 832, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 833, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 834, Training:  Loss: 0.0176, Accuracy: 1.0000\n",
            "Batch number: 835, Training:  Loss: 0.0092, Accuracy: 1.0000\n",
            "Batch number: 836, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 837, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 838, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 839, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 840, Training:  Loss: 0.0050, Accuracy: 1.0000\n",
            "Batch number: 841, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 842, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 843, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 844, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 845, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 846, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 847, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 848, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 849, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 850, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 851, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 852, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 853, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 854, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 855, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 856, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 857, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 858, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 859, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 860, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 861, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 862, Training:  Loss: 0.0059, Accuracy: 1.0000\n",
            "Batch number: 863, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Epoch: 40/60\n",
            "Batch number: 000, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 001, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 002, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 003, Training:  Loss: 0.0160, Accuracy: 1.0000\n",
            "Batch number: 004, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 005, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 006, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 007, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 008, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 009, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 010, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 011, Training:  Loss: 0.0420, Accuracy: 0.9688\n",
            "Batch number: 012, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 013, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 014, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 015, Training:  Loss: 0.0105, Accuracy: 1.0000\n",
            "Batch number: 016, Training:  Loss: 0.0065, Accuracy: 1.0000\n",
            "Batch number: 017, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 018, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 019, Training:  Loss: 0.0169, Accuracy: 1.0000\n",
            "Batch number: 020, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 021, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 022, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 023, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 024, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 025, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 026, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 027, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 028, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 029, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 030, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 031, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 032, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 033, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 034, Training:  Loss: 0.0292, Accuracy: 1.0000\n",
            "Batch number: 035, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 036, Training:  Loss: 0.0234, Accuracy: 1.0000\n",
            "Batch number: 037, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 038, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 039, Training:  Loss: 0.0069, Accuracy: 1.0000\n",
            "Batch number: 040, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 041, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 042, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 043, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 044, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 045, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 046, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 047, Training:  Loss: 0.0047, Accuracy: 1.0000\n",
            "Batch number: 048, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 049, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 050, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 051, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 052, Training:  Loss: 0.0560, Accuracy: 0.9688\n",
            "Batch number: 053, Training:  Loss: 0.0149, Accuracy: 1.0000\n",
            "Batch number: 054, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 055, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 056, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 057, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 058, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 059, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 060, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 061, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 062, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 063, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 064, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 065, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 066, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 067, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 068, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 069, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 070, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 071, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 072, Training:  Loss: 0.0170, Accuracy: 1.0000\n",
            "Batch number: 073, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 074, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 075, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 076, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 077, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 078, Training:  Loss: 0.0046, Accuracy: 1.0000\n",
            "Batch number: 079, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 080, Training:  Loss: 0.1824, Accuracy: 0.9375\n",
            "Batch number: 081, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 082, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 083, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 084, Training:  Loss: 0.0058, Accuracy: 1.0000\n",
            "Batch number: 085, Training:  Loss: 0.0417, Accuracy: 0.9688\n",
            "Batch number: 086, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 087, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 088, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 089, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 090, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 091, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 092, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 093, Training:  Loss: 0.1339, Accuracy: 0.9688\n",
            "Batch number: 094, Training:  Loss: 0.0210, Accuracy: 1.0000\n",
            "Batch number: 095, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 096, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 097, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 098, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 099, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 100, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 101, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 102, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 103, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 104, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 105, Training:  Loss: 0.0315, Accuracy: 0.9688\n",
            "Batch number: 106, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 107, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 108, Training:  Loss: 0.1141, Accuracy: 0.9688\n",
            "Batch number: 109, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 110, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 111, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 112, Training:  Loss: 0.0628, Accuracy: 0.9688\n",
            "Batch number: 113, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 114, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 115, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 116, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 117, Training:  Loss: 0.0244, Accuracy: 0.9688\n",
            "Batch number: 118, Training:  Loss: 0.0092, Accuracy: 1.0000\n",
            "Batch number: 119, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 120, Training:  Loss: 0.0082, Accuracy: 1.0000\n",
            "Batch number: 121, Training:  Loss: 0.0079, Accuracy: 1.0000\n",
            "Batch number: 122, Training:  Loss: 0.0092, Accuracy: 1.0000\n",
            "Batch number: 123, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 124, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 125, Training:  Loss: 0.0073, Accuracy: 1.0000\n",
            "Batch number: 126, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 127, Training:  Loss: 0.0249, Accuracy: 0.9688\n",
            "Batch number: 128, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 129, Training:  Loss: 0.0057, Accuracy: 1.0000\n",
            "Batch number: 130, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 131, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 132, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 133, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 134, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 135, Training:  Loss: 0.0918, Accuracy: 0.9688\n",
            "Batch number: 136, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 137, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 138, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 139, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 140, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 141, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 142, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 143, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 144, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 145, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 146, Training:  Loss: 0.1700, Accuracy: 0.9688\n",
            "Batch number: 147, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 148, Training:  Loss: 0.0120, Accuracy: 1.0000\n",
            "Batch number: 149, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 150, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 151, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 152, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 153, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 154, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 155, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 156, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 157, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 158, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 159, Training:  Loss: 0.1506, Accuracy: 0.9688\n",
            "Batch number: 160, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 161, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 162, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 163, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 164, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 165, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 166, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 167, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 168, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 169, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 170, Training:  Loss: 0.0712, Accuracy: 0.9688\n",
            "Batch number: 171, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 172, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 173, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 174, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 175, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 176, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 177, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 178, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 179, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 180, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 181, Training:  Loss: 0.0045, Accuracy: 1.0000\n",
            "Batch number: 182, Training:  Loss: 0.0027, Accuracy: 1.0000\n",
            "Batch number: 183, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 184, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 185, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 186, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 187, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 188, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 189, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 190, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 191, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 192, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 193, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 194, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 195, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 196, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 197, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 198, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 199, Training:  Loss: 0.0112, Accuracy: 1.0000\n",
            "Batch number: 200, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 201, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 202, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 203, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 204, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 205, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 206, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 207, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 208, Training:  Loss: 0.0051, Accuracy: 1.0000\n",
            "Batch number: 209, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 210, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 211, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 212, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 213, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 214, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 215, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 216, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 217, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 218, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 219, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 220, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 221, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 222, Training:  Loss: 0.2271, Accuracy: 0.9688\n",
            "Batch number: 223, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 224, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 225, Training:  Loss: 0.0091, Accuracy: 1.0000\n",
            "Batch number: 226, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 227, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 228, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 229, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 230, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 231, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 232, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 233, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 234, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 235, Training:  Loss: 0.0132, Accuracy: 1.0000\n",
            "Batch number: 236, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 237, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 238, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 239, Training:  Loss: 0.0759, Accuracy: 0.9688\n",
            "Batch number: 240, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 241, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 242, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 243, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 244, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 245, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 246, Training:  Loss: 0.1338, Accuracy: 0.9688\n",
            "Batch number: 247, Training:  Loss: 0.0061, Accuracy: 1.0000\n",
            "Batch number: 248, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 249, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 250, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 251, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 252, Training:  Loss: 0.0033, Accuracy: 1.0000\n",
            "Batch number: 253, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 254, Training:  Loss: 0.0047, Accuracy: 1.0000\n",
            "Batch number: 255, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 256, Training:  Loss: 0.0600, Accuracy: 0.9688\n",
            "Batch number: 257, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 258, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 259, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 260, Training:  Loss: 0.0064, Accuracy: 1.0000\n",
            "Batch number: 261, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 262, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 263, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 264, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 265, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 266, Training:  Loss: 0.1652, Accuracy: 0.9688\n",
            "Batch number: 267, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 268, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 269, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 270, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 271, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 272, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 273, Training:  Loss: 0.1665, Accuracy: 0.9688\n",
            "Batch number: 274, Training:  Loss: 0.0047, Accuracy: 1.0000\n",
            "Batch number: 275, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 276, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 277, Training:  Loss: 0.1314, Accuracy: 0.9688\n",
            "Batch number: 278, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 279, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 280, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 281, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 282, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 283, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 284, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 285, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 286, Training:  Loss: 0.0157, Accuracy: 1.0000\n",
            "Batch number: 287, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 288, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 289, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 290, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 291, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 292, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 293, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 294, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 295, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 296, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 297, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 298, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 299, Training:  Loss: 0.0080, Accuracy: 1.0000\n",
            "Batch number: 300, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 301, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 302, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 303, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 304, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 305, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 306, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 307, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 308, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 309, Training:  Loss: 0.0057, Accuracy: 1.0000\n",
            "Batch number: 310, Training:  Loss: 0.0220, Accuracy: 1.0000\n",
            "Batch number: 311, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 312, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 313, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 314, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 315, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 316, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 317, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 318, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 319, Training:  Loss: 0.0024, Accuracy: 1.0000\n",
            "Batch number: 320, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 321, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 322, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 323, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 324, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 325, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 326, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 327, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 328, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 329, Training:  Loss: 0.1578, Accuracy: 0.9375\n",
            "Batch number: 330, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 331, Training:  Loss: 0.0664, Accuracy: 0.9688\n",
            "Batch number: 332, Training:  Loss: 0.0106, Accuracy: 1.0000\n",
            "Batch number: 333, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 334, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 335, Training:  Loss: 0.0051, Accuracy: 1.0000\n",
            "Batch number: 336, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 337, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 338, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 339, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 340, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 341, Training:  Loss: 0.0067, Accuracy: 1.0000\n",
            "Batch number: 342, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 343, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 344, Training:  Loss: 0.0685, Accuracy: 0.9688\n",
            "Batch number: 345, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 346, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 347, Training:  Loss: 0.0793, Accuracy: 0.9688\n",
            "Batch number: 348, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 349, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 350, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 351, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 352, Training:  Loss: 0.0195, Accuracy: 1.0000\n",
            "Batch number: 353, Training:  Loss: 0.0042, Accuracy: 1.0000\n",
            "Batch number: 354, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 355, Training:  Loss: 0.0034, Accuracy: 1.0000\n",
            "Batch number: 356, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 357, Training:  Loss: 0.0161, Accuracy: 1.0000\n",
            "Batch number: 358, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 359, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 360, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 361, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 362, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 363, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 364, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 365, Training:  Loss: 0.0187, Accuracy: 1.0000\n",
            "Batch number: 366, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 367, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 368, Training:  Loss: 0.0057, Accuracy: 1.0000\n",
            "Batch number: 369, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 370, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 371, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 372, Training:  Loss: 0.1070, Accuracy: 0.9688\n",
            "Batch number: 373, Training:  Loss: 0.0311, Accuracy: 1.0000\n",
            "Batch number: 374, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 375, Training:  Loss: 0.0123, Accuracy: 1.0000\n",
            "Batch number: 376, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 377, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 378, Training:  Loss: 0.0061, Accuracy: 1.0000\n",
            "Batch number: 379, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 380, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 381, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 382, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 383, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 384, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 385, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 386, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 387, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 388, Training:  Loss: 0.0072, Accuracy: 1.0000\n",
            "Batch number: 389, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 390, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 391, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 392, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 393, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 394, Training:  Loss: 0.0419, Accuracy: 0.9688\n",
            "Batch number: 395, Training:  Loss: 0.1632, Accuracy: 0.9688\n",
            "Batch number: 396, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 397, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 398, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 399, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 400, Training:  Loss: 0.0052, Accuracy: 1.0000\n",
            "Batch number: 401, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 402, Training:  Loss: 0.0039, Accuracy: 1.0000\n",
            "Batch number: 403, Training:  Loss: 0.0090, Accuracy: 1.0000\n",
            "Batch number: 404, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 405, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 406, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 407, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 408, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 409, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 410, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 411, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 412, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 413, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 414, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 415, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 416, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 417, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 418, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 419, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 420, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 421, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 422, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 423, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 424, Training:  Loss: 0.0042, Accuracy: 1.0000\n",
            "Batch number: 425, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 426, Training:  Loss: 0.0029, Accuracy: 1.0000\n",
            "Batch number: 427, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 428, Training:  Loss: 0.0990, Accuracy: 0.9688\n",
            "Batch number: 429, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 430, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 431, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 432, Training:  Loss: 0.0035, Accuracy: 1.0000\n",
            "Batch number: 433, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 434, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 435, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 436, Training:  Loss: 0.0044, Accuracy: 1.0000\n",
            "Batch number: 437, Training:  Loss: 0.1339, Accuracy: 0.9688\n",
            "Batch number: 438, Training:  Loss: 0.0067, Accuracy: 1.0000\n",
            "Batch number: 439, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 440, Training:  Loss: 0.0020, Accuracy: 1.0000\n",
            "Batch number: 441, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 442, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 443, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 444, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 445, Training:  Loss: 0.0038, Accuracy: 1.0000\n",
            "Batch number: 446, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 447, Training:  Loss: 0.0064, Accuracy: 1.0000\n",
            "Batch number: 448, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 449, Training:  Loss: 0.0061, Accuracy: 1.0000\n",
            "Batch number: 450, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 451, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 452, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 453, Training:  Loss: 0.0036, Accuracy: 1.0000\n",
            "Batch number: 454, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 455, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 456, Training:  Loss: 0.0115, Accuracy: 1.0000\n",
            "Batch number: 457, Training:  Loss: 0.0286, Accuracy: 0.9688\n",
            "Batch number: 458, Training:  Loss: 0.1095, Accuracy: 0.9688\n",
            "Batch number: 459, Training:  Loss: 0.0030, Accuracy: 1.0000\n",
            "Batch number: 460, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 461, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 462, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 463, Training:  Loss: 0.0012, Accuracy: 1.0000\n",
            "Batch number: 464, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 465, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 466, Training:  Loss: 0.1072, Accuracy: 0.9688\n",
            "Batch number: 467, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 468, Training:  Loss: 0.0153, Accuracy: 1.0000\n",
            "Batch number: 469, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 470, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 471, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 472, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 473, Training:  Loss: 0.0058, Accuracy: 1.0000\n",
            "Batch number: 474, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 475, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 476, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 477, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 478, Training:  Loss: 0.0021, Accuracy: 1.0000\n",
            "Batch number: 479, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 480, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 481, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 482, Training:  Loss: 0.0041, Accuracy: 1.0000\n",
            "Batch number: 483, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 484, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 485, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 486, Training:  Loss: 0.1115, Accuracy: 0.9375\n",
            "Batch number: 487, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 488, Training:  Loss: 0.0032, Accuracy: 1.0000\n",
            "Batch number: 489, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 490, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 491, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 492, Training:  Loss: 0.0369, Accuracy: 0.9688\n",
            "Batch number: 493, Training:  Loss: 0.0025, Accuracy: 1.0000\n",
            "Batch number: 494, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 495, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 496, Training:  Loss: 0.0055, Accuracy: 1.0000\n",
            "Batch number: 497, Training:  Loss: 0.2220, Accuracy: 0.9688\n",
            "Batch number: 498, Training:  Loss: 0.0206, Accuracy: 1.0000\n",
            "Batch number: 499, Training:  Loss: 0.0082, Accuracy: 1.0000\n",
            "Batch number: 500, Training:  Loss: 0.0064, Accuracy: 1.0000\n",
            "Batch number: 501, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 502, Training:  Loss: 0.0048, Accuracy: 1.0000\n",
            "Batch number: 503, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 504, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 505, Training:  Loss: 0.0064, Accuracy: 1.0000\n",
            "Batch number: 506, Training:  Loss: 0.0075, Accuracy: 1.0000\n",
            "Batch number: 507, Training:  Loss: 0.0546, Accuracy: 0.9688\n",
            "Batch number: 508, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 509, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 510, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 511, Training:  Loss: 0.0031, Accuracy: 1.0000\n",
            "Batch number: 512, Training:  Loss: 0.0023, Accuracy: 1.0000\n",
            "Batch number: 513, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 514, Training:  Loss: 0.1004, Accuracy: 0.9688\n",
            "Batch number: 515, Training:  Loss: 0.0066, Accuracy: 1.0000\n",
            "Batch number: 516, Training:  Loss: 0.0133, Accuracy: 1.0000\n",
            "Batch number: 517, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 518, Training:  Loss: 0.0037, Accuracy: 1.0000\n",
            "Batch number: 519, Training:  Loss: 0.0499, Accuracy: 0.9688\n",
            "Batch number: 520, Training:  Loss: 0.0081, Accuracy: 1.0000\n",
            "Batch number: 521, Training:  Loss: 0.0066, Accuracy: 1.0000\n",
            "Batch number: 522, Training:  Loss: 0.0068, Accuracy: 1.0000\n",
            "Batch number: 523, Training:  Loss: 0.0022, Accuracy: 1.0000\n",
            "Batch number: 524, Training:  Loss: 0.0009, Accuracy: 1.0000\n",
            "Batch number: 525, Training:  Loss: 0.0013, Accuracy: 1.0000\n",
            "Batch number: 526, Training:  Loss: 0.0006, Accuracy: 1.0000\n",
            "Batch number: 527, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 528, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 529, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 530, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 531, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 532, Training:  Loss: 0.0040, Accuracy: 1.0000\n",
            "Batch number: 533, Training:  Loss: 0.0028, Accuracy: 1.0000\n",
            "Batch number: 534, Training:  Loss: 0.0011, Accuracy: 1.0000\n",
            "Batch number: 535, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 536, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 537, Training:  Loss: 0.0014, Accuracy: 1.0000\n",
            "Batch number: 538, Training:  Loss: 0.0018, Accuracy: 1.0000\n",
            "Batch number: 539, Training:  Loss: 0.0150, Accuracy: 1.0000\n",
            "Batch number: 540, Training:  Loss: 0.0019, Accuracy: 1.0000\n",
            "Batch number: 541, Training:  Loss: 0.0017, Accuracy: 1.0000\n",
            "Batch number: 542, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 543, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 544, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 545, Training:  Loss: 0.0078, Accuracy: 1.0000\n",
            "Batch number: 546, Training:  Loss: 0.0001, Accuracy: 1.0000\n",
            "Batch number: 547, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 548, Training:  Loss: 0.0015, Accuracy: 1.0000\n",
            "Batch number: 549, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 550, Training:  Loss: 0.1385, Accuracy: 0.9688\n",
            "Batch number: 551, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 552, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 553, Training:  Loss: 0.0010, Accuracy: 1.0000\n",
            "Batch number: 554, Training:  Loss: 0.0003, Accuracy: 1.0000\n",
            "Batch number: 555, Training:  Loss: 0.0007, Accuracy: 1.0000\n",
            "Batch number: 556, Training:  Loss: 0.0004, Accuracy: 1.0000\n",
            "Batch number: 557, Training:  Loss: 0.0016, Accuracy: 1.0000\n",
            "Batch number: 558, Training:  Loss: 0.0005, Accuracy: 1.0000\n",
            "Batch number: 559, Training:  Loss: 0.0002, Accuracy: 1.0000\n",
            "Batch number: 560, Training:  Loss: 0.0026, Accuracy: 1.0000\n",
            "Batch number: 561, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 562, Training:  Loss: 0.0008, Accuracy: 1.0000\n",
            "Batch number: 563, Training:  Loss: 0.0019, Accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3-toFhvhIOT"
      },
      "source": [
        "def computeTestSetAccuracy(model, loss_criterion):\n",
        "    '''\n",
        "    Function to compute the accuracy on the test set\n",
        "    Parameters\n",
        "        :param model: Model to test\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "    '''\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    test_acc = 0.0\n",
        "    test_loss = 0.0\n",
        "\n",
        "    # Validation - No gradient tracking needed\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Set to evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        # Validation loop\n",
        "        for j, (inputs, labels) in enumerate(test_data):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "\n",
        "            # Compute the total loss for the batch and add it to valid_loss\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "            # Compute total accuracy in the whole batch and add to valid_acc\n",
        "            test_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "            print(\"Test Batch number: {:03d}, Test: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "\n",
        "    # Find average test loss and test accuracy\n",
        "    avg_test_loss = test_loss/test_data_size \n",
        "    avg_test_acc = test_acc/test_data_size\n",
        "\n",
        "    print(\"Test accuracy : \" + str(avg_test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwQ5VbTJhNBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37d9c91f-b232-4e55-bd06-15f6f2ca6330"
      },
      "source": [
        "computeTestSetAccuracy(shufflenet,criterion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Batch number: 000, Test: Loss: 5.9853, Accuracy: 0.5938\n",
            "Test Batch number: 001, Test: Loss: 7.1340, Accuracy: 0.5312\n",
            "Test Batch number: 002, Test: Loss: 9.3866, Accuracy: 0.4375\n",
            "Test Batch number: 003, Test: Loss: 9.5402, Accuracy: 0.4375\n",
            "Test Batch number: 004, Test: Loss: 8.3127, Accuracy: 0.4688\n",
            "Test Batch number: 005, Test: Loss: 6.2054, Accuracy: 0.5938\n",
            "Test Batch number: 006, Test: Loss: 7.4648, Accuracy: 0.5000\n",
            "Test Batch number: 007, Test: Loss: 7.1688, Accuracy: 0.5312\n",
            "Test Batch number: 008, Test: Loss: 6.2773, Accuracy: 0.5938\n",
            "Test Batch number: 009, Test: Loss: 6.9335, Accuracy: 0.5000\n",
            "Test Batch number: 010, Test: Loss: 7.5362, Accuracy: 0.5000\n",
            "Test Batch number: 011, Test: Loss: 8.3816, Accuracy: 0.5312\n",
            "Test Batch number: 012, Test: Loss: 8.8667, Accuracy: 0.4062\n",
            "Test Batch number: 013, Test: Loss: 8.5604, Accuracy: 0.4688\n",
            "Test Batch number: 014, Test: Loss: 7.4907, Accuracy: 0.5000\n",
            "Test Batch number: 015, Test: Loss: 6.4543, Accuracy: 0.5312\n",
            "Test Batch number: 016, Test: Loss: 9.5094, Accuracy: 0.3750\n",
            "Test Batch number: 017, Test: Loss: 5.5408, Accuracy: 0.6250\n",
            "Test Batch number: 018, Test: Loss: 8.5457, Accuracy: 0.5000\n",
            "Test Batch number: 019, Test: Loss: 6.6279, Accuracy: 0.5312\n",
            "Test Batch number: 020, Test: Loss: 5.7121, Accuracy: 0.5625\n",
            "Test Batch number: 021, Test: Loss: 7.0693, Accuracy: 0.5000\n",
            "Test Batch number: 022, Test: Loss: 9.7581, Accuracy: 0.3750\n",
            "Test Batch number: 023, Test: Loss: 7.6111, Accuracy: 0.4688\n",
            "Test Batch number: 024, Test: Loss: 6.4758, Accuracy: 0.5000\n",
            "Test Batch number: 025, Test: Loss: 10.8703, Accuracy: 0.3125\n",
            "Test Batch number: 026, Test: Loss: 6.1597, Accuracy: 0.5938\n",
            "Test Batch number: 027, Test: Loss: 8.5008, Accuracy: 0.5000\n",
            "Test Batch number: 028, Test: Loss: 7.2317, Accuracy: 0.5312\n",
            "Test Batch number: 029, Test: Loss: 8.3300, Accuracy: 0.4375\n",
            "Test Batch number: 030, Test: Loss: 6.3859, Accuracy: 0.6250\n",
            "Test Batch number: 031, Test: Loss: 5.8607, Accuracy: 0.6250\n",
            "Test Batch number: 032, Test: Loss: 8.0256, Accuracy: 0.4688\n",
            "Test Batch number: 033, Test: Loss: 7.1024, Accuracy: 0.5625\n",
            "Test Batch number: 034, Test: Loss: 6.8706, Accuracy: 0.5625\n",
            "Test Batch number: 035, Test: Loss: 8.1107, Accuracy: 0.5312\n",
            "Test Batch number: 036, Test: Loss: 5.5805, Accuracy: 0.6250\n",
            "Test Batch number: 037, Test: Loss: 9.2715, Accuracy: 0.4062\n",
            "Test Batch number: 038, Test: Loss: 8.0006, Accuracy: 0.5000\n",
            "Test Batch number: 039, Test: Loss: 6.1278, Accuracy: 0.6250\n",
            "Test Batch number: 040, Test: Loss: 4.8344, Accuracy: 0.6562\n",
            "Test Batch number: 041, Test: Loss: 6.0299, Accuracy: 0.5625\n",
            "Test Batch number: 042, Test: Loss: 7.1630, Accuracy: 0.5938\n",
            "Test Batch number: 043, Test: Loss: 7.8015, Accuracy: 0.5000\n",
            "Test Batch number: 044, Test: Loss: 8.0966, Accuracy: 0.4375\n",
            "Test Batch number: 045, Test: Loss: 9.5406, Accuracy: 0.5000\n",
            "Test Batch number: 046, Test: Loss: 6.2239, Accuracy: 0.5625\n",
            "Test Batch number: 047, Test: Loss: 7.3267, Accuracy: 0.5625\n",
            "Test Batch number: 048, Test: Loss: 8.1613, Accuracy: 0.4688\n",
            "Test Batch number: 049, Test: Loss: 8.1187, Accuracy: 0.4688\n",
            "Test Batch number: 050, Test: Loss: 9.1702, Accuracy: 0.4375\n",
            "Test Batch number: 051, Test: Loss: 9.8766, Accuracy: 0.3750\n",
            "Test Batch number: 052, Test: Loss: 6.7674, Accuracy: 0.5625\n",
            "Test Batch number: 053, Test: Loss: 6.9790, Accuracy: 0.5312\n",
            "Test Batch number: 054, Test: Loss: 6.3083, Accuracy: 0.5312\n",
            "Test Batch number: 055, Test: Loss: 8.1091, Accuracy: 0.5000\n",
            "Test Batch number: 056, Test: Loss: 5.4915, Accuracy: 0.5625\n",
            "Test Batch number: 057, Test: Loss: 11.2068, Accuracy: 0.3438\n",
            "Test Batch number: 058, Test: Loss: 6.5159, Accuracy: 0.5312\n",
            "Test Batch number: 059, Test: Loss: 8.1422, Accuracy: 0.5000\n",
            "Test Batch number: 060, Test: Loss: 5.4631, Accuracy: 0.6250\n",
            "Test Batch number: 061, Test: Loss: 5.8811, Accuracy: 0.5938\n",
            "Test Batch number: 062, Test: Loss: 7.0934, Accuracy: 0.5312\n",
            "Test Batch number: 063, Test: Loss: 8.6366, Accuracy: 0.5312\n",
            "Test Batch number: 064, Test: Loss: 8.2321, Accuracy: 0.4688\n",
            "Test Batch number: 065, Test: Loss: 5.1524, Accuracy: 0.6562\n",
            "Test Batch number: 066, Test: Loss: 7.3807, Accuracy: 0.4062\n",
            "Test Batch number: 067, Test: Loss: 8.5714, Accuracy: 0.4688\n",
            "Test Batch number: 068, Test: Loss: 7.8370, Accuracy: 0.5312\n",
            "Test Batch number: 069, Test: Loss: 7.0540, Accuracy: 0.5625\n",
            "Test Batch number: 070, Test: Loss: 5.2566, Accuracy: 0.6562\n",
            "Test Batch number: 071, Test: Loss: 11.3001, Accuracy: 0.3438\n",
            "Test Batch number: 072, Test: Loss: 6.7990, Accuracy: 0.5000\n",
            "Test Batch number: 073, Test: Loss: 9.4146, Accuracy: 0.3750\n",
            "Test Batch number: 074, Test: Loss: 5.9097, Accuracy: 0.5938\n",
            "Test Batch number: 075, Test: Loss: 6.8688, Accuracy: 0.5625\n",
            "Test Batch number: 076, Test: Loss: 8.8828, Accuracy: 0.4062\n",
            "Test Batch number: 077, Test: Loss: 9.6137, Accuracy: 0.4062\n",
            "Test Batch number: 078, Test: Loss: 6.4930, Accuracy: 0.5938\n",
            "Test Batch number: 079, Test: Loss: 5.1810, Accuracy: 0.6250\n",
            "Test Batch number: 080, Test: Loss: 8.7684, Accuracy: 0.4688\n",
            "Test Batch number: 081, Test: Loss: 8.0079, Accuracy: 0.4688\n",
            "Test Batch number: 082, Test: Loss: 8.8635, Accuracy: 0.4375\n",
            "Test Batch number: 083, Test: Loss: 8.8786, Accuracy: 0.3750\n",
            "Test Batch number: 084, Test: Loss: 8.1305, Accuracy: 0.4375\n",
            "Test Batch number: 085, Test: Loss: 8.3899, Accuracy: 0.5000\n",
            "Test Batch number: 086, Test: Loss: 7.5517, Accuracy: 0.5000\n",
            "Test Batch number: 087, Test: Loss: 6.7234, Accuracy: 0.5312\n",
            "Test Batch number: 088, Test: Loss: 8.9285, Accuracy: 0.3750\n",
            "Test Batch number: 089, Test: Loss: 8.3076, Accuracy: 0.4688\n",
            "Test Batch number: 090, Test: Loss: 7.8812, Accuracy: 0.4375\n",
            "Test Batch number: 091, Test: Loss: 5.8528, Accuracy: 0.6250\n",
            "Test Batch number: 092, Test: Loss: 9.8248, Accuracy: 0.4062\n",
            "Test Batch number: 093, Test: Loss: 7.8638, Accuracy: 0.5312\n",
            "Test Batch number: 094, Test: Loss: 7.5483, Accuracy: 0.5000\n",
            "Test Batch number: 095, Test: Loss: 9.1656, Accuracy: 0.4375\n",
            "Test Batch number: 096, Test: Loss: 6.7860, Accuracy: 0.5625\n",
            "Test Batch number: 097, Test: Loss: 8.1099, Accuracy: 0.4688\n",
            "Test Batch number: 098, Test: Loss: 7.7541, Accuracy: 0.5000\n",
            "Test Batch number: 099, Test: Loss: 6.6358, Accuracy: 0.5625\n",
            "Test Batch number: 100, Test: Loss: 8.7337, Accuracy: 0.4688\n",
            "Test Batch number: 101, Test: Loss: 9.0278, Accuracy: 0.3438\n",
            "Test Batch number: 102, Test: Loss: 6.5987, Accuracy: 0.5312\n",
            "Test Batch number: 103, Test: Loss: 8.2951, Accuracy: 0.5000\n",
            "Test Batch number: 104, Test: Loss: 10.2805, Accuracy: 0.3438\n",
            "Test Batch number: 105, Test: Loss: 5.9779, Accuracy: 0.5938\n",
            "Test Batch number: 106, Test: Loss: 9.6643, Accuracy: 0.3750\n",
            "Test Batch number: 107, Test: Loss: 9.3197, Accuracy: 0.4375\n",
            "Test Batch number: 108, Test: Loss: 7.0979, Accuracy: 0.5000\n",
            "Test Batch number: 109, Test: Loss: 7.1779, Accuracy: 0.5312\n",
            "Test Batch number: 110, Test: Loss: 8.5851, Accuracy: 0.5312\n",
            "Test Batch number: 111, Test: Loss: 4.4404, Accuracy: 0.7188\n",
            "Test Batch number: 112, Test: Loss: 5.9606, Accuracy: 0.5938\n",
            "Test Batch number: 113, Test: Loss: 10.1307, Accuracy: 0.4062\n",
            "Test Batch number: 114, Test: Loss: 6.5965, Accuracy: 0.5938\n",
            "Test Batch number: 115, Test: Loss: 6.5099, Accuracy: 0.5938\n",
            "Test Batch number: 116, Test: Loss: 7.9764, Accuracy: 0.4688\n",
            "Test Batch number: 117, Test: Loss: 9.3803, Accuracy: 0.4375\n",
            "Test Batch number: 118, Test: Loss: 6.9671, Accuracy: 0.5312\n",
            "Test Batch number: 119, Test: Loss: 6.4290, Accuracy: 0.6250\n",
            "Test Batch number: 120, Test: Loss: 6.5209, Accuracy: 0.5000\n",
            "Test Batch number: 121, Test: Loss: 7.9168, Accuracy: 0.5000\n",
            "Test Batch number: 122, Test: Loss: 10.7437, Accuracy: 0.3438\n",
            "Test Batch number: 123, Test: Loss: 5.0685, Accuracy: 0.5938\n",
            "Test Batch number: 124, Test: Loss: 7.7033, Accuracy: 0.4688\n",
            "Test Batch number: 125, Test: Loss: 6.3619, Accuracy: 0.5312\n",
            "Test Batch number: 126, Test: Loss: 8.3438, Accuracy: 0.4688\n",
            "Test Batch number: 127, Test: Loss: 8.4627, Accuracy: 0.5000\n",
            "Test Batch number: 128, Test: Loss: 6.6074, Accuracy: 0.6250\n",
            "Test Batch number: 129, Test: Loss: 8.1124, Accuracy: 0.4688\n",
            "Test Batch number: 130, Test: Loss: 8.6404, Accuracy: 0.4688\n",
            "Test Batch number: 131, Test: Loss: 9.9342, Accuracy: 0.4375\n",
            "Test Batch number: 132, Test: Loss: 5.4774, Accuracy: 0.5938\n",
            "Test Batch number: 133, Test: Loss: 7.9286, Accuracy: 0.4688\n",
            "Test Batch number: 134, Test: Loss: 9.2190, Accuracy: 0.4375\n",
            "Test Batch number: 135, Test: Loss: 9.2330, Accuracy: 0.4062\n",
            "Test Batch number: 136, Test: Loss: 9.5167, Accuracy: 0.4062\n",
            "Test Batch number: 137, Test: Loss: 6.2977, Accuracy: 0.6250\n",
            "Test Batch number: 138, Test: Loss: 6.4601, Accuracy: 0.5938\n",
            "Test Batch number: 139, Test: Loss: 8.3032, Accuracy: 0.5000\n",
            "Test Batch number: 140, Test: Loss: 7.3989, Accuracy: 0.5000\n",
            "Test Batch number: 141, Test: Loss: 5.5389, Accuracy: 0.5938\n",
            "Test Batch number: 142, Test: Loss: 8.7454, Accuracy: 0.4375\n",
            "Test Batch number: 143, Test: Loss: 7.6170, Accuracy: 0.5312\n",
            "Test Batch number: 144, Test: Loss: 9.9616, Accuracy: 0.4062\n",
            "Test Batch number: 145, Test: Loss: 7.6822, Accuracy: 0.5625\n",
            "Test Batch number: 146, Test: Loss: 7.1914, Accuracy: 0.5000\n",
            "Test Batch number: 147, Test: Loss: 7.6900, Accuracy: 0.5938\n",
            "Test Batch number: 148, Test: Loss: 8.3652, Accuracy: 0.4688\n",
            "Test Batch number: 149, Test: Loss: 6.1126, Accuracy: 0.4688\n",
            "Test Batch number: 150, Test: Loss: 5.6917, Accuracy: 0.5938\n",
            "Test Batch number: 151, Test: Loss: 9.7428, Accuracy: 0.3750\n",
            "Test Batch number: 152, Test: Loss: 8.5072, Accuracy: 0.4688\n",
            "Test Batch number: 153, Test: Loss: 8.7417, Accuracy: 0.4375\n",
            "Test Batch number: 154, Test: Loss: 6.6590, Accuracy: 0.5625\n",
            "Test Batch number: 155, Test: Loss: 7.4020, Accuracy: 0.5938\n",
            "Test Batch number: 156, Test: Loss: 6.0522, Accuracy: 0.6250\n",
            "Test Batch number: 157, Test: Loss: 8.6296, Accuracy: 0.4688\n",
            "Test Batch number: 158, Test: Loss: 4.9433, Accuracy: 0.6875\n",
            "Test Batch number: 159, Test: Loss: 8.5426, Accuracy: 0.4688\n",
            "Test Batch number: 160, Test: Loss: 9.6802, Accuracy: 0.3438\n",
            "Test Batch number: 161, Test: Loss: 8.7067, Accuracy: 0.4062\n",
            "Test Batch number: 162, Test: Loss: 9.2235, Accuracy: 0.4062\n",
            "Test Batch number: 163, Test: Loss: 8.5058, Accuracy: 0.5000\n",
            "Test Batch number: 164, Test: Loss: 8.2816, Accuracy: 0.5000\n",
            "Test Batch number: 165, Test: Loss: 7.9753, Accuracy: 0.4375\n",
            "Test Batch number: 166, Test: Loss: 8.1145, Accuracy: 0.4688\n",
            "Test Batch number: 167, Test: Loss: 8.6818, Accuracy: 0.4062\n",
            "Test Batch number: 168, Test: Loss: 8.4162, Accuracy: 0.5312\n",
            "Test Batch number: 169, Test: Loss: 9.1573, Accuracy: 0.4375\n",
            "Test Batch number: 170, Test: Loss: 12.0559, Accuracy: 0.2812\n",
            "Test Batch number: 171, Test: Loss: 8.4656, Accuracy: 0.5312\n",
            "Test Batch number: 172, Test: Loss: 7.2185, Accuracy: 0.5312\n",
            "Test Batch number: 173, Test: Loss: 7.7317, Accuracy: 0.4688\n",
            "Test Batch number: 174, Test: Loss: 7.0252, Accuracy: 0.5625\n",
            "Test Batch number: 175, Test: Loss: 10.2455, Accuracy: 0.4062\n",
            "Test Batch number: 176, Test: Loss: 6.9011, Accuracy: 0.5625\n",
            "Test Batch number: 177, Test: Loss: 6.7528, Accuracy: 0.5625\n",
            "Test Batch number: 178, Test: Loss: 6.8695, Accuracy: 0.5312\n",
            "Test Batch number: 179, Test: Loss: 9.1259, Accuracy: 0.4375\n",
            "Test Batch number: 180, Test: Loss: 7.2143, Accuracy: 0.5312\n",
            "Test Batch number: 181, Test: Loss: 8.7464, Accuracy: 0.3750\n",
            "Test Batch number: 182, Test: Loss: 6.1530, Accuracy: 0.5938\n",
            "Test Batch number: 183, Test: Loss: 7.4648, Accuracy: 0.5000\n",
            "Test Batch number: 184, Test: Loss: 6.2541, Accuracy: 0.6250\n",
            "Test Batch number: 185, Test: Loss: 8.7820, Accuracy: 0.4375\n",
            "Test Batch number: 186, Test: Loss: 9.4668, Accuracy: 0.3750\n",
            "Test Batch number: 187, Test: Loss: 7.8583, Accuracy: 0.4688\n",
            "Test Batch number: 188, Test: Loss: 7.0421, Accuracy: 0.4688\n",
            "Test Batch number: 189, Test: Loss: 6.5461, Accuracy: 0.5625\n",
            "Test Batch number: 190, Test: Loss: 9.4280, Accuracy: 0.4375\n",
            "Test Batch number: 191, Test: Loss: 8.6330, Accuracy: 0.4375\n",
            "Test Batch number: 192, Test: Loss: 10.5163, Accuracy: 0.4062\n",
            "Test Batch number: 193, Test: Loss: 6.7187, Accuracy: 0.5938\n",
            "Test Batch number: 194, Test: Loss: 7.4730, Accuracy: 0.4688\n",
            "Test Batch number: 195, Test: Loss: 7.9634, Accuracy: 0.4688\n",
            "Test Batch number: 196, Test: Loss: 9.6752, Accuracy: 0.4062\n",
            "Test Batch number: 197, Test: Loss: 8.1194, Accuracy: 0.5312\n",
            "Test Batch number: 198, Test: Loss: 5.1159, Accuracy: 0.6562\n",
            "Test Batch number: 199, Test: Loss: 8.8876, Accuracy: 0.4062\n",
            "Test Batch number: 200, Test: Loss: 7.4834, Accuracy: 0.5312\n",
            "Test Batch number: 201, Test: Loss: 7.7102, Accuracy: 0.5312\n",
            "Test Batch number: 202, Test: Loss: 6.6989, Accuracy: 0.5000\n",
            "Test Batch number: 203, Test: Loss: 10.3133, Accuracy: 0.3438\n",
            "Test Batch number: 204, Test: Loss: 7.5705, Accuracy: 0.5000\n",
            "Test Batch number: 205, Test: Loss: 6.0227, Accuracy: 0.6562\n",
            "Test Batch number: 206, Test: Loss: 10.0249, Accuracy: 0.4062\n",
            "Test Batch number: 207, Test: Loss: 8.0074, Accuracy: 0.5312\n",
            "Test Batch number: 208, Test: Loss: 5.1911, Accuracy: 0.6562\n",
            "Test Batch number: 209, Test: Loss: 5.7107, Accuracy: 0.6250\n",
            "Test Batch number: 210, Test: Loss: 6.6436, Accuracy: 0.5000\n",
            "Test Batch number: 211, Test: Loss: 7.1818, Accuracy: 0.5000\n",
            "Test Batch number: 212, Test: Loss: 11.3510, Accuracy: 0.4062\n",
            "Test Batch number: 213, Test: Loss: 7.0870, Accuracy: 0.5312\n",
            "Test Batch number: 214, Test: Loss: 9.8060, Accuracy: 0.3438\n",
            "Test Batch number: 215, Test: Loss: 9.9404, Accuracy: 0.3438\n",
            "Test accuracy : 0.5002893518518519\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}