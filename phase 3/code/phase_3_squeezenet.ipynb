{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "phase_3_squeezenet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpGC6nSSUxv3",
        "outputId": "850ea19c-c612-4478-ff1a-16330b0b6f0a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4bgeSpmWRKL",
        "outputId": "2a4d03f8-6d95-486a-c7a0-48e0bb30a8ba"
      },
      "source": [
        "pip install torchvision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8ar3proWXa3"
      },
      "source": [
        "from torchvision import models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "ce5fcf30e54848f0801c3abab4682072",
            "f870a6d1dec9439ea0db3fe8f2e00dab",
            "adf883903af746b69fcad054a76b4bb7",
            "e1b848448ce645f9b03f4498c3612658",
            "03b55b3acd5f4df891f01937d96873b2",
            "75927faa6f0e4a6fb8f7975f972d121f",
            "49ec5757c5584b37ad7a09d73d156a7d",
            "d3bc9b2a7ebc4e8faa3cc2dbca43b242"
          ]
        },
        "id": "C3gnsUPyWcBu",
        "outputId": "6a319255-3cdf-4e4e-8018-8d2ed160dfc5"
      },
      "source": [
        "squeezenet = models.squeezenet1_0(pretrained=True)\n",
        "for name,child in squeezenet.named_children():\n",
        "  print(name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_0-a815701f.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_0-a815701f.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce5fcf30e54848f0801c3abab4682072",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=5017600.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "features\n",
            "classifier\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpEiRAZzWiaa"
      },
      "source": [
        "use_gpu = torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOqRbN62zGkN"
      },
      "source": [
        "def save_checkpoint(state, filename=\"/content/drive/MyDrive/Checkpoints/phase3_squeezenet_transparent_trial_1.pth\"):\n",
        "  print(\"Saving checkpoint\")\n",
        "  torch.save(state, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F6bXMuPYPb7"
      },
      "source": [
        "from torchvision import transforms\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])\n",
        "image_transforms = transforms.Compose([\n",
        "                            transforms.Resize((224,224)),\n",
        "                            transforms.ColorJitter(hue=.05, saturation=.05),\n",
        "                            transforms.RandomRotation(degrees=15),\n",
        "                            transforms.RandomHorizontalFlip(),\n",
        "                            transforms.ToTensor(),\n",
        "                            normalize,\n",
        "                        ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTzRMYFFYVAC",
        "outputId": "1713de2f-62e2-4ff6-ae91-73272d50768c"
      },
      "source": [
        "from torchvision import *\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Load the Data\n",
        "\n",
        "# Set train and valid directory paths\n",
        "train_directory = '/content/drive/MyDrive/img final dset/Train'\n",
        "valid_directory = '/content/drive/MyDrive/img final dset/Test'\n",
        "\n",
        "# Batch size\n",
        "bs = 32\n",
        "\n",
        "# Load Data from folders\n",
        "data = {\n",
        "    'train': datasets.ImageFolder(train_directory, image_transforms ),\n",
        "    'test': datasets.ImageFolder(valid_directory, image_transforms )\n",
        "}\n",
        "\n",
        "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
        "train_data_size = len(data['train'])\n",
        "test_data_size = len(data['test'])\n",
        "\n",
        "# Create iterators for the Data loaded using DataLoader module\n",
        "train_data = DataLoader(data['train'], batch_size=bs, shuffle=True)\n",
        "test_data = DataLoader(data['test'], batch_size=bs, shuffle=True)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Print the train, validation and test set data sizes\n",
        "train_data_size, test_data_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13824, 6912)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szplUSxpdzSo"
      },
      "source": [
        "# HYPER-PARAMETERS\n",
        "epochs=60\n",
        "train_only_last_layer = False # boolean variable\n",
        "num_of_output_classes=2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s65rJ2Lid2_b"
      },
      "source": [
        "# Learning rate scheduler\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^\n",
        "# Let's create our learning rate scheduler. We will exponentially\n",
        "# decrease the learning rate once every few epochs.\n",
        "\n",
        "\"\"\"This function is useful only if using SGD otherwise no use\"\"\"\n",
        "\n",
        "def exp_lr_scheduler(optimizer, epoch, init_lr=0.001, lr_decay_epoch=7):\n",
        "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
        "    lr = init_lr * (0.1**(epoch // lr_decay_epoch))\n",
        "\n",
        "    if epoch % lr_decay_epoch == 0:\n",
        "        print('LR is set to {}'.format(lr))\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXopzsXwd7gx"
      },
      "source": [
        "if use_gpu:\n",
        "    squeezenet = squeezenet.cuda()\n",
        "\n",
        "# defining loss criterion, for these\n",
        "# models CrossEntropyLoss works the best\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opoosed to before.\n",
        "\"\"\" Defining an optimiser function here, can use Adam, RMSprop or simple SGD\"\"\"\n",
        "optimizer_conv = optim.SGD(squeezenet.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEI-qCRzeEow"
      },
      "source": [
        "# Training the model\n",
        "# ------------------\n",
        "#\n",
        "# -  Saving (deep copying) the best model\n",
        "#\n",
        "# In the following, parameter ``lr_scheduler(optimizer, epoch)``\n",
        "# is a function  which modifies ``optimizer`` so that the learning\n",
        "# rate is changed according to desired schedule.\n",
        "\n",
        "def train_model(model, criterion, optimizer, lr_scheduler, num_epochs):\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "     epoch_start = time.time()\n",
        "     print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "     \n",
        "     if epoch==1:\n",
        "       checkpoint= {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch, 'loss': loss}\n",
        "       save_checkpoint(checkpoint)\n",
        "     \n",
        "     # Set to training mode\n",
        "     squeezenet.train()\n",
        "     \n",
        "     # Loss and Accuracy within the epoch\n",
        "     train_loss = 0.0\n",
        "     train_acc = 0.0\n",
        "     \n",
        "     valid_loss = 0.0\n",
        "     valid_acc = 0.0\n",
        "     \n",
        "     for i, (inputs, labels) in enumerate(train_data):\n",
        "       \n",
        "       inputs = inputs.to(device)\n",
        "       labels = labels.to(device)\n",
        "       \n",
        "       # Clean existing gradients\n",
        "       optimizer_conv.zero_grad()\n",
        "       \n",
        "       # Forward pass - compute outputs on input data using the model\n",
        "       outputs = squeezenet(inputs)\n",
        "       \n",
        "       # Compute loss\n",
        "       loss = criterion(outputs, labels)\n",
        "       \n",
        "       # Backpropagate the gradients\n",
        "       loss.backward()\n",
        "       \n",
        "       # Update the parameters\n",
        "       optimizer_conv.step()\n",
        "       \n",
        "       # Compute the total loss for the batch and add it to train_loss\n",
        "       train_loss += loss.item() * inputs.size(0)\n",
        "       \n",
        "       # Compute the accuracy\n",
        "       ret, predictions = torch.max(outputs.data, 1)\n",
        "       correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "       \n",
        "       # Convert correct_counts to float and then compute the mean\n",
        "       acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "       \n",
        "       # Compute total accuracy in the whole batch and add to train_acc\n",
        "       train_acc += acc.item() * inputs.size(0)\n",
        "       print(\"Batch number: {:03d}, Training:  Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "690nozMxgiGI",
        "outputId": "0ca6f877-3e4d-4098-f0ad-ba9bee9ceb54"
      },
      "source": [
        "train_model(squeezenet,criterion,optimizer_conv,exp_lr_scheduler,epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Batch number: 195, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 196, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 197, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 198, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 199, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 200, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 201, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 202, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 203, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 204, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 205, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 206, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 207, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 208, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 209, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 210, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 211, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 212, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 213, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 214, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 215, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 216, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 217, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 218, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 219, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 220, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 221, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 222, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 223, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 224, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 225, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 226, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 227, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 228, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 229, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 230, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 231, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 232, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 233, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 234, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 235, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 236, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 237, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 238, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 239, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 240, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 241, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 242, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 243, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 244, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 245, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 246, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 247, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 248, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 249, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 250, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 251, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 252, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 253, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 254, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 255, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 256, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 257, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 258, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 259, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 260, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 261, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 262, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 263, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 264, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 265, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 266, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 267, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 268, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 269, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 270, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 271, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 272, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 273, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 274, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 275, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 276, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 277, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 278, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 279, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 280, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 281, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 282, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 283, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 284, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 285, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 286, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 287, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 288, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 289, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 290, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 291, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 292, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 293, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 294, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 295, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 296, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 297, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 298, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 299, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 300, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 301, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 302, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 303, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 304, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 305, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 306, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 307, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 308, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 309, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 310, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 311, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 312, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 313, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 314, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 315, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 316, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 317, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 318, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 319, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 320, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 321, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 322, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 323, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 324, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 325, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 326, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 327, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 328, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 329, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 330, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 331, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 332, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 333, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 334, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 335, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 336, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 337, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 338, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 339, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 340, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 341, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 342, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 343, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 344, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 345, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 346, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 347, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 348, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 349, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 350, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 351, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 352, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 353, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 354, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 355, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 356, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 357, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 358, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 359, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 360, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 361, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 362, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 363, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 364, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 365, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 366, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 367, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 368, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 369, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 370, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 371, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 372, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 373, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 374, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 375, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 376, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 377, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 378, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 379, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 380, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 381, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 382, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 383, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 384, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 385, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 386, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 387, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 388, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 389, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 390, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 391, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 392, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 393, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 394, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 395, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 396, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 397, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 398, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 399, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 400, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 401, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 402, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 403, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 404, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 405, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 406, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 407, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 408, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 409, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 410, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 411, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 412, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 413, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 414, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 415, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 416, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 417, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 418, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 419, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 420, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 421, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 422, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 423, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 424, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 425, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 426, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 427, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 428, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 429, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 430, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 431, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Epoch: 50/60\n",
            "Batch number: 000, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 001, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 002, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 003, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 004, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 005, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 006, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 007, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 008, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 009, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 010, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 011, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 012, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 013, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 014, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 015, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 016, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 017, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 018, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 019, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 020, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 021, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 022, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 023, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 024, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 025, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 026, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 027, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 028, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 029, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 030, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 031, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 032, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 033, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 034, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 035, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 036, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 037, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 038, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 039, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 040, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 041, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 042, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 043, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 044, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 045, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 046, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 047, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 048, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 049, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 050, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 051, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 052, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 053, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 054, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 055, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 056, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 057, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 058, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 059, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 060, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 061, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 062, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 063, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 064, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 065, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 066, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 067, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 068, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 069, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 070, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 071, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 072, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 073, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 074, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 075, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 076, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 077, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 078, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 079, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 080, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 081, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 082, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 083, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 084, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 085, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 086, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 087, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 088, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 089, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 090, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 091, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 092, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 093, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 094, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 095, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 096, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 097, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 098, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 099, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 100, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 101, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 102, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 103, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 104, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 105, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 106, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 107, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 108, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 109, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 110, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 111, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 112, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 113, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 114, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 115, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 116, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 117, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 118, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 119, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 120, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 121, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 122, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 123, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 124, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 125, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 126, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 127, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 128, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 129, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 130, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 131, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 132, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 133, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 134, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 135, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 136, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 137, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 138, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 139, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 140, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 141, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 142, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 143, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 144, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 145, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 146, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 147, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 148, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 149, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 150, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 151, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 152, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 153, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 154, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 155, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 156, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 157, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 158, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 159, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 160, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 161, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 162, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 163, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 164, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 165, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 166, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 167, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 168, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 169, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 170, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 171, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 172, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 173, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 174, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 175, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 176, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 177, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 178, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 179, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 180, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 181, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 182, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 183, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 184, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 185, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 186, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 187, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 188, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 189, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 190, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 191, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 192, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 193, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 194, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 195, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 196, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 197, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 198, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 199, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 200, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 201, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 202, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 203, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 204, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 205, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 206, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 207, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 208, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 209, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 210, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 211, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 212, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 213, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 214, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 215, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 216, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 217, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 218, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 219, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 220, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 221, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 222, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 223, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 224, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 225, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 226, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 227, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 228, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 229, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 230, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 231, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 232, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 233, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 234, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 235, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 236, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 237, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 238, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 239, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 240, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 241, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 242, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 243, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 244, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 245, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 246, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 247, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 248, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 249, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 250, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 251, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 252, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 253, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 254, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 255, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 256, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 257, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 258, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 259, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 260, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 261, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 262, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 263, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 264, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 265, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 266, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 267, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 268, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 269, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 270, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 271, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 272, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 273, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 274, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 275, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 276, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 277, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 278, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 279, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 280, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 281, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 282, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 283, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 284, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 285, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 286, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 287, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 288, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 289, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 290, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 291, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 292, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 293, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 294, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 295, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 296, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 297, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 298, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 299, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 300, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 301, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 302, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 303, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 304, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 305, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 306, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 307, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 308, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 309, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 310, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 311, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 312, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 313, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 314, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 315, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 316, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 317, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 318, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 319, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 320, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 321, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 322, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 323, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 324, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 325, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 326, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 327, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 328, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 329, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 330, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 331, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 332, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 333, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 334, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 335, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 336, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 337, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 338, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 339, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 340, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 341, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 342, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 343, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 344, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 345, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 346, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 347, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 348, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 349, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 350, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 351, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 352, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 353, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 354, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 355, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 356, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 357, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 358, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 359, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 360, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 361, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 362, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 363, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 364, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 365, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 366, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 367, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 368, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 369, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 370, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 371, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 372, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 373, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 374, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 375, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 376, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 377, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 378, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 379, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 380, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 381, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 382, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 383, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 384, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 385, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 386, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 387, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 388, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 389, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 390, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 391, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 392, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 393, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 394, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 395, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 396, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 397, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 398, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 399, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 400, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 401, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 402, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 403, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 404, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 405, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 406, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 407, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 408, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 409, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 410, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 411, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 412, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 413, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 414, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 415, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 416, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 417, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 418, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 419, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 420, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 421, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 422, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 423, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 424, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 425, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 426, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 427, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 428, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 429, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 430, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 431, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Epoch: 51/60\n",
            "Batch number: 000, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 001, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 002, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 003, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 004, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 005, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 006, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 007, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 008, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 009, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 010, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 011, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 012, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 013, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 014, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 015, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 016, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 017, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 018, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 019, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 020, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 021, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 022, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 023, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 024, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 025, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 026, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 027, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 028, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 029, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 030, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 031, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 032, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 033, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 034, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 035, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 036, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 037, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 038, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 039, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 040, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 041, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 042, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 043, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 044, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 045, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 046, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 047, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 048, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 049, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 050, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 051, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 052, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 053, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 054, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 055, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 056, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 057, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 058, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 059, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 060, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 061, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 062, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 063, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 064, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 065, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 066, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 067, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 068, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 069, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 070, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 071, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 072, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 073, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 074, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 075, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 076, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 077, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 078, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 079, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 080, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 081, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 082, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 083, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 084, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 085, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 086, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 087, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 088, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 089, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 090, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 091, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 092, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 093, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 094, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 095, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 096, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 097, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 098, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 099, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 100, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 101, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 102, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 103, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 104, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 105, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 106, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 107, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 108, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 109, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 110, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 111, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 112, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 113, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 114, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 115, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 116, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 117, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 118, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 119, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 120, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 121, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 122, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 123, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 124, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 125, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 126, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 127, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 128, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 129, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 130, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 131, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 132, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 133, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 134, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 135, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 136, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 137, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 138, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 139, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 140, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 141, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 142, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 143, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 144, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 145, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 146, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 147, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 148, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 149, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 150, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 151, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 152, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 153, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 154, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 155, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 156, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 157, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 158, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 159, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 160, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 161, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 162, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 163, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 164, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 165, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 166, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 167, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 168, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 169, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 170, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 171, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 172, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 173, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 174, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 175, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 176, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 177, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 178, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 179, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 180, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 181, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 182, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 183, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 184, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 185, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 186, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 187, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 188, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 189, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 190, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 191, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 192, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 193, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 194, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 195, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 196, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 197, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 198, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 199, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 200, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 201, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 202, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 203, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 204, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 205, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 206, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 207, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 208, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 209, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 210, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 211, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 212, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 213, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 214, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 215, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 216, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 217, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 218, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 219, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 220, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 221, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 222, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 223, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 224, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 225, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 226, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 227, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 228, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 229, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 230, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 231, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 232, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 233, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 234, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 235, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 236, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 237, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 238, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 239, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 240, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 241, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 242, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 243, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 244, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 245, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 246, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 247, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 248, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 249, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 250, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 251, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 252, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 253, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 254, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 255, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 256, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 257, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 258, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 259, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 260, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 261, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 262, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 263, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 264, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 265, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 266, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 267, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 268, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 269, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 270, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 271, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 272, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 273, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 274, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 275, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 276, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 277, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 278, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 279, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 280, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 281, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 282, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 283, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 284, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 285, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 286, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 287, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 288, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 289, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 290, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 291, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 292, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 293, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 294, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 295, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 296, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 297, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 298, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 299, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 300, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 301, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 302, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 303, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 304, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 305, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 306, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 307, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 308, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 309, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 310, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 311, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 312, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 313, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 314, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 315, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 316, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 317, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 318, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 319, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 320, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 321, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 322, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 323, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 324, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 325, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 326, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 327, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 328, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 329, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 330, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 331, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 332, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 333, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 334, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 335, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 336, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 337, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 338, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 339, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 340, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 341, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 342, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 343, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 344, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 345, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 346, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 347, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 348, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 349, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 350, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 351, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 352, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 353, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 354, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 355, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 356, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 357, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 358, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 359, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 360, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 361, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 362, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 363, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 364, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 365, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 366, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 367, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 368, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 369, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 370, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 371, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 372, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 373, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 374, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 375, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 376, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 377, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 378, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 379, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 380, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 381, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 382, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 383, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 384, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 385, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 386, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 387, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 388, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 389, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 390, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 391, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 392, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 393, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 394, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 395, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 396, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 397, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 398, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 399, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 400, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 401, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 402, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 403, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 404, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 405, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 406, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 407, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 408, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 409, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 410, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 411, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 412, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 413, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 414, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 415, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 416, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 417, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 418, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 419, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 420, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 421, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 422, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 423, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 424, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 425, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 426, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 427, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 428, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 429, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 430, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 431, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Epoch: 52/60\n",
            "Batch number: 000, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 001, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 002, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 003, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 004, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 005, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 006, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 007, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 008, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 009, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 010, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 011, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 012, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 013, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 014, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 015, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 016, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 017, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 018, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 019, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 020, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 021, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 022, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 023, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 024, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 025, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 026, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 027, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 028, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 029, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 030, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 031, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 032, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 033, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 034, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 035, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 036, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 037, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 038, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 039, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 040, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 041, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 042, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 043, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 044, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 045, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 046, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 047, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 048, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 049, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 050, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 051, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 052, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 053, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 054, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 055, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 056, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 057, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 058, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 059, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 060, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 061, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 062, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 063, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 064, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 065, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 066, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 067, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 068, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 069, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 070, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 071, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 072, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 073, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 074, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 075, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 076, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 077, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 078, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 079, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 080, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 081, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 082, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 083, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 084, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 085, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 086, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 087, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 088, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 089, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 090, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 091, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 092, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 093, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 094, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 095, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 096, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 097, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 098, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 099, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 100, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 101, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 102, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 103, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 104, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 105, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 106, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 107, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 108, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 109, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 110, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 111, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 112, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 113, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 114, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 115, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 116, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 117, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 118, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 119, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 120, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 121, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 122, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 123, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 124, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 125, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 126, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 127, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 128, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 129, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 130, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 131, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 132, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 133, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 134, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 135, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 136, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 137, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 138, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 139, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 140, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 141, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 142, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 143, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 144, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 145, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 146, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 147, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 148, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 149, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 150, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 151, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 152, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 153, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 154, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 155, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 156, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 157, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 158, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 159, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 160, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 161, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 162, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 163, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 164, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 165, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 166, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 167, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 168, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 169, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 170, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 171, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 172, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 173, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 174, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 175, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 176, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 177, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 178, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 179, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 180, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 181, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 182, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 183, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 184, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 185, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 186, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 187, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 188, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 189, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 190, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 191, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 192, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 193, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 194, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 195, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 196, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 197, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 198, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 199, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 200, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 201, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 202, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 203, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 204, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 205, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 206, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 207, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 208, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 209, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 210, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 211, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 212, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 213, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 214, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 215, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 216, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 217, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 218, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 219, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 220, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 221, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 222, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 223, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 224, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 225, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 226, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 227, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 228, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 229, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 230, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 231, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 232, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 233, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 234, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 235, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 236, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 237, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 238, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 239, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 240, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 241, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 242, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 243, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 244, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 245, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 246, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 247, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 248, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 249, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 250, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 251, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 252, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 253, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 254, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 255, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 256, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 257, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 258, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 259, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 260, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 261, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 262, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 263, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 264, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 265, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 266, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 267, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 268, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 269, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 270, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 271, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 272, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 273, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 274, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 275, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 276, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 277, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 278, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 279, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 280, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 281, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 282, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 283, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 284, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 285, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 286, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 287, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 288, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 289, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 290, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 291, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 292, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 293, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 294, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 295, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 296, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 297, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 298, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 299, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 300, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 301, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 302, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 303, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 304, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 305, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 306, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 307, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 308, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 309, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 310, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 311, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 312, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 313, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 314, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 315, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 316, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 317, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 318, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 319, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 320, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 321, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 322, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 323, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 324, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 325, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 326, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 327, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 328, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 329, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 330, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 331, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 332, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 333, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 334, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 335, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 336, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 337, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 338, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 339, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 340, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 341, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 342, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 343, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 344, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 345, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 346, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 347, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 348, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 349, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 350, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 351, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 352, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 353, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 354, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 355, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 356, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 357, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 358, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 359, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 360, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 361, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 362, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 363, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 364, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 365, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 366, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 367, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 368, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 369, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 370, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 371, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 372, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 373, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 374, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 375, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 376, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 377, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 378, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 379, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 380, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 381, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 382, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 383, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 384, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 385, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 386, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 387, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 388, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 389, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 390, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 391, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 392, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 393, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 394, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 395, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 396, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 397, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 398, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 399, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 400, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 401, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 402, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 403, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 404, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 405, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 406, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 407, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 408, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 409, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 410, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 411, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 412, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 413, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 414, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 415, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 416, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 417, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 418, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 419, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 420, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 421, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 422, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 423, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 424, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 425, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 426, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 427, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 428, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 429, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 430, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 431, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Epoch: 53/60\n",
            "Batch number: 000, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 001, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 002, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 003, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 004, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 005, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 006, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 007, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 008, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 009, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 010, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 011, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 012, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 013, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 014, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 015, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 016, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 017, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 018, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 019, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 020, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 021, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 022, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 023, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 024, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 025, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 026, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 027, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 028, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 029, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 030, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 031, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 032, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 033, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 034, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 035, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 036, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 037, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 038, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 039, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 040, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 041, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 042, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 043, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 044, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 045, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 046, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 047, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 048, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 049, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 050, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 051, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 052, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 053, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 054, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 055, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 056, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 057, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 058, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 059, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 060, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 061, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 062, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 063, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 064, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 065, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 066, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 067, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 068, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 069, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 070, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 071, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 072, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 073, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 074, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 075, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 076, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 077, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 078, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 079, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 080, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 081, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 082, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 083, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 084, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 085, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 086, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 087, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 088, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 089, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 090, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 091, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 092, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 093, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 094, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 095, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 096, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 097, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 098, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 099, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 100, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 101, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 102, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 103, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 104, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 105, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 106, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 107, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 108, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 109, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 110, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 111, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 112, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 113, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 114, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 115, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 116, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 117, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 118, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 119, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 120, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 121, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 122, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 123, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 124, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 125, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 126, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 127, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 128, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 129, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 130, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 131, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 132, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 133, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 134, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 135, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 136, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 137, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 138, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 139, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 140, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 141, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 142, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 143, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 144, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 145, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 146, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 147, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 148, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 149, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 150, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 151, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 152, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 153, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 154, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 155, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 156, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 157, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 158, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 159, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 160, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 161, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 162, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 163, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 164, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 165, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 166, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 167, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 168, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 169, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 170, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 171, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 172, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 173, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 174, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 175, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 176, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 177, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 178, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 179, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 180, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 181, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 182, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 183, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 184, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 185, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 186, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 187, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 188, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 189, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 190, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 191, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 192, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 193, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 194, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 195, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 196, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 197, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 198, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 199, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 200, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 201, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 202, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 203, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 204, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 205, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 206, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 207, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 208, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 209, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 210, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 211, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 212, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 213, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 214, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 215, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 216, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 217, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 218, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 219, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 220, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 221, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 222, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 223, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 224, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 225, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 226, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 227, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 228, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 229, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 230, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 231, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 232, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 233, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 234, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 235, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 236, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 237, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 238, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 239, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 240, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 241, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 242, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 243, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 244, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 245, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 246, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 247, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 248, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 249, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 250, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 251, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 252, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 253, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 254, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 255, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 256, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 257, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 258, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 259, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 260, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 261, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 262, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 263, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 264, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 265, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 266, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 267, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 268, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 269, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 270, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 271, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 272, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 273, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 274, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 275, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 276, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 277, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 278, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 279, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 280, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 281, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 282, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 283, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 284, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 285, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 286, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 287, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 288, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 289, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 290, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 291, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 292, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 293, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 294, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 295, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 296, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 297, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 298, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 299, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 300, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 301, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 302, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 303, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 304, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 305, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 306, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 307, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 308, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 309, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 310, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 311, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 312, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 313, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 314, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 315, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 316, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 317, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 318, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 319, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 320, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 321, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 322, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 323, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 324, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 325, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 326, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 327, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 328, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 329, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 330, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 331, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 332, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 333, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 334, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 335, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 336, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 337, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 338, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 339, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 340, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 341, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 342, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 343, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 344, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 345, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 346, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 347, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 348, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 349, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 350, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 351, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 352, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 353, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 354, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 355, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 356, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 357, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 358, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 359, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 360, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 361, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 362, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 363, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 364, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 365, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 366, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 367, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 368, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 369, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 370, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 371, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 372, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 373, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 374, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 375, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 376, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 377, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 378, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 379, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 380, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 381, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 382, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 383, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 384, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 385, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 386, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 387, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 388, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 389, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 390, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 391, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 392, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 393, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 394, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 395, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 396, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 397, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 398, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 399, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 400, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 401, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 402, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 403, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 404, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 405, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 406, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 407, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 408, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 409, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 410, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 411, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 412, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 413, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 414, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 415, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 416, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 417, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 418, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 419, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 420, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 421, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 422, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 423, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 424, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 425, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 426, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 427, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 428, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 429, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 430, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 431, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Epoch: 54/60\n",
            "Batch number: 000, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 001, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 002, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 003, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 004, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 005, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 006, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 007, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 008, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 009, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 010, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 011, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 012, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 013, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 014, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 015, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 016, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 017, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 018, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 019, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 020, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 021, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 022, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 023, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 024, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 025, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 026, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 027, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 028, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 029, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 030, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 031, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 032, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 033, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 034, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 035, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 036, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 037, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 038, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 039, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 040, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 041, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 042, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 043, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 044, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 045, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 046, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 047, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 048, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 049, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 050, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 051, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 052, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 053, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 054, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 055, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 056, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 057, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 058, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 059, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 060, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 061, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 062, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 063, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 064, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 065, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 066, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 067, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 068, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 069, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 070, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 071, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 072, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 073, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 074, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 075, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 076, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 077, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 078, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 079, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 080, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 081, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 082, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 083, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 084, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 085, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 086, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 087, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 088, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 089, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 090, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 091, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 092, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 093, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 094, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 095, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 096, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 097, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 098, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 099, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 100, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 101, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 102, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 103, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 104, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 105, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 106, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 107, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 108, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 109, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 110, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 111, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 112, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 113, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 114, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 115, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 116, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 117, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 118, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 119, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 120, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 121, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 122, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 123, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 124, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 125, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 126, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 127, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 128, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 129, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 130, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 131, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 132, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 133, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 134, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 135, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 136, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 137, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 138, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 139, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 140, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 141, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 142, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 143, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 144, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 145, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 146, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 147, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 148, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 149, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 150, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 151, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 152, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 153, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 154, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 155, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 156, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 157, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 158, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 159, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 160, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 161, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 162, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 163, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 164, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 165, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 166, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 167, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 168, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 169, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 170, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 171, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 172, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 173, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 174, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 175, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 176, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 177, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 178, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 179, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 180, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 181, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 182, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 183, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 184, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 185, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 186, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 187, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 188, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 189, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 190, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 191, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 192, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 193, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 194, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 195, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 196, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 197, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 198, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 199, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 200, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 201, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 202, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 203, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 204, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 205, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 206, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 207, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 208, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 209, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 210, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 211, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 212, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 213, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 214, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 215, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 216, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 217, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 218, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 219, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 220, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 221, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 222, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 223, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 224, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 225, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 226, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 227, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 228, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 229, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 230, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 231, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 232, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 233, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 234, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 235, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 236, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 237, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 238, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 239, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 240, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 241, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 242, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 243, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 244, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 245, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 246, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 247, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 248, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 249, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 250, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 251, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 252, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 253, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 254, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 255, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 256, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 257, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 258, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 259, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 260, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 261, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 262, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 263, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 264, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 265, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 266, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 267, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 268, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 269, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 270, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 271, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 272, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 273, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 274, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 275, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 276, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 277, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 278, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 279, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 280, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 281, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 282, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 283, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 284, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 285, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 286, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 287, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 288, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 289, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 290, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 291, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 292, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 293, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 294, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 295, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 296, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 297, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 298, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 299, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 300, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 301, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 302, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 303, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 304, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 305, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 306, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 307, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 308, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 309, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 310, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 311, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 312, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 313, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 314, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 315, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 316, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 317, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 318, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 319, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 320, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 321, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 322, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 323, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 324, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 325, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 326, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 327, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 328, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 329, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 330, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 331, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 332, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 333, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 334, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 335, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 336, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 337, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 338, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 339, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 340, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 341, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 342, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 343, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 344, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 345, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 346, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 347, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 348, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 349, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 350, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 351, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 352, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 353, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 354, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 355, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 356, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 357, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 358, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 359, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 360, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 361, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 362, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 363, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 364, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 365, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 366, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 367, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 368, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 369, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 370, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 371, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 372, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 373, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 374, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 375, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 376, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 377, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 378, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 379, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 380, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 381, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 382, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 383, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 384, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 385, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 386, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 387, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 388, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 389, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 390, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 391, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 392, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 393, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 394, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 395, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 396, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 397, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 398, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 399, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 400, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 401, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 402, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 403, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 404, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 405, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 406, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 407, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 408, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 409, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 410, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 411, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 412, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 413, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 414, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 415, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 416, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 417, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 418, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 419, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 420, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 421, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 422, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 423, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 424, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 425, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 426, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 427, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 428, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 429, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 430, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 431, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Epoch: 55/60\n",
            "Batch number: 000, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 001, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 002, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 003, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 004, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 005, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 006, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 007, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 008, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 009, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 010, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 011, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 012, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 013, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 014, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 015, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 016, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 017, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 018, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 019, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 020, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 021, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 022, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 023, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 024, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 025, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 026, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 027, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 028, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 029, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 030, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 031, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 032, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 033, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 034, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 035, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 036, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 037, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 038, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 039, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 040, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 041, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 042, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 043, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 044, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 045, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 046, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 047, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 048, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 049, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 050, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 051, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 052, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 053, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 054, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 055, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 056, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 057, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 058, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 059, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 060, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 061, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 062, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 063, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 064, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 065, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 066, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 067, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 068, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 069, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 070, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 071, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 072, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 073, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 074, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 075, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 076, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 077, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 078, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 079, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 080, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 081, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 082, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 083, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 084, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 085, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 086, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 087, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 088, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 089, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 090, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 091, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 092, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 093, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 094, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 095, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 096, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 097, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 098, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 099, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 100, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 101, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 102, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 103, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 104, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 105, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 106, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 107, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 108, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 109, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 110, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 111, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 112, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 113, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 114, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 115, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 116, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 117, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 118, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 119, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 120, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 121, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 122, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 123, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 124, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 125, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 126, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 127, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 128, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 129, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 130, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 131, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 132, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 133, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 134, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 135, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 136, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 137, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 138, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 139, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 140, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 141, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 142, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 143, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 144, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 145, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 146, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 147, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 148, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 149, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 150, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 151, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 152, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 153, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 154, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 155, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 156, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 157, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 158, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 159, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 160, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 161, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 162, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 163, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 164, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 165, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 166, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 167, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 168, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 169, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 170, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 171, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 172, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 173, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 174, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 175, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 176, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 177, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 178, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 179, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 180, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 181, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 182, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 183, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 184, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 185, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 186, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 187, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 188, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 189, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 190, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 191, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 192, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 193, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 194, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 195, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 196, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 197, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 198, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 199, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 200, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 201, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 202, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 203, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 204, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 205, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 206, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 207, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 208, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 209, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 210, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 211, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 212, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 213, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 214, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 215, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 216, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 217, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 218, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 219, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 220, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 221, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 222, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 223, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 224, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 225, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 226, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 227, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 228, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 229, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 230, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 231, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 232, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 233, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 234, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 235, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 236, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 237, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 238, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 239, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 240, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 241, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 242, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 243, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 244, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 245, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 246, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 247, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 248, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 249, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 250, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 251, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 252, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 253, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 254, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 255, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 256, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 257, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 258, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 259, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 260, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 261, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 262, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 263, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 264, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 265, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 266, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 267, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 268, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 269, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 270, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 271, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 272, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 273, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 274, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 275, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 276, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 277, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 278, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 279, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 280, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 281, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 282, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 283, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 284, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 285, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 286, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 287, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 288, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 289, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 290, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 291, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 292, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 293, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 294, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 295, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 296, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 297, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 298, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 299, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 300, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 301, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 302, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 303, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 304, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 305, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 306, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 307, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 308, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 309, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 310, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 311, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 312, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 313, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 314, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 315, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 316, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 317, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 318, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 319, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 320, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 321, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 322, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 323, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 324, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 325, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 326, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 327, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 328, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 329, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 330, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 331, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 332, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 333, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 334, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 335, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 336, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 337, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 338, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 339, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 340, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 341, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 342, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 343, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 344, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 345, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 346, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 347, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 348, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 349, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 350, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 351, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 352, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 353, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 354, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 355, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 356, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 357, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 358, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 359, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 360, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 361, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 362, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 363, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 364, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 365, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 366, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 367, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 368, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 369, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 370, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 371, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 372, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 373, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 374, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 375, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 376, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 377, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 378, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 379, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 380, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 381, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 382, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 383, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 384, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 385, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 386, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 387, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 388, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 389, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 390, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 391, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 392, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 393, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 394, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 395, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 396, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 397, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 398, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 399, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 400, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 401, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 402, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 403, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 404, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 405, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 406, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 407, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 408, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 409, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 410, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 411, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 412, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 413, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 414, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 415, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 416, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 417, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 418, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 419, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 420, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 421, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 422, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 423, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 424, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 425, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 426, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 427, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 428, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 429, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 430, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 431, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Epoch: 56/60\n",
            "Batch number: 000, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 001, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 002, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 003, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 004, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 005, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 006, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 007, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 008, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 009, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 010, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 011, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 012, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 013, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 014, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 015, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 016, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 017, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 018, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 019, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 020, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 021, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 022, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 023, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 024, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 025, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 026, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 027, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 028, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 029, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 030, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 031, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 032, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 033, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 034, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 035, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 036, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 037, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 038, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 039, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 040, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 041, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 042, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 043, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 044, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 045, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 046, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 047, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 048, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 049, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 050, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 051, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 052, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 053, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 054, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 055, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 056, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 057, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 058, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 059, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 060, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 061, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 062, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 063, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 064, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 065, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 066, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 067, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 068, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 069, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 070, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 071, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 072, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 073, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 074, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 075, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 076, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 077, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 078, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 079, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 080, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 081, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 082, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 083, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 084, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 085, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 086, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 087, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 088, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 089, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 090, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 091, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 092, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 093, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 094, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 095, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 096, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 097, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 098, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 099, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 100, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 101, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 102, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 103, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 104, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 105, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 106, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 107, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 108, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 109, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 110, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 111, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 112, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 113, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 114, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 115, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 116, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 117, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 118, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 119, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 120, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 121, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 122, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 123, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 124, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 125, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 126, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 127, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 128, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 129, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 130, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 131, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 132, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 133, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 134, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 135, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 136, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 137, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 138, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 139, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 140, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 141, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 142, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 143, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 144, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 145, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 146, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 147, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 148, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 149, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 150, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 151, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 152, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 153, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 154, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 155, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 156, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 157, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 158, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 159, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 160, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 161, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 162, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 163, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 164, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 165, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 166, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 167, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 168, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 169, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 170, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 171, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 172, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 173, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 174, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 175, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 176, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 177, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 178, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 179, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 180, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 181, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 182, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 183, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 184, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 185, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 186, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 187, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 188, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 189, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 190, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 191, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 192, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 193, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 194, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 195, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 196, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 197, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 198, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 199, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 200, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 201, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 202, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 203, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 204, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 205, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 206, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 207, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 208, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 209, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 210, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 211, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 212, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 213, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 214, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 215, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 216, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 217, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 218, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 219, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 220, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 221, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 222, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 223, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 224, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 225, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 226, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 227, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 228, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 229, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 230, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 231, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 232, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 233, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 234, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 235, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 236, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 237, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 238, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 239, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 240, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 241, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 242, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 243, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 244, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 245, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 246, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 247, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 248, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 249, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 250, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 251, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 252, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 253, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 254, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 255, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 256, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 257, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 258, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 259, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 260, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 261, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 262, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 263, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 264, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 265, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 266, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 267, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 268, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 269, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 270, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 271, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 272, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 273, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 274, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 275, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 276, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 277, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 278, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 279, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 280, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 281, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 282, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 283, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 284, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 285, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 286, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 287, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 288, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 289, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 290, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 291, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 292, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 293, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 294, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 295, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 296, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 297, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 298, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 299, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 300, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 301, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 302, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 303, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 304, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 305, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 306, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 307, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 308, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 309, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 310, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 311, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 312, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 313, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 314, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 315, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 316, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 317, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 318, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 319, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 320, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 321, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 322, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 323, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 324, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 325, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 326, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 327, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 328, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 329, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 330, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 331, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 332, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 333, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 334, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 335, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 336, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 337, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 338, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 339, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 340, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 341, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 342, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 343, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 344, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 345, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 346, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 347, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 348, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 349, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 350, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 351, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 352, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 353, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 354, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 355, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 356, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 357, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 358, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 359, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 360, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 361, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 362, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 363, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 364, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 365, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 366, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 367, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 368, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 369, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 370, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 371, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 372, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 373, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 374, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 375, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 376, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 377, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 378, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 379, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 380, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 381, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 382, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 383, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 384, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 385, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 386, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 387, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 388, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 389, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 390, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 391, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 392, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 393, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 394, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 395, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 396, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 397, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 398, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 399, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 400, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 401, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 402, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 403, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 404, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 405, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 406, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 407, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 408, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 409, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 410, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 411, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 412, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 413, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 414, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 415, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 416, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 417, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 418, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 419, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 420, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 421, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 422, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 423, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 424, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 425, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 426, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 427, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 428, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 429, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 430, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 431, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Epoch: 57/60\n",
            "Batch number: 000, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 001, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 002, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 003, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 004, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 005, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 006, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 007, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 008, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 009, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 010, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 011, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 012, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 013, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 014, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 015, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 016, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 017, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 018, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 019, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 020, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 021, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 022, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 023, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 024, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 025, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 026, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 027, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 028, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 029, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 030, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 031, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 032, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 033, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 034, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 035, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 036, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 037, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 038, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 039, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 040, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 041, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 042, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 043, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 044, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 045, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 046, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 047, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 048, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 049, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 050, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 051, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 052, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 053, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 054, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 055, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 056, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 057, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 058, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 059, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 060, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 061, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 062, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 063, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 064, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 065, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 066, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 067, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 068, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 069, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 070, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 071, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 072, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 073, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 074, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 075, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 076, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 077, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 078, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 079, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 080, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 081, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 082, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 083, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 084, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 085, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 086, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 087, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 088, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 089, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 090, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 091, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 092, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 093, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 094, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 095, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 096, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 097, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 098, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 099, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 100, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 101, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 102, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 103, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 104, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 105, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 106, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 107, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 108, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 109, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 110, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 111, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 112, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 113, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 114, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 115, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 116, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 117, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 118, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 119, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 120, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 121, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 122, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 123, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 124, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 125, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 126, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 127, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 128, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 129, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 130, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 131, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 132, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 133, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 134, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 135, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 136, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 137, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 138, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 139, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 140, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 141, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 142, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 143, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 144, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 145, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 146, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 147, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 148, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 149, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 150, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 151, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 152, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 153, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 154, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 155, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 156, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 157, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 158, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 159, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 160, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 161, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 162, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 163, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 164, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 165, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 166, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 167, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 168, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 169, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 170, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 171, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 172, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 173, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 174, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 175, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 176, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 177, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 178, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 179, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 180, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 181, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 182, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 183, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 184, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 185, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 186, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 187, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 188, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 189, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 190, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 191, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 192, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 193, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 194, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 195, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 196, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 197, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 198, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 199, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 200, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 201, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 202, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 203, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 204, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 205, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 206, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 207, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 208, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 209, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 210, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 211, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 212, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 213, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 214, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 215, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 216, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 217, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 218, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 219, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 220, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 221, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 222, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 223, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 224, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 225, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 226, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 227, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 228, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 229, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 230, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 231, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 232, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 233, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 234, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 235, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 236, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 237, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 238, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 239, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 240, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 241, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 242, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 243, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 244, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 245, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 246, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 247, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 248, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 249, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 250, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 251, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 252, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 253, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 254, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 255, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 256, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 257, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 258, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 259, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 260, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 261, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 262, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 263, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 264, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 265, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 266, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 267, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 268, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 269, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 270, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 271, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 272, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 273, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 274, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 275, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 276, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 277, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 278, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 279, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 280, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 281, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 282, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 283, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 284, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 285, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 286, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 287, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 288, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 289, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 290, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 291, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 292, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 293, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 294, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 295, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 296, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 297, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 298, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 299, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 300, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 301, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 302, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 303, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 304, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 305, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 306, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 307, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 308, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 309, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 310, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 311, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 312, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 313, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 314, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 315, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 316, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 317, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 318, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 319, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 320, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 321, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 322, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 323, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 324, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 325, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 326, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 327, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 328, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 329, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 330, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 331, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 332, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 333, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 334, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 335, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 336, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 337, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 338, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 339, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 340, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 341, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 342, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 343, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 344, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 345, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 346, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 347, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 348, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 349, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 350, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 351, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 352, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 353, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 354, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 355, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 356, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 357, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 358, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 359, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 360, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 361, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 362, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 363, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 364, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 365, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 366, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 367, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 368, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 369, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 370, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 371, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 372, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 373, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 374, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 375, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 376, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 377, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 378, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 379, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 380, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 381, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 382, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 383, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 384, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 385, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 386, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 387, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 388, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 389, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 390, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 391, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 392, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 393, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 394, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 395, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 396, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 397, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 398, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 399, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 400, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 401, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 402, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 403, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 404, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 405, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 406, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 407, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 408, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 409, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 410, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 411, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 412, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 413, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 414, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 415, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 416, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 417, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 418, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 419, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 420, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 421, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 422, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 423, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 424, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 425, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 426, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 427, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 428, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 429, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 430, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 431, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Epoch: 58/60\n",
            "Batch number: 000, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 001, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 002, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 003, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 004, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 005, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 006, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 007, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 008, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 009, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 010, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 011, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 012, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 013, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 014, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 015, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 016, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 017, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 018, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 019, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 020, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 021, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 022, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 023, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 024, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 025, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 026, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 027, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 028, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 029, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 030, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 031, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 032, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 033, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 034, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 035, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 036, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 037, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 038, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 039, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 040, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 041, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 042, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 043, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 044, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 045, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 046, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 047, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 048, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 049, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 050, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 051, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 052, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 053, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 054, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 055, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 056, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 057, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 058, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 059, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 060, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 061, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 062, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 063, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 064, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 065, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 066, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 067, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 068, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 069, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 070, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 071, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 072, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 073, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 074, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 075, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 076, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 077, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 078, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 079, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 080, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 081, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 082, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 083, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 084, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 085, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 086, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 087, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 088, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 089, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 090, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 091, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 092, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 093, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 094, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 095, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 096, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 097, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 098, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 099, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 100, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 101, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 102, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 103, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 104, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 105, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 106, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 107, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 108, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 109, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 110, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 111, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 112, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 113, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 114, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 115, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 116, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 117, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 118, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 119, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 120, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 121, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 122, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 123, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 124, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 125, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 126, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 127, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 128, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 129, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 130, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 131, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 132, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 133, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 134, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 135, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 136, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 137, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 138, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 139, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 140, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 141, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 142, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 143, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 144, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 145, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 146, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 147, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 148, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 149, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 150, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 151, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 152, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 153, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 154, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 155, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 156, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 157, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 158, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 159, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 160, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 161, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 162, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 163, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 164, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 165, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 166, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 167, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 168, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 169, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 170, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 171, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 172, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 173, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 174, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 175, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 176, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 177, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 178, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 179, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 180, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 181, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 182, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 183, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 184, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 185, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 186, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 187, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 188, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 189, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 190, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 191, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 192, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 193, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 194, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 195, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 196, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 197, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 198, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 199, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 200, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 201, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 202, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 203, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 204, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 205, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 206, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 207, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 208, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 209, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 210, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 211, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 212, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 213, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 214, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 215, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 216, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 217, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 218, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 219, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 220, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 221, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 222, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 223, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 224, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 225, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 226, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 227, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 228, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 229, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 230, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 231, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 232, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 233, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 234, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 235, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 236, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 237, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 238, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 239, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 240, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 241, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 242, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 243, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 244, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 245, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 246, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 247, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 248, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 249, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 250, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 251, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 252, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 253, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 254, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 255, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 256, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 257, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 258, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 259, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 260, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 261, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 262, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 263, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 264, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 265, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 266, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 267, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 268, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 269, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 270, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 271, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 272, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 273, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 274, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 275, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 276, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 277, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 278, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 279, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 280, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 281, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 282, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 283, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 284, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 285, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 286, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 287, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 288, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 289, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 290, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 291, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 292, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 293, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 294, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 295, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 296, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 297, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 298, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 299, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 300, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 301, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 302, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 303, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 304, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 305, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 306, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 307, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 308, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 309, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 310, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 311, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 312, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 313, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 314, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 315, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 316, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 317, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 318, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 319, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 320, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 321, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 322, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 323, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 324, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 325, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 326, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 327, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 328, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 329, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 330, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 331, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 332, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 333, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 334, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 335, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 336, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 337, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 338, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 339, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 340, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 341, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 342, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 343, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 344, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 345, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 346, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 347, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 348, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 349, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 350, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 351, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 352, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 353, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 354, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 355, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 356, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 357, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 358, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 359, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 360, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 361, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 362, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 363, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 364, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 365, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 366, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 367, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 368, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 369, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 370, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 371, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 372, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 373, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 374, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 375, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 376, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 377, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 378, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 379, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 380, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 381, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 382, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 383, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 384, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 385, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 386, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 387, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 388, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 389, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 390, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 391, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 392, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 393, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 394, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 395, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 396, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 397, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 398, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 399, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 400, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 401, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 402, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 403, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 404, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 405, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 406, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 407, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 408, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 409, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 410, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 411, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 412, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 413, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 414, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 415, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 416, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 417, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 418, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 419, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 420, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 421, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 422, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 423, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 424, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 425, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 426, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 427, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 428, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 429, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 430, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 431, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Epoch: 59/60\n",
            "Batch number: 000, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 001, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 002, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 003, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 004, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 005, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 006, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 007, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 008, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 009, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 010, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 011, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 012, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 013, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 014, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 015, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 016, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 017, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 018, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 019, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 020, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 021, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 022, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 023, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 024, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 025, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 026, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 027, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 028, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 029, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 030, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 031, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 032, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 033, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 034, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 035, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 036, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 037, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 038, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 039, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 040, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 041, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 042, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 043, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 044, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 045, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 046, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 047, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 048, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 049, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 050, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 051, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 052, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 053, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 054, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 055, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 056, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 057, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 058, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 059, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 060, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 061, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 062, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 063, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 064, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 065, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 066, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 067, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 068, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 069, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 070, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 071, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 072, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 073, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 074, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 075, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 076, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 077, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 078, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 079, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 080, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 081, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 082, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 083, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 084, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 085, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 086, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 087, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 088, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 089, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 090, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 091, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 092, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 093, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 094, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 095, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 096, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 097, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 098, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 099, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 100, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 101, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 102, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 103, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 104, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 105, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 106, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 107, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 108, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 109, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 110, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 111, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 112, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 113, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 114, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 115, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 116, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 117, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 118, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 119, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 120, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 121, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 122, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 123, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 124, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 125, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 126, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 127, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 128, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 129, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 130, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 131, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 132, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 133, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 134, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 135, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 136, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 137, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 138, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 139, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 140, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 141, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 142, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 143, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 144, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 145, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 146, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 147, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 148, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 149, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 150, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 151, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 152, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 153, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 154, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 155, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 156, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 157, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 158, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 159, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 160, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 161, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 162, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 163, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 164, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 165, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 166, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 167, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 168, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 169, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 170, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 171, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 172, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 173, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 174, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 175, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 176, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 177, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 178, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 179, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 180, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 181, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 182, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 183, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 184, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 185, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 186, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 187, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 188, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 189, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 190, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 191, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 192, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 193, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 194, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 195, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 196, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 197, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 198, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 199, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 200, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 201, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 202, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 203, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 204, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 205, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 206, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 207, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 208, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 209, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 210, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 211, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 212, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 213, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 214, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 215, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 216, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 217, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 218, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 219, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 220, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 221, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 222, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 223, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 224, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 225, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 226, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 227, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 228, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 229, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 230, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 231, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 232, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 233, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 234, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 235, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 236, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 237, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 238, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 239, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 240, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 241, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 242, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 243, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 244, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 245, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 246, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 247, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 248, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 249, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 250, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 251, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 252, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 253, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 254, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 255, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 256, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 257, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 258, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 259, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 260, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 261, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 262, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 263, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 264, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 265, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 266, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 267, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 268, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 269, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 270, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 271, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 272, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 273, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 274, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 275, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 276, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 277, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 278, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 279, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 280, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 281, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 282, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 283, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 284, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 285, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 286, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 287, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 288, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 289, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 290, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 291, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 292, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 293, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 294, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 295, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 296, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 297, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 298, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 299, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 300, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 301, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 302, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 303, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 304, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 305, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 306, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 307, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 308, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 309, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 310, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 311, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 312, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 313, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 314, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 315, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 316, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 317, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 318, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 319, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 320, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 321, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 322, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 323, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 324, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 325, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 326, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 327, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 328, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 329, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 330, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 331, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 332, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 333, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 334, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 335, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 336, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 337, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 338, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 339, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 340, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 341, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 342, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 343, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 344, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 345, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 346, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 347, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 348, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 349, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 350, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 351, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 352, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 353, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 354, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 355, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 356, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 357, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 358, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 359, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 360, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 361, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 362, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 363, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 364, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 365, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 366, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 367, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 368, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 369, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 370, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 371, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 372, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 373, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 374, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 375, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 376, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 377, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 378, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 379, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 380, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 381, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 382, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 383, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 384, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 385, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 386, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 387, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 388, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 389, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 390, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 391, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 392, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 393, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 394, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 395, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 396, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 397, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 398, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 399, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 400, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 401, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 402, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 403, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 404, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 405, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 406, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 407, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 408, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 409, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 410, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 411, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 412, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 413, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 414, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 415, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 416, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 417, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 418, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 419, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 420, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 421, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 422, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 423, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 424, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 425, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 426, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 427, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 428, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 429, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 430, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 431, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Epoch: 60/60\n",
            "Batch number: 000, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 001, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 002, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 003, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 004, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 005, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 006, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 007, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 008, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 009, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 010, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 011, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 012, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 013, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 014, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 015, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 016, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 017, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 018, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 019, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 020, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 021, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 022, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 023, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 024, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 025, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 026, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 027, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 028, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 029, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 030, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 031, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 032, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 033, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 034, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 035, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 036, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 037, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 038, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 039, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 040, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 041, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 042, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 043, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 044, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 045, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 046, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 047, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 048, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 049, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 050, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 051, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 052, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 053, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 054, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 055, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 056, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 057, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 058, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 059, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 060, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 061, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 062, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 063, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 064, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 065, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 066, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 067, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 068, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 069, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 070, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 071, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 072, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 073, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 074, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 075, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 076, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 077, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 078, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 079, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 080, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 081, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 082, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 083, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 084, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 085, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 086, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 087, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 088, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 089, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 090, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 091, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 092, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 093, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 094, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 095, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 096, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 097, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 098, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 099, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 100, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 101, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 102, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 103, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 104, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 105, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 106, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 107, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 108, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 109, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 110, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 111, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 112, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 113, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 114, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 115, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 116, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 117, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 118, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 119, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 120, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 121, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 122, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 123, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 124, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 125, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 126, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 127, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 128, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 129, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 130, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 131, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 132, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 133, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 134, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 135, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 136, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 137, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 138, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 139, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 140, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 141, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 142, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 143, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 144, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 145, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 146, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 147, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 148, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 149, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 150, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 151, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 152, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 153, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 154, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 155, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 156, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 157, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 158, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 159, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 160, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 161, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 162, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 163, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 164, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 165, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 166, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 167, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 168, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 169, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 170, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 171, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 172, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 173, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 174, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 175, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 176, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 177, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 178, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 179, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 180, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 181, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 182, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 183, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 184, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 185, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 186, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 187, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 188, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 189, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 190, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 191, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 192, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 193, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 194, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 195, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 196, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 197, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 198, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 199, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 200, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 201, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 202, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 203, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 204, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 205, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 206, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 207, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 208, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 209, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 210, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 211, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 212, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 213, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 214, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 215, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 216, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 217, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 218, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 219, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 220, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 221, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 222, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 223, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 224, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 225, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 226, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 227, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 228, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 229, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 230, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 231, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 232, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 233, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 234, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 235, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 236, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 237, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 238, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 239, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 240, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 241, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 242, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 243, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 244, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 245, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 246, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 247, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 248, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 249, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 250, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 251, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 252, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 253, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 254, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 255, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 256, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 257, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 258, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 259, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 260, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 261, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 262, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 263, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 264, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 265, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 266, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 267, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 268, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 269, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 270, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 271, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 272, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 273, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 274, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 275, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 276, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 277, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 278, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 279, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 280, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 281, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 282, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 283, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 284, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 285, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 286, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 287, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 288, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 289, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 290, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 291, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 292, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 293, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 294, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 295, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 296, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 297, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 298, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 299, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 300, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 301, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 302, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 303, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 304, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 305, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 306, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 307, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 308, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 309, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 310, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 311, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 312, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 313, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 314, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 315, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 316, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 317, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 318, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 319, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 320, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 321, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 322, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 323, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 324, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 325, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 326, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 327, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 328, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 329, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 330, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 331, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 332, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 333, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 334, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 335, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 336, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 337, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 338, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 339, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 340, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 341, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 342, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 343, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 344, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 345, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 346, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 347, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 348, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 349, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 350, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 351, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 352, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 353, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 354, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 355, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 356, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 357, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 358, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 359, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 360, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 361, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 362, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 363, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 364, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 365, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 366, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 367, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 368, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 369, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 370, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 371, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 372, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 373, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 374, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 375, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 376, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 377, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 378, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 379, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 380, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 381, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 382, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 383, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 384, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 385, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 386, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 387, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 388, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 389, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 390, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 391, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 392, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 393, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 394, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 395, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 396, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 397, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 398, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 399, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 400, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 401, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 402, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 403, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 404, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 405, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 406, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 407, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 408, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 409, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 410, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 411, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 412, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 413, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 414, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 415, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 416, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 417, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 418, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 419, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 420, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 421, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 422, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 423, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 424, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 425, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 426, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 427, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 428, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 429, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 430, Training:  Loss: 0.0000, Accuracy: 1.0000\n",
            "Batch number: 431, Training:  Loss: 0.0000, Accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3-toFhvhIOT"
      },
      "source": [
        "def computeTestSetAccuracy(model, loss_criterion):\n",
        "    '''\n",
        "    Function to compute the accuracy on the test set\n",
        "    Parameters\n",
        "        :param model: Model to test\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "    '''\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    test_acc = 0.0\n",
        "    test_loss = 0.0\n",
        "\n",
        "    # Validation - No gradient tracking needed\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Set to evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        # Validation loop\n",
        "        for j, (inputs, labels) in enumerate(test_data):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "\n",
        "            # Compute the total loss for the batch and add it to valid_loss\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "            # Compute total accuracy in the whole batch and add to valid_acc\n",
        "            test_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "            print(\"Test Batch number: {:03d}, Test: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "\n",
        "    # Find average test loss and test accuracy\n",
        "    avg_test_loss = test_loss/test_data_size \n",
        "    avg_test_acc = test_acc/test_data_size\n",
        "\n",
        "    print(\"Test accuracy : \" + str(avg_test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwQ5VbTJhNBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87fd1848-72c8-4157-9d63-84acf7d22409"
      },
      "source": [
        "computeTestSetAccuracy(squeezenet,criterion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Batch number: 000, Test: Loss: 1430.2837, Accuracy: 0.5000\n",
            "Test Batch number: 001, Test: Loss: 1430.6698, Accuracy: 0.5000\n",
            "Test Batch number: 002, Test: Loss: 1874.0046, Accuracy: 0.3438\n",
            "Test Batch number: 003, Test: Loss: 1305.3087, Accuracy: 0.5312\n",
            "Test Batch number: 004, Test: Loss: 1476.5216, Accuracy: 0.4688\n",
            "Test Batch number: 005, Test: Loss: 1324.3857, Accuracy: 0.5312\n",
            "Test Batch number: 006, Test: Loss: 1494.9224, Accuracy: 0.4688\n",
            "Test Batch number: 007, Test: Loss: 1388.9426, Accuracy: 0.5000\n",
            "Test Batch number: 008, Test: Loss: 1181.1544, Accuracy: 0.5938\n",
            "Test Batch number: 009, Test: Loss: 1696.5991, Accuracy: 0.4062\n",
            "Test Batch number: 010, Test: Loss: 1222.7479, Accuracy: 0.5625\n",
            "Test Batch number: 011, Test: Loss: 1041.5779, Accuracy: 0.6250\n",
            "Test Batch number: 012, Test: Loss: 1699.3436, Accuracy: 0.4062\n",
            "Test Batch number: 013, Test: Loss: 1200.3232, Accuracy: 0.5625\n",
            "Test Batch number: 014, Test: Loss: 1194.5555, Accuracy: 0.5625\n",
            "Test Batch number: 015, Test: Loss: 1511.5737, Accuracy: 0.4688\n",
            "Test Batch number: 016, Test: Loss: 961.3561, Accuracy: 0.6562\n",
            "Test Batch number: 017, Test: Loss: 1109.0455, Accuracy: 0.5938\n",
            "Test Batch number: 018, Test: Loss: 1382.1010, Accuracy: 0.5000\n",
            "Test Batch number: 019, Test: Loss: 1965.5505, Accuracy: 0.3125\n",
            "Test Batch number: 020, Test: Loss: 1598.6143, Accuracy: 0.4375\n",
            "Test Batch number: 021, Test: Loss: 1695.3400, Accuracy: 0.3750\n",
            "Test Batch number: 022, Test: Loss: 1243.0800, Accuracy: 0.5625\n",
            "Test Batch number: 023, Test: Loss: 1577.6761, Accuracy: 0.4375\n",
            "Test Batch number: 024, Test: Loss: 1381.9208, Accuracy: 0.5000\n",
            "Test Batch number: 025, Test: Loss: 1655.3149, Accuracy: 0.4062\n",
            "Test Batch number: 026, Test: Loss: 1577.7842, Accuracy: 0.4375\n",
            "Test Batch number: 027, Test: Loss: 1068.5156, Accuracy: 0.6250\n",
            "Test Batch number: 028, Test: Loss: 1317.5516, Accuracy: 0.5312\n",
            "Test Batch number: 029, Test: Loss: 1147.2529, Accuracy: 0.5938\n",
            "Test Batch number: 030, Test: Loss: 1700.6482, Accuracy: 0.3750\n",
            "Test Batch number: 031, Test: Loss: 1410.9666, Accuracy: 0.5000\n",
            "Test Batch number: 032, Test: Loss: 1173.5942, Accuracy: 0.5938\n",
            "Test Batch number: 033, Test: Loss: 1598.1661, Accuracy: 0.4375\n",
            "Test Batch number: 034, Test: Loss: 1598.9937, Accuracy: 0.4375\n",
            "Test Batch number: 035, Test: Loss: 1378.8580, Accuracy: 0.5000\n",
            "Test Batch number: 036, Test: Loss: 1053.1605, Accuracy: 0.6250\n",
            "Test Batch number: 037, Test: Loss: 1593.6342, Accuracy: 0.4375\n",
            "Test Batch number: 038, Test: Loss: 1266.7562, Accuracy: 0.5625\n",
            "Test Batch number: 039, Test: Loss: 1742.1741, Accuracy: 0.3750\n",
            "Test Batch number: 040, Test: Loss: 1290.9459, Accuracy: 0.5312\n",
            "Test Batch number: 041, Test: Loss: 1402.2744, Accuracy: 0.5000\n",
            "Test Batch number: 042, Test: Loss: 1650.2883, Accuracy: 0.4062\n",
            "Test Batch number: 043, Test: Loss: 1208.0061, Accuracy: 0.5625\n",
            "Test Batch number: 044, Test: Loss: 1304.8395, Accuracy: 0.5312\n",
            "Test Batch number: 045, Test: Loss: 1487.4648, Accuracy: 0.4688\n",
            "Test Batch number: 046, Test: Loss: 1358.0809, Accuracy: 0.5000\n",
            "Test Batch number: 047, Test: Loss: 1937.8026, Accuracy: 0.3125\n",
            "Test Batch number: 048, Test: Loss: 1383.2163, Accuracy: 0.5000\n",
            "Test Batch number: 049, Test: Loss: 1567.0095, Accuracy: 0.4375\n",
            "Test Batch number: 050, Test: Loss: 1501.6840, Accuracy: 0.4688\n",
            "Test Batch number: 051, Test: Loss: 1321.1926, Accuracy: 0.5312\n",
            "Test Batch number: 052, Test: Loss: 1140.0895, Accuracy: 0.5938\n",
            "Test Batch number: 053, Test: Loss: 1389.8328, Accuracy: 0.5000\n",
            "Test Batch number: 054, Test: Loss: 1338.6489, Accuracy: 0.5312\n",
            "Test Batch number: 055, Test: Loss: 1672.5359, Accuracy: 0.4062\n",
            "Test Batch number: 056, Test: Loss: 1071.0614, Accuracy: 0.6250\n",
            "Test Batch number: 057, Test: Loss: 1334.0641, Accuracy: 0.5312\n",
            "Test Batch number: 058, Test: Loss: 1241.8687, Accuracy: 0.5625\n",
            "Test Batch number: 059, Test: Loss: 1168.1300, Accuracy: 0.5938\n",
            "Test Batch number: 060, Test: Loss: 1688.4769, Accuracy: 0.4062\n",
            "Test Batch number: 061, Test: Loss: 1389.5535, Accuracy: 0.5000\n",
            "Test Batch number: 062, Test: Loss: 1349.0522, Accuracy: 0.5312\n",
            "Test Batch number: 063, Test: Loss: 1457.6091, Accuracy: 0.4688\n",
            "Test Batch number: 064, Test: Loss: 1295.0883, Accuracy: 0.5312\n",
            "Test Batch number: 065, Test: Loss: 1322.1747, Accuracy: 0.5312\n",
            "Test Batch number: 066, Test: Loss: 1040.3391, Accuracy: 0.6250\n",
            "Test Batch number: 067, Test: Loss: 1118.1888, Accuracy: 0.5938\n",
            "Test Batch number: 068, Test: Loss: 1392.7675, Accuracy: 0.5000\n",
            "Test Batch number: 069, Test: Loss: 1230.7542, Accuracy: 0.5625\n",
            "Test Batch number: 070, Test: Loss: 1391.3882, Accuracy: 0.5000\n",
            "Test Batch number: 071, Test: Loss: 1313.6099, Accuracy: 0.5312\n",
            "Test Batch number: 072, Test: Loss: 1505.0812, Accuracy: 0.4688\n",
            "Test Batch number: 073, Test: Loss: 1695.2330, Accuracy: 0.4062\n",
            "Test Batch number: 074, Test: Loss: 1407.0060, Accuracy: 0.5000\n",
            "Test Batch number: 075, Test: Loss: 947.8621, Accuracy: 0.6562\n",
            "Test Batch number: 076, Test: Loss: 784.5406, Accuracy: 0.7188\n",
            "Test Batch number: 077, Test: Loss: 1210.6600, Accuracy: 0.5625\n",
            "Test Batch number: 078, Test: Loss: 1433.5662, Accuracy: 0.5000\n",
            "Test Batch number: 079, Test: Loss: 1326.8784, Accuracy: 0.5312\n",
            "Test Batch number: 080, Test: Loss: 1688.2343, Accuracy: 0.4062\n",
            "Test Batch number: 081, Test: Loss: 1310.2953, Accuracy: 0.5312\n",
            "Test Batch number: 082, Test: Loss: 1488.6473, Accuracy: 0.4688\n",
            "Test Batch number: 083, Test: Loss: 1754.1495, Accuracy: 0.3750\n",
            "Test Batch number: 084, Test: Loss: 1689.1838, Accuracy: 0.4062\n",
            "Test Batch number: 085, Test: Loss: 1410.2351, Accuracy: 0.5000\n",
            "Test Batch number: 086, Test: Loss: 1412.2213, Accuracy: 0.5000\n",
            "Test Batch number: 087, Test: Loss: 1642.0996, Accuracy: 0.4062\n",
            "Test Batch number: 088, Test: Loss: 1311.2697, Accuracy: 0.5312\n",
            "Test Batch number: 089, Test: Loss: 1487.6156, Accuracy: 0.4688\n",
            "Test Batch number: 090, Test: Loss: 1297.8951, Accuracy: 0.5312\n",
            "Test Batch number: 091, Test: Loss: 1585.1089, Accuracy: 0.4375\n",
            "Test Batch number: 092, Test: Loss: 1463.4816, Accuracy: 0.4688\n",
            "Test Batch number: 093, Test: Loss: 1620.0854, Accuracy: 0.4375\n",
            "Test Batch number: 094, Test: Loss: 1581.3696, Accuracy: 0.4375\n",
            "Test Batch number: 095, Test: Loss: 1226.2665, Accuracy: 0.5625\n",
            "Test Batch number: 096, Test: Loss: 1598.4167, Accuracy: 0.4375\n",
            "Test Batch number: 097, Test: Loss: 1259.3137, Accuracy: 0.5312\n",
            "Test Batch number: 098, Test: Loss: 1548.3911, Accuracy: 0.4375\n",
            "Test Batch number: 099, Test: Loss: 1402.4071, Accuracy: 0.5000\n",
            "Test Batch number: 100, Test: Loss: 1270.9556, Accuracy: 0.5312\n",
            "Test Batch number: 101, Test: Loss: 1683.4410, Accuracy: 0.4062\n",
            "Test Batch number: 102, Test: Loss: 1165.7118, Accuracy: 0.5938\n",
            "Test Batch number: 103, Test: Loss: 1031.5347, Accuracy: 0.6250\n",
            "Test Batch number: 104, Test: Loss: 1419.7559, Accuracy: 0.5000\n",
            "Test Batch number: 105, Test: Loss: 1151.0476, Accuracy: 0.5938\n",
            "Test Batch number: 106, Test: Loss: 1412.7678, Accuracy: 0.5000\n",
            "Test Batch number: 107, Test: Loss: 1482.3866, Accuracy: 0.4688\n",
            "Test Batch number: 108, Test: Loss: 1581.7493, Accuracy: 0.4375\n",
            "Test Batch number: 109, Test: Loss: 1241.1108, Accuracy: 0.5625\n",
            "Test Batch number: 110, Test: Loss: 1499.5559, Accuracy: 0.4688\n",
            "Test Batch number: 111, Test: Loss: 1491.1370, Accuracy: 0.4688\n",
            "Test Batch number: 112, Test: Loss: 1752.6326, Accuracy: 0.3750\n",
            "Test Batch number: 113, Test: Loss: 1353.4337, Accuracy: 0.5312\n",
            "Test Batch number: 114, Test: Loss: 1623.6736, Accuracy: 0.4375\n",
            "Test Batch number: 115, Test: Loss: 1165.4163, Accuracy: 0.5938\n",
            "Test Batch number: 116, Test: Loss: 2155.0623, Accuracy: 0.2500\n",
            "Test Batch number: 117, Test: Loss: 1658.4718, Accuracy: 0.4062\n",
            "Test Batch number: 118, Test: Loss: 977.8677, Accuracy: 0.6562\n",
            "Test Batch number: 119, Test: Loss: 1836.2627, Accuracy: 0.3438\n",
            "Test Batch number: 120, Test: Loss: 1752.4994, Accuracy: 0.3750\n",
            "Test Batch number: 121, Test: Loss: 1252.5656, Accuracy: 0.5625\n",
            "Test Batch number: 122, Test: Loss: 1316.0719, Accuracy: 0.5312\n",
            "Test Batch number: 123, Test: Loss: 1965.9061, Accuracy: 0.3125\n",
            "Test Batch number: 124, Test: Loss: 1489.1783, Accuracy: 0.4688\n",
            "Test Batch number: 125, Test: Loss: 1482.4868, Accuracy: 0.4688\n",
            "Test Batch number: 126, Test: Loss: 1310.9866, Accuracy: 0.5312\n",
            "Test Batch number: 127, Test: Loss: 1839.6001, Accuracy: 0.3438\n",
            "Test Batch number: 128, Test: Loss: 1395.4563, Accuracy: 0.5000\n",
            "Test Batch number: 129, Test: Loss: 1308.7316, Accuracy: 0.5312\n",
            "Test Batch number: 130, Test: Loss: 1388.9960, Accuracy: 0.5000\n",
            "Test Batch number: 131, Test: Loss: 1620.5404, Accuracy: 0.4062\n",
            "Test Batch number: 132, Test: Loss: 1402.1846, Accuracy: 0.5000\n",
            "Test Batch number: 133, Test: Loss: 1048.5254, Accuracy: 0.6250\n",
            "Test Batch number: 134, Test: Loss: 1269.6047, Accuracy: 0.5312\n",
            "Test Batch number: 135, Test: Loss: 1517.8217, Accuracy: 0.4688\n",
            "Test Batch number: 136, Test: Loss: 1724.2462, Accuracy: 0.3750\n",
            "Test Batch number: 137, Test: Loss: 1672.8534, Accuracy: 0.4062\n",
            "Test Batch number: 138, Test: Loss: 1554.9535, Accuracy: 0.4375\n",
            "Test Batch number: 139, Test: Loss: 1121.0094, Accuracy: 0.5938\n",
            "Test Batch number: 140, Test: Loss: 1507.6367, Accuracy: 0.4688\n",
            "Test Batch number: 141, Test: Loss: 1620.9590, Accuracy: 0.4062\n",
            "Test Batch number: 142, Test: Loss: 1489.9570, Accuracy: 0.4688\n",
            "Test Batch number: 143, Test: Loss: 1500.2426, Accuracy: 0.4688\n",
            "Test Batch number: 144, Test: Loss: 1304.9128, Accuracy: 0.5312\n",
            "Test Batch number: 145, Test: Loss: 1223.2725, Accuracy: 0.5625\n",
            "Test Batch number: 146, Test: Loss: 1671.8798, Accuracy: 0.4062\n",
            "Test Batch number: 147, Test: Loss: 1229.1271, Accuracy: 0.5625\n",
            "Test Batch number: 148, Test: Loss: 1408.6694, Accuracy: 0.5000\n",
            "Test Batch number: 149, Test: Loss: 1227.8171, Accuracy: 0.5625\n",
            "Test Batch number: 150, Test: Loss: 1131.8273, Accuracy: 0.5938\n",
            "Test Batch number: 151, Test: Loss: 716.1196, Accuracy: 0.7500\n",
            "Test Batch number: 152, Test: Loss: 1236.5717, Accuracy: 0.5625\n",
            "Test Batch number: 153, Test: Loss: 1401.7667, Accuracy: 0.5000\n",
            "Test Batch number: 154, Test: Loss: 1388.2565, Accuracy: 0.5000\n",
            "Test Batch number: 155, Test: Loss: 1673.4124, Accuracy: 0.4062\n",
            "Test Batch number: 156, Test: Loss: 2113.6377, Accuracy: 0.2500\n",
            "Test Batch number: 157, Test: Loss: 1577.6002, Accuracy: 0.4375\n",
            "Test Batch number: 158, Test: Loss: 2198.7449, Accuracy: 0.2188\n",
            "Test Batch number: 159, Test: Loss: 1551.7972, Accuracy: 0.4375\n",
            "Test Batch number: 160, Test: Loss: 1023.2485, Accuracy: 0.6250\n",
            "Test Batch number: 161, Test: Loss: 1206.2791, Accuracy: 0.5625\n",
            "Test Batch number: 162, Test: Loss: 1690.5872, Accuracy: 0.4062\n",
            "Test Batch number: 163, Test: Loss: 1416.4783, Accuracy: 0.5000\n",
            "Test Batch number: 164, Test: Loss: 1757.9414, Accuracy: 0.3750\n",
            "Test Batch number: 165, Test: Loss: 1574.6929, Accuracy: 0.4375\n",
            "Test Batch number: 166, Test: Loss: 1391.1459, Accuracy: 0.5000\n",
            "Test Batch number: 167, Test: Loss: 1591.4578, Accuracy: 0.4375\n",
            "Test Batch number: 168, Test: Loss: 1591.9065, Accuracy: 0.4375\n",
            "Test Batch number: 169, Test: Loss: 1424.4993, Accuracy: 0.5000\n",
            "Test Batch number: 170, Test: Loss: 1469.9805, Accuracy: 0.4688\n",
            "Test Batch number: 171, Test: Loss: 1326.1472, Accuracy: 0.5312\n",
            "Test Batch number: 172, Test: Loss: 1731.7747, Accuracy: 0.3750\n",
            "Test Batch number: 173, Test: Loss: 1455.3759, Accuracy: 0.4688\n",
            "Test Batch number: 174, Test: Loss: 1534.9803, Accuracy: 0.4375\n",
            "Test Batch number: 175, Test: Loss: 1317.6769, Accuracy: 0.5312\n",
            "Test Batch number: 176, Test: Loss: 854.3090, Accuracy: 0.6875\n",
            "Test Batch number: 177, Test: Loss: 1139.7001, Accuracy: 0.5938\n",
            "Test Batch number: 178, Test: Loss: 1311.9335, Accuracy: 0.5312\n",
            "Test Batch number: 179, Test: Loss: 1191.2010, Accuracy: 0.5625\n",
            "Test Batch number: 180, Test: Loss: 1205.0811, Accuracy: 0.5625\n",
            "Test Batch number: 181, Test: Loss: 1075.8333, Accuracy: 0.6250\n",
            "Test Batch number: 182, Test: Loss: 1912.1124, Accuracy: 0.3125\n",
            "Test Batch number: 183, Test: Loss: 1143.8340, Accuracy: 0.5938\n",
            "Test Batch number: 184, Test: Loss: 1360.1523, Accuracy: 0.5000\n",
            "Test Batch number: 185, Test: Loss: 1153.2831, Accuracy: 0.5938\n",
            "Test Batch number: 186, Test: Loss: 1930.1536, Accuracy: 0.3125\n",
            "Test Batch number: 187, Test: Loss: 1402.1617, Accuracy: 0.5000\n",
            "Test Batch number: 188, Test: Loss: 1162.3599, Accuracy: 0.5938\n",
            "Test Batch number: 189, Test: Loss: 1640.2144, Accuracy: 0.4062\n",
            "Test Batch number: 190, Test: Loss: 1424.9519, Accuracy: 0.5000\n",
            "Test Batch number: 191, Test: Loss: 1367.4531, Accuracy: 0.5000\n",
            "Test Batch number: 192, Test: Loss: 1739.2294, Accuracy: 0.3750\n",
            "Test Batch number: 193, Test: Loss: 975.1077, Accuracy: 0.6562\n",
            "Test Batch number: 194, Test: Loss: 1137.7855, Accuracy: 0.5938\n",
            "Test Batch number: 195, Test: Loss: 1133.0942, Accuracy: 0.5938\n",
            "Test Batch number: 196, Test: Loss: 1755.7203, Accuracy: 0.3750\n",
            "Test Batch number: 197, Test: Loss: 1226.7551, Accuracy: 0.5625\n",
            "Test Batch number: 198, Test: Loss: 1637.1722, Accuracy: 0.4062\n",
            "Test Batch number: 199, Test: Loss: 1239.9515, Accuracy: 0.5625\n",
            "Test Batch number: 200, Test: Loss: 1166.2926, Accuracy: 0.5938\n",
            "Test Batch number: 201, Test: Loss: 1264.8547, Accuracy: 0.5625\n",
            "Test Batch number: 202, Test: Loss: 1666.7415, Accuracy: 0.4062\n",
            "Test Batch number: 203, Test: Loss: 1311.4136, Accuracy: 0.5312\n",
            "Test Batch number: 204, Test: Loss: 1053.8785, Accuracy: 0.6250\n",
            "Test Batch number: 205, Test: Loss: 1219.3370, Accuracy: 0.5625\n",
            "Test Batch number: 206, Test: Loss: 1045.1964, Accuracy: 0.6250\n",
            "Test Batch number: 207, Test: Loss: 1502.9349, Accuracy: 0.4688\n",
            "Test Batch number: 208, Test: Loss: 729.4915, Accuracy: 0.7500\n",
            "Test Batch number: 209, Test: Loss: 1239.7990, Accuracy: 0.5625\n",
            "Test Batch number: 210, Test: Loss: 987.4995, Accuracy: 0.6562\n",
            "Test Batch number: 211, Test: Loss: 1243.6877, Accuracy: 0.5625\n",
            "Test Batch number: 212, Test: Loss: 1303.2607, Accuracy: 0.5312\n",
            "Test Batch number: 213, Test: Loss: 692.4877, Accuracy: 0.7500\n",
            "Test Batch number: 214, Test: Loss: 1394.2115, Accuracy: 0.5000\n",
            "Test Batch number: 215, Test: Loss: 1487.9403, Accuracy: 0.4688\n",
            "Test accuracy : 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}